\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amsthm, amssymb}
\usepackage[margin=3cm]{geometry}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}
\usepackage{todonotes}
\usepackage{nicefrac}
\usepackage{mathrsfs}
\usepackage{tikz}
\usepackage{thm-restate}

\usepackage{etoc}

%%%%%%%%    THEOREM DEFINITIONS AND RESTATABLE
% \newcounter{claim}
% \setcounter{claim}{0}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{dependency}{Dependency}

\newcommand{\matt}[1]{\todo[color=red!50, prepend, caption={Matt}, tickmarkheight=0.25cm]{#1}}
\newcommand{\inlinetodo}[1]{\textcolor{red}{{\Large TODO:} #1}}




%%%%%%%%    NOTATION DEFINITIONS FOR EASIER WRITING
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\ketbra}[2]{| #1\rangle\! \langle #2|}
\newcommand{\parens}[1]{\left( #1 \right)}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left| \left| #1 \right| \right|}
\newcommand{\diamondnorm}[1]{\left| \left| #1 \right| \right|_\diamond}
\newcommand{\anglebrackets}[1]{\left< #1 \right>}
\newcommand{\overlap}[2]{\anglebrackets{#1 , #2 }}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\openone}{\mathds{1}}
\newcommand{\expect}[1]{\mathbb{E}\brackets{#1}}
\newcommand{\variance}[1]{\textit{Var} \brackets{ #1 }}
\newcommand{\prob}[1]{\text{Pr}\left[ #1 \right]}
\newcommand{\bigo}[1]{O\left( #1 \right)}
\newcommand{\bigotilde}[1]{\widetilde{O} \left( #1 \right)}
\newcommand{\ts}{\textsuperscript}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\trace}[1]{\Tr \brackets{ #1 }}
\newcommand{\partrace}[2]{\Tr_{#1} \brackets{ #2 }}
\newcommand{\complex}{\mathbb{C}}

%%%%% COMMONLY USED OBJECTS
\newcommand{\hilb}{\mathcal{H}}
\newcommand{\partfun}{\mathcal{Z}}
\newcommand{\identity}{\mathds{1}}
\newcommand{\gue}{\rm GUE}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\hermMathOp}{Herm}
\DeclareMathOperator{\im}{Im}
\newcommand{\herm}[1]{\hermMathOp\parens{#1}}


\title{Thermal State Prep}
\author{Matthew Hagan, Nathan Wiebe}
\date{May 2022}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Going to leave this blank for now. \cite{shiraishi_undecidability_2021}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
We denote the Hilbert space of the system as $\hilb_{S}$ and the environment as $\hilb_{E}$, with the Hamiltonians governing each as $H_{S}$ and $H_{E}$. We will assume without loss of generality that the system's Hilbert space can be encoded with $n$ qubits, giving $\dim_S = 2^{n}$, and the environment's Hilbert space can be encoded with $m$ qubits giving $\dim_E = 2^{m}$. The Hamiltonian for the joint system on $\hilb_{S} \otimes \hilb_{E}$ is then $H = H_{S} \otimes \identity + \identity \otimes H_{E}$. The Hilbert space of the combined system and environment is of dimension $\dim = \dim_E \cdot \dim_S = 2^{n + m}$. 

We will primarily work in the eigenbasis for each Hamiltonian:
\begin{equation}
    H_{S} = \sum_{i = 0}^{2^n - 1} \lambda_S(i) \ketbra{s_i}{s_i} ~,~ H_{E} = \sum_{j=0}^{2^m - 1} \lambda_E(j) \ketbra{e_j}{e_j} ~,~ H = \sum_{i=0}^{2^n - 1} \sum_{j=0}^{2^m - 1} \lambda(i,j) (\ket{s_i} \otimes \ket{e_j})(\bra{s_i} \otimes \bra{e_j}),
\end{equation}
for convenience we will denote the tensor product of eigenvectors simply by their indices $\ket{i,j} \coloneqq \ket{s_i} \otimes \ket{e_j}$. For convenience we define $\lambda(i,j) \coloneqq \lambda_S(i) + \lambda_E(j)$. We also make use of the following notation for the energy differences of the system-environment Hamiltonian
$$\Delta(i,j|k,l) \coloneqq \lambda(i,j) - \lambda(k,l),$$
and will use $\Delta_S(i,j) \coloneqq \lambda_S(i) - \lambda_S(j)$ for just the system differences. To define the degeneracy of each eigenvalue we use the symbol
$$\eta(i,j) \coloneqq \sum_{(k,l) : \Delta(i,j|k,l) = 0} 1.$$ We use the notation $\delta(i,j|k,l)$ to denote the product of Kronecker delta functions $\delta(i,j|k,l) = \delta_{i,k} \delta_{j,l}$.

For input states we will typically assume thermal states of the form $\rho_S(\beta) = \frac{e^{-\beta H_S}}{\partfun_S}$, where $\partfun_S = \trace{e^{-\beta H_S}}$, where the inverse temperature $\beta$ of the partition function will typically be assumed but written explicitly if need be. We will assume environment states of the form $\rho_E(\beta) = \frac{e^{-\beta H_E}}{\partfun_E}$ and we will typically denote the tensor product of the system and environment states as $\rho(\beta_S, \beta_E) = \rho_S(\beta_S) \otimes \rho_E(\beta_E)$. The inputs $\beta_S, \beta_E$ will typically be surpressed in most cases as well.


Overall one application of our channel is represented as
\begin{equation}
    \Phi(\rho) := \int \partrace{\hilb_E}{e^{+i(H + \alpha G)t} \rho e^{-i(H + \alpha G) t}} dG.
\end{equation}
It will prove convenient to study the time evolution of a fixed interaction as a map from the total system-environment space to itself. We denote this channel for a specific random interaction $G$ as
\begin{equation}
    \Phi_G(\rho_S \otimes \rho_E) := e^{+i (H+ \alpha G) t} \rho_S \otimes \rho_E e^{-i (H + \alpha G) t}. \label{eq:phi_g_definition}
\end{equation}
Clearly then $\Phi(\rho_S) = \partrace{env}{\int \Phi_G (\rho_S \otimes \rho_E) dG}$. We use $G$ to denote the randomized interaction term, where $G = U_G D U_G^\dagger$. The measure we choose for the eigenbasis of $G$ is $U_G \sim Haar$ and the eigenvalues are i.i.d with mean 0 and variance $1$.  This gives the overall interaction measure as the decomposition $\int dG = \int \int dD ~ dU_G$. The interaction strength of $G$ will be controlled through the coupling coefficient $\alpha$.

\begin{restatable}{lemma}{haar_two_moment} \label{lem:haar_two_moment}
    Let $U$ be a unitary matrix over $\dim$ dimensions that is distributed according to the Haar measure. Then the following average is
    \begin{align}
        \int \bra{i_1} U \ket{j_1} \bra{i_2} U \ket{j_2} \bra{k_1} U^\dagger \ket{l_1} ~ \bra{k_2} U^\dagger \ket{l_2} dU =& ~\frac{1}{\dim^2 - 1} \parens{\delta_{i_1, l_1} \delta_{j_1, k_1} \delta_{i_2, l_2} \delta_{j_2, k_2} + \delta_{i_1, l_2} \delta_{j_1, k_2} \delta_{i_2, l_1} \delta_{j_2, k_1}} \nonumber \\
        &- \frac{1}{\dim(\dim^2 - 1)} \parens{\delta_{i_1, l_2} \delta_{j_1, k_1} \delta_{i_2, l_1} \delta_{j_2, k_2} + \delta_{i_1, l_1} \delta_{j_1, k_2} \delta_{i_2, l_2} \delta_{j_2, k_1}}. \label{eq:haar_two_moment_integral}
    \end{align}
\end{restatable}

\begin{lemma}{lemma}\label{lem:sinc_poly_approx}
    Let $f(x) = \frac{\sin^2(x)}{x^2}$. The constant approximation $f(x) = 1$ has error $|f(x) - 1| \leq \epsilon_{1}$ if $|x| \leq \sqrt{2 \epsilon_{1}}$. Further, if $x = \Delta t$, with $\Delta \geq \Delta_{\min}$, we note that $t \geq (\Delta_{\min} \sqrt{\epsilon_{\sinc} } )^{-1}$ suffices for $f(\Delta t) \leq \epsilon_{\sinc}$.
\end{lemma}
\begin{proof}
    We use the form of $\sinc$ as $\frac{\sin(x)}{x} = \int_0^1 \cos(sx) ds$. The first 3 derivatives are computed using straightforward calculus
    \begin{align}
        \frac{df}{dx} &= -2 \int_0^1 \sin(sx) s ds \int_0^1 \cos(sx) ds \\
        \frac{d^2f}{dx^2} &= -2 \int_0^1 \cos(sx)s^2 ds \int_0^1 \cos(sx) ds + \int_0^1 \sin(sx) s ~ds \int_0^1 \sin(sx) s ~ds.
        % \frac{d^3 f}{dx^3} &= 2 \int_0^1 \sin(sx)s^3 ds \int_0^1 \cos(sx) ds + 4 \int_0^1 \cos(sx) s^2 ds \int_0^1 \sin(sx) s ds .
    \end{align}
    By using the fact that $\cos(sx) \to 1$ and $\sin(sx) \to 0$ as $x \to 0$ we can evaluate the Taylor's series to $f(x)$ directly. We also make use of the inequality $\abs{\int_a^b f(x) dx} \leq \int_a^b \abs{f(x)} dx$ and that sine and cosine are bounded by 1.
    \begin{align}
        f(x) &= \frac{\sin^2(x)}{x^2} \bigg|_{x = 0} + x \frac{df}{dx}\bigg|_{x = 0} + \frac{x^2}{2!} \frac{d^2f}{dx^2}\bigg|_{x=c} \\
        f(x) &= 1 + \frac{x^2}{2} \frac{d^2f}{dx^2}\bigg|_{x=c}  \\
        \abs{f(x) - 1} &= \frac{\abs{x}^2}{2} \abs{\frac{d^2f}{dx^2}(x=c)} \\
        &\leq \frac{|x|^2}{3}
    \end{align}
    Requiring $|x|^2/3 \leq \epsilon$ yields the statement.
    
    We will also have a need for bounding $f(x)$ when $x$ is of the form $x = \Delta t$. We would like to choose $t$ large enough so that $\Delta \geq \Delta_{\text{min}}$ implies that $f(\Delta t) \leq \epsilon_{\sinc}$. This can be given using the fact that $\sin(x) \leq 1$:
    \begin{align}
        f(\Delta t) &= \frac{\sin^2(\Delta t)}{\Delta^2 t^2} \\
        &\leq \frac{1}{\Delta^2 t^2} \\
        \frac{1}{\Delta_{\text{min}}^2 t^2} &\leq \frac{1}{\Delta^2 t^2} \leq \epsilon_{\sinc} \\
        \frac{1}{\Delta_{\text{min}} \sqrt{\epsilon_{\sinc}}} &\leq t.
    \end{align}
    We now use this bound on $t$ to investigate when the polynomial approximation given above holds for inputs $x = (\Delta - \gamma)t$. We assume $\gamma \geq 0 $.
    \begin{align}
        |(\Delta - \gamma)t| &\leq \sqrt[3]{6\epsilon} \\
        \abs{\Delta - \gamma} &\leq \frac{\sqrt[3]{6 \epsilon}}{t} \\
        &\leq \Delta_{\text{min}} \sqrt{\epsilon_{\sinc}}\sqrt[3]{6 \epsilon}.
    \end{align}
    We also would like to note the differences if one uses the constant term for $f(x)$ as opposed to a quadratic.
    $$f(x) = 1 + x \frac{df}{dx}\bigg|_{x=c}$$
    Using the fact that the first derivative is 0 at $x= 0$ and the second is bounded by $\abs{\frac{d^2f}{dx^2}} \leq 1$, we get
    $$f(x) = 1 + R(x)$$
    and $\abs{R(x)} \leq |x|^2 / 2$ implies that $|x| \leq \sqrt{2\epsilon}$ suffices for $\abs{f(x) - 1} \leq \epsilon$. Using this bound with $f((\Delta - \gamma)t)$, we get $\abs{\Delta - \gamma} \leq \Delta_{\text{min}} \sqrt{2 \epsilon_{\sinc} \epsilon} $.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Taylor's Series for $\Phi$}

\begin{lemma}[First Order $\alpha$ Correction to $\Phi$]
   Given a randomized environment interaction channel $\Phi$ with coupling coefficient $\alpha$, the first order correction is
   \begin{equation}
        \frac{\partial}{\partial \alpha} \Phi(\rho_S) \bigg|_{\alpha = 0} = 0.
   \end{equation}
\end{lemma}
\begin{proof}
    We start by using linearity of derivatives, integration, and partial trace to pull the $\alpha$ derivative to act on $\Phi_G$ as
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi(\rho_S) \bigg|_{\alpha = 0} &= \frac{\partial}{\partial \alpha} \partrace{\mathcal{H}_E}{\int \Phi_G(\rho_s) dG} \bigg|_{\alpha = 0} \\
         &= \partrace{\mathcal{H}_E}{\int \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) dG \bigg|_{\alpha = 0} } .
    \end{align}
    Now we use Eq. \eqref{eq:phi_g_definition} to compute the derivatives, we remind the reader that $\rho = \rho_S \otimes \rho_E$,
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi_G (\rho_S) &= \parens{\frac{\partial}{\partial \alpha} e^{+ i (H + \alpha G)t}} \rho e^{-i (H + \alpha G) t} + e^{+i (H + \alpha G)t} \rho \parens{\frac{\partial}{\partial \alpha} e^{- i (H + \alpha G)t}} \\
        &= \parens{\int_{0}^{1} e^{i s (H+\alpha G)t} (i t G) e^{i (1-s) (H+\alpha G)t} ds} \rho e^{-i(H+\alpha G)t} \nonumber \\
    &~ ~+ e^{i(H+\alpha G)t} \rho \parens{\int_{0}^1 e^{-i s (H+\alpha G) t} (- i t G) e^{-i (1-s) (H+\alpha G)t} ds}. \label{eq:first_order_alpha_derivative}
    \end{align}
    We can further simplify this by bringing in the evaluation of $\alpha = 0$ through the partial trace and integration, as they are uniformly convergent over $\alpha$ (is that the correct notion that allows us to switch orders?)
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) \bigg|_{\alpha = 0} &= i t \int_0^1 e^{i s H t} G e^{-i s H t} ds e^{i H t} \rho e^{-i H t} - i t e^{+i H t} \rho \int_0^1 e^{-is H t} G e^{-i(1-s) Ht} ds \\
        &= i t \parens{\int_0^1 G(s t) ds} \rho(t) - it \rho(t) \parens{\int_0^1 G(s t) ds} \\
        &= i t \int_0^1 [G(s t), \rho(t)] ds,
    \end{align}
    where we have used the Heisenberg picture $\rho(t) = e^{i H t} \rho e^{-i H t}$ to simplify the notation.

    This expression is now amenable to computing the correction to the total channel. We do so by performing the integration over the randomized interactions. We take advantage of the structure of our interaction measure, that is $G = U_G D U_G^\dagger$ and $dG = dU_G dD$, which allows us to write
    \begin{align}
        \int \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) \bigg|_{\alpha = 0} dG &= it \int \int_0^1 \left[ e^{i H s t} G e^{-i H s t}, \rho(t) \right] ds ~dG \\
        &= it \int_0^1 \left[ e^{i H s t} \parens{\int \int U_G D U_G^\dagger ~dU_G ~ dD} e^{-i H s t}, \rho(t)  \right] ds \\
        &= i t \int_0^1 \left[ e^{i H s t} \parens{\int U_G \parens{\int D ~ dD} ~ U_G^\dagger dU_G } e^{-i H s t}, \rho(t) \right] ds \\
        &= 0.
    \end{align}
    This last step relies on the use of random eigenvalues with mean 0, implying $\int D ~dD = 0$ which shows that $\frac{\partial}{\partial \alpha} \Phi(\rho_S) \big|_{\alpha = 0 } = 0$.
\end{proof}

\begin{lemma} \label{lem:two_heisenberg_interactions}
    Let $G(t)$ denote the Heisenberg evolved random interaction $G(t) = e^{iHt} G e^{-iHt}$ for a total Hamiltonian $H$. After averaging over the interaction measure the product $G(x) G(y)$ can be computed as
    \begin{equation}
        \int G(x) G(y) dG = \frac{1}{\dim + 1} \parens{\sum_{(i,j),(k,l)} e^{i \Delta(i,j|k,l) (x-y)} \ketbra{i,j}{i,j} + \identity}.
    \end{equation}
\end{lemma}
\begin{proof}
The overall structure of this proof is to evaluate the product in the Hamiltonian eigenbasis and split the product into three factors: a phase contribution from the time evolution, a Haar integral from the eigenvalues of the random interaction, and the eigenvalue contribution of the random interaction. Since this involves the use of multiple indices, it will greatly simplify the proof to use a single index over the total Hilbert space $\hilb$ as opposed to two indices over $\hilb_S \otimes \hilb_E$. For example, the index $a$ should be thought of as a pair $(a_s, a_e)$, and functions $\lambda(a)$ should be thought of as $\lambda(a_s, a_e)$. Once the final form of the expression is reached we will substitute in pairs of indices for easier use of the lemma in other places.
    \begin{align}
        \int G(x) G(y) dG &= \int e^{+i H x} U_G D U_G^\dagger e^{-i H x} e^{+i H y} U_G D U_G^\dagger e^{-i H y} dU_G ~dD \\
        &= \int \bigg[\sum_a e^{+i \lambda(a)x}\ketbra{a}{a}  U_G \sum_b D(b)\ketbra{b}{b} U_G^\dagger \nonumber \\
        &\quad \sum_c e^{-i \lambda(c) (x - y)} \ketbra{c}{c} U_G \sum_d D(d)\ketbra{d}{d} U_G^\dagger \sum_e e^{-i \lambda(e) y} \ketbra{e}{e} \bigg] dU_G ~dD\\
        &=\sum_{a,b,c,d,e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \nonumber \\
        &\quad \times \int \bra{a} U_G \ket{b} \bra{c} U_G \ket{d} \bra{b} U_G^{\dagger} \ket{c} \bra{d} U_G^\dagger \ket{e} dU_G \int D(b) D(d) dD \\
        &=  \sum_{a, b, c, d, e} \delta_{bd} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \nonumber \\
        &\quad \times \int \bra{a} U_G \ket{b} \bra{c} U_G \ket{d} \bra{b} U_G^{\dagger} \ket{c} \bra{d} U_G^\dagger \ket{e} dU_G. \\
    \end{align}
    Now the summation over $d$ fixes $d=b$ and we use Lemma \ref{lem:haar_two_moment} to compute the Haar integral, which simplifies greatly due to the repeated $b$ index. Plugging the result into the above yields the following
    \begin{align}
        &= \frac{1}{\dim^2 - 1} \sum_{a, b, c, e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \parens{\delta_{ac} \delta_{ce} + \delta_{ae} - \frac{1}{\dim} \parens{\delta_{ac} \delta_{ce} + \delta_{ae}}}  \\
        &= \frac{1}{\dim^2 - 1} \parens{1 - \frac{1}{\dim}} \sum_{a, b, c, e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \delta_{ae} (1 + \delta_{ac}) \\
        &= \frac{1}{\dim^2 - 1} \parens{1 - \frac{1}{\dim}} \sum_{a, b, c} \ketbra{a}{a} e^{i (\lambda(a) - \lambda(c))(x-y)} (1 + \delta_{ac}) \\
        &= \frac{1 \parens{\dim - 1}}{\dim^2 - 1} \sum_{a,c} \ketbra{a}{a} e^{i (\lambda(a) - \lambda(c))(x - y)} (1 + \delta_{ac}) \\
        &= \frac{1}{\dim + 1} \parens{\sum_{a,c} e^{i (\lambda(a) - \lambda(c))(x-y)} \ketbra{a}{a} + \identity}.
    \end{align}
    Reindexing by $a \mapsto i,j$, $c \mapsto k,l$, and plugging in the definition of $\Delta$ yields the statement of the lemma.
\end{proof}


\begin{lemma} \label{lem:sandwiched_interaction}
    Given two Heisenberg evolved random interactions, $G(x)$ and $G(y)$, we compute their action on an outer-product $\ketbra{i,j}{k,l}$ as
    \begin{equation}
        \int G(x) \ketbra{i,j}{k,l} G(y) ~dG = \frac{1}{\dim + 1} \parens{\ketbra{i,j}{k,l} + \delta(i,j|k,l) \sum_{m,n} e^{i \Delta(m,n | i,j) (x-y)} \ketbra{m,n}{m,n}}
    \end{equation}
\end{lemma}
\begin{proof}
This proof is structured the same as Lemma \ref{lem:two_heisenberg_interactions} and similarly we will use a single index of the total Hilbert space $\hilb$ and switch to two indices to match the rest of the exposition.
    \begin{align}
        \int G(x) \ketbra{a}{b} G(y) dG &=  \int e^{i H x} U_G D U_G^{\dagger} e^{-i H x} \ketbra{a}{b} e^{i H y} U_G D U_G^\dagger e^{-i H y} ~dG \\
        &= \sum_{c, d, e, f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \nonumber \\
        &\quad \times \int \ketbra{c}{c} U_G D(d) \ketbra{d}{d} U_G^\dagger \ketbra{a}{b} U_G D(e) \ketbra{e}{e} U_G^\dagger \ketbra{f}{f} dG \\
        &= \sum_{c, d, e, f}  e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} \nonumber \\
        &\quad \times \int D(d) D(e) dD \int \bra{c} U_G \ket{d} \bra{b} U_G \ket{e} \bra{d} U_G^\dagger \ket{a} \bra{e} U_G^\dagger \ket{f} dU_G \\
        &=  \sum_{c,d,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} \nonumber \\ 
        &\quad \times \int \bra{c} U_G \ket{d} \bra{b} U_G \ket{d} \bra{a} \overline{U_G} \ket{d} \bra{f} \overline{U_G} \ket{d} dU_G \\
        &= \frac{1}{\dim^2 - 1} \sum_{c,d,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} (\delta_{ca} \delta_{bf} + \delta_{cf}\delta_{ab})\parens{1 - \frac{1}{\dim}} \\
        &= \frac{1}{\dim + 1} \sum_{c,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} (\delta_{ca} \delta_{bf} + \delta_{cf}\delta_{ab}) \\
        &= \frac{1}{\dim + 1} \parens{\ketbra{a}{b} + \delta_{ab} \sum_{c} e^{i(\lambda(c) - \lambda(a))(x-y)} \ketbra{c}{c} }.
    \end{align}
    Now re-indexing by $a \mapsto (i,j)$, $b \mapsto (k,l)$ and $c \mapsto (m,n)$ results in the expression given in the statement of the lemma.
\end{proof}


\begin{lemma}[Second Order Correction] \label{lem:the_double_duhamel}
    Given a system Hamiltonian $H_{S}$, an environment Hamiltonian $H_{E}$, a simulation time $t$, and coupling coefficient $\alpha$, let $\Phi_G : \hilb_S \otimes \hilb_E \to \hilb_S \otimes \hilb_E$ denote the fixed interaction channel 
    \begin{equation}
        \Phi_G(\rho) = e^{+i (H + \alpha G)t} \rho e^{-i (H + \alpha G)t},
    \end{equation}
    where $H = H_S \otimes \identity + \identity \otimes H_E$. We compute the output of the averaged channel at $\alpha = 0$ for the basis $\ketbra{a}{b}$ of linear operators as:
 \begin{align}
     &\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{k,l})\bigg|_{\alpha = 0} dG \\
     &= -\frac{2  e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2} \nonumber \\
     &~+ \sum_{(a,b): \Delta(k,l|a,b) \neq 0} \frac{1 + i \Delta(k,l|a,b) t - e^{i \Delta(k,l|a,b) t}}{\Delta(k,l|a,b)^2} + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) \ketbra{i,j}{k,l} \nonumber \\
    &~ +\delta_{i,k} \delta_{j,l} \frac{2 e^{i \Delta(i,j|k,l)t}}{\dim+1} \parens{ \sum_{(a,b): \Delta(i,j|a,b) \neq 0 } \frac{2(1- \cos (\Delta(i,j|a,b)t))}{\Delta(i,j|a,b)^2} \ketbra{a,b}{a,b} + t^2 \sum_{(a,b) : \Delta(i,j|a,b) = 0} \ketbra{a,b}{a,b}}
 \end{align}
\end{lemma}
\begin{proof}
To start we would like to note that we will use a single index notation to refer to the joint system-environment eigenbasis during this proof to help shorten the already lengthy expressions. We will convert back to a double index notation to match the statement of the theorem. We start from the expression for the first derivative of the channel $\frac{\partial}{\partial \alpha} \Phi_G(\rho_S)$ given by Eq. \eqref{eq:first_order_alpha_derivative}. To take the second derivative there are six factors involving $\alpha$, so we will end up with six terms. We repeat Eq. \eqref{eq:first_order_alpha_derivative} below, add a derivative, and label each factor containing an $\alpha$ for easier computation
\begin{align}
    \frac{\partial^2}{\partial \alpha^2} \Phi_G(\rho_S) =& \frac{\partial}{\partial \alpha} \parens{\int_{0}^{1} \underset{\substack{\downarrow \\ (A)}}{e^{i s (H+\alpha G)t}} (i t G) \underset{\substack{\downarrow \\ (B)}}{e^{i (1-s) (H+\alpha G)t}} ds ~ \rho \underset{\substack{\downarrow \\ (C)}}{e^{-i(H+\alpha G)t}} } \nonumber \\
    &~ ~+\frac{\partial}{\partial \alpha} \parens{ \underset{\substack{\downarrow \\ (D)} }{e^{i(H+\alpha G)t}} \rho \int_{0}^1 \underset{\substack{\downarrow \\ (E)} }{e^{-i s (H+\alpha G) t} } (- i t G) \underset{\substack{\downarrow \\ (F)}}{e^{-i (1-s) (H+\alpha G)t}} ds }.
\end{align}
Our goal is to get each of these terms in a form in which we can use either Lemma \ref{lem:two_heisenberg_interactions} or \ref{lem:sandwiched_interaction}. 
\begin{align}
    (A) &=i t\int_0^1 \parens{\frac{\partial}{\partial \alpha} e^{i s_1 (H+ \alpha G)t}} G e^{i(1-s_1)(H+\alpha G)t} ds_1 \rho e^{-i (H+\alpha G)t} \bigg|_{\alpha=0} \\
    &= (it)^2 \int_0^1 \parens{\int_0^1 e^{i s_1 s_2 (H+\alpha G)t} s_1 G e^{i s_1 (1-s_2) (H+\alpha G)t} ds_2} G e^{i(1-s_1) (H+\alpha G)t} ds_1 \rho e^{-i(H+\alpha G) t} \bigg|_{\alpha=0} \label{eq:second_order_deriv_intermediate_a}\\
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 s_2 H t} G e^{-i s_1 s_2 H t} e^{i s_1 H t} G e^{-i s_1 H t} s_1 ds_1 ds_2 e^{i H t} \rho e^{-i H t} \\
    &= -t^2 \int_0^1 \int_0^1 G(s_1 s_2 t) G(s_1 t) s_1 ds_1 ds_2 \rho(t). \label{eq:second_deriv_alpha_first_term}
\end{align}

\begin{align}
    (B) &= it \int_0^1 e^{i s_1 (H + \alpha G)t} G \frac{\partial}{\partial \alpha}\parens{e^{i(1-s_1)(H + \alpha G)t}} ds_1 \rho e^{-i(H + \alpha G) t} \bigg|_{\alpha = 0} \\
    &= (it)^2 \int_0^1 e^{i s_1 (H + \alpha G)t} G \parens{\int_0^1 e^{i(1-s_1)s_2 (H + \alpha G)t} (1-s_1) G e^{i(1 - s_1)(1 - s_2)(H + \alpha G)t} ds_2} ds_1 ~ \rho e^{-i ( H + \alpha G)t} \bigg|_{\alpha = 0} \\
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 H t} G e^{i(1-s_1)s_2 H t} G e^{i(1-s_1)(1-s_2) H t} (1-s_1) ds_1 ds_2 ~ \rho e^{-i H t}\\ 
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 H t} G e^{-i s_1 H t} e^{i(s_1 + s_2 - s_1 s_2) H t} G e^{-i (s_1 + s_2 - s_1 s_2) H t} (1-s_1) ds_1 ds_2 ~ \rho(t) \\
    &= -t^2 \int_0^1 \int_0^1 G(s_1 t) G((s_1 + s_2 - s_1 s_2)t) (1-s_1) ds_1 ds_2 ~ \rho(t)
\end{align}

\begin{align}
    (C) &= it \int_0^1 e^{i s (H + \alpha G)t} G e^{i(1-s) (H + \alpha G) t} ds ~\rho ~ \frac{\partial}{\partial \alpha} \parens{ e^{-i (H + \alpha G) t} } \bigg|_{\alpha = 0} \\
    &= (i t) (-it) \int_0^1 e^{i s (H + \alpha G)t} G e^{i (1 - s) (H + \alpha G)t} ds ~ \rho ~ \parens{ \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1- s) ( H + \alpha G)t } ds}\bigg|_{\alpha = 0} \\
    &= + t^2 \parens{\int_0^1 e^{i s H t} G e^{-i s H t} ds} e^{i H t} \rho e^{-i H t} \parens{\int_0^1 e^{i (1-s) H t} G e^{-i (1-s) H t} ds} \\
    &= + t^2 \int_0^1 G(st) ds ~ \rho(t) \int_0^1 G((1-s)t) ds
\end{align}

\begin{align}
    (D) &= (-it) \frac{\partial}{\partial \alpha} \parens{e^{i(H + \alpha G)t}} \rho \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1-s)(H + \alpha G)t} ds \bigg|_{\alpha = 0} \\
    &= t^2 \parens{\int_0^1 e^{i s (H+ \alpha G)t} G e^{i (1-s) (H + \alpha G)t}ds} \rho \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1-s)(H + \alpha G)t} ds \bigg|_{\alpha = 0} \\
    &=  t^2 \int_0^1 e^{i s H t} G e^{-i s H t} ds ~\rho(t) \int_0^1 e^{i (1-s) H t} G e^{-i (1-s) H t} ds \\
    &= t^2 \int_0^1 G(st) ds ~ \rho(t) ~ \int_0^1 G((1-s)t) ds
\end{align}

\begin{align}
    (E) &= (-it) e^{i (H+ \alpha G) t} ~ \rho ~\int_0^1 \frac{\partial}{\partial \alpha} \parens{e^{-i s_1 (H + \alpha G)t}} G e^{-i (1-s_1)(H + \alpha G)t} ds_1 \bigg|_{\alpha = 0} \\
    &= - t^2 e^{i(H + \alpha G)t} ~ \rho ~\int_0^1 \parens{\int_0^1 e^{-i s_1 s_2 (H + \alpha G) t} (s_1 G) e^{-i s_1 (1-s_2) (H + \alpha G)t} ds_2} G e^{-i(1-s_1)(H + \alpha G)t} ds_1 \bigg|_{\alpha = 0} \\
    &= -t^2 e^{i H t} \rho e^{-i H t} \int_0^1 \int_0^1 e^{i (1 - s_1 s_2) H t} G e^{-i (s_1 - s_1 s_2)H t} G e^{-i (1-s_1)H t} s_1 ds_1 ds_2 \\
    &= -t^2 \rho(t) \int_0^1 \int_0^1 G((1- s_1 s_2) t) G((1-s_1)t) s_1 ds_1 ds_2
\end{align}

\begin{align}
    (F) &= (-it) e^{i(H + \alpha G) t} \rho \int_0^1 e^{-i s_1 ( H + \alpha G) t} G \frac{\partial}{\partial \alpha} \parens{ e^{-i (1-s_1) ( H +\alpha G)t}} ds_1 \bigg|_{\alpha = 0} \\
    &= (-it)^2 e^{i (H + \alpha G)t} \rho \int_0^1 e^{-i s_1 (H + \alpha G)t} G \parens{\int_0^1 e^{-i(1-s_1) s_2 (H + \alpha G)t} (1-s_1) G e^{-i(1-s_1) (1-s_2) (H + \alpha G) t} ds_2} ds_1 \bigg|_{\alpha = 0} \\
    &= -t^2 e^{-i H t} \rho e^{-i H t} \int_0^1 \int_0^1 e^{i (1- s_1) H t} G e^{-i (1-s_1) H t} e^{i (1-s_1)(1-s_2) H t} G e^{-i(1-s_1)(1-s_2) H t} (1-s_1) ds_1 ds_2 \\
    &= -t^2 \rho(t) \int_0^1 \int_0^1 G((1-s_1)t) G((1-s_1)(1 - s_2) t) (1-s_1)ds_1 ds_2
\end{align}

Now our goal is to compute the effects of averaging over the interaction $G$ on the above terms, starting with $(A)$. As this involves a lot of index manipulations, similarly to the proofs of Lemmas \ref{lem:two_heisenberg_interactions} and \ref{lem:sandwiched_interaction} we will use a single index for the total system-environment Hilbert space and switch back to a double index to state the results. We will make heavy use of Lemma \ref{lem:two_heisenberg_interactions}.
\begin{align}
    \int (A) dG &= -t^2 \int_0^1 \int_0^1 \int G(s_1 s_2 t) G(s_1 t) dG s_1 ds_1 ds_2 \rho(t) \\
    &= \frac{-t^2 }{\dim + 1} \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i (\lambda(i) - \lambda(j)) (s_1 s_2 t - s_1 t)} \ketbra{i}{i} + \identity} s_1 ds_1 ds_2 \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \int_0^1 \int_0^1 e^{i(\lambda(i) - \lambda(j))t (s_1 s_2 - s_1)} s_1 ds_1 ds_2 \ketbra{i}{i} + \sum_{i} \sum_{j : \lambda(i) = \lambda(j)}\frac{1}{2} \ketbra{i}{i} + \frac{1}{2} \identity} \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 - i (\lambda(i) - \lambda(j))t - e^{-i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i} } \rho(t) \\
    &= \frac{- 1}{\dim + 1}\parens{\sum_{i} \sum_{j: \Delta_{ij} \neq 0} \frac{1 - i \Delta_{ij}t - e^{-i \Delta_{ij} t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2} \sum_{i} (\eta(i) + 1)\ketbra{i}{i} } \rho(t)
\end{align}

We can similarly compute the averaged $(B)$ term:
\begin{align}
    \int (B) dG &= -t^2 \int_0^1 \int_0^1 \int G(s_1 t) G((s_1 + s_2 - s_1 s_2) t) dG (1-s_1) ds_1 ds_2 ~ \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i (\lambda(i) - \lambda(j))(s_1 s_2 - s_2) t} \ketbra{i}{i} + \identity} (1 -s_1) ds_1 ds_2 \rho \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \int_0^1 \int_0^1 e^{i(\lambda(i) - \lambda(j))t (s_1 s_2 - s_2)} (1 - s_1) ds_1 ds_2 \ketbra{i}{i} + \sum_{i} \sum_{j : \lambda(i) = \lambda(j)}\frac{1}{2} \ketbra{i}{i} + \frac{1}{2} \identity} \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 - i (\lambda(i) - \lambda(j))t - e^{-i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i} } \rho(t) \\
    &= \frac{-1}{\dim + 1}\parens{\sum_{i} \sum_{j: \Delta_{ij} \neq 0} \frac{1 - i \Delta_{ij}t - e^{-i \Delta_{ij} t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2} \sum_{i} (\eta(i) + 1)\ketbra{i}{i} } \rho(t),
\end{align}
which we note is identical to $\int (A) dG$. As terms $(C)$ and $(D)$ involve a different method of computation we skip them for now and compute $(E)$ and $(F)$. 
\begin{align}
    \int (E) dG &= -t^2 \rho(t) \int_0^1 \int_0^1 \int G((1- s_1 s_2) t) G((1-s_1)t) dG s_1 ds_1 ds_2 \\
    &= \frac{- t^2}{\dim + 1} \rho(t) \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i(\lambda(i) - \lambda(j)) t (s_1 - s_1 s_2)} \ketbra{i}{i} + \identity } s_1 ds_1 ds_2 \\
    &= \frac{- t^2}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 + i (\lambda(i) - \lambda(j))t - e^{i(\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2}\ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1 )\ketbra{i}{i}} \\
    &= \frac{- 1}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j: (\Delta_{ij} \neq 0)} \frac{1 + i \Delta_{ij}t - e^{i\Delta_{ij}t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2}\sum_i (\eta(i) + 1) \ketbra{i}{i}}.
\end{align}
Computing $(F)$ yields
\begin{align}
    \int (F) dG &= -t^2 \rho(t) \int_0^1 \int_0^1 \int G((1-s_1)t) G((1-s_1)(1 - s_2) t) dG (1-s_1)ds_1 ds_2 \\
    &= \frac{- t^2 \sigma ^2}{\dim + 1} \rho(t) \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i(\lambda(i) - \lambda(j))t (s_2 - s_1 s_2)}\ketbra{i}{i} + \identity} (1-s_1) ds_1 ds_2 \\
    &= \frac{- t^2 }{\dim + 1} \rho(t) \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 + i (\lambda(i) - \lambda(j))t - e^{i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} +\frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i}} \\
    &= \frac{- 1}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j: (\Delta_{ij} \neq 0)} \frac{1 + i \Delta_{ij}t - e^{i\Delta_{ij}t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2}\sum_i (\eta(i) + 1) \ketbra{i}{i}}
\end{align}
 which is identical to $\int (E) dG$.

 The last two terms $(C) = (D)$ are computed as follows:
 \begin{align}
     \int (C) dG &= t^2 \int_0^1 \int_0^1 \int G(s_1 t) \rho(t) G((1-s_2)t) ~dG ~ ds_1 ds_2 \\
     &= t^2 \sum_{i,j} \rho_{ij} e^{i(\lambda(i) - \lambda(j))t} \int_0^1 \int_0^1 \int G(s_1 t) \ketbra{i}{j} G((1-s_2)t) ~ dG ~ ds_1 ds_2 \\
     &= \frac{ t^2}{\dim + 1} \sum_{i,j} \rho_{ij} e^{i(\lambda(i) - \lambda(j))t} \parens{ \ketbra{i}{j} + \delta_{ij} \sum_{a} \int_0^1 \int_0^1 e^{i(\lambda(a) - \lambda(i))(s_1 + s_2 - 1)t} ds_1 ds_2 \ketbra{a}{a}} \\
     &= \frac{ t^2}{\dim + 1} \sum_{i,j} \rho_{ij} e^{i \Delta_{ij} t} \parens{\ketbra{i}{j} + \delta_{ij} \sum_{a : \Delta_{ai} \neq 0} \frac{2( 1- \cos (\Delta_{ai} t))}{\Delta_{ai}^2 t^2} \ketbra{a}{a} + \delta_{ij} \sum_{a : \Delta_{ai} = 0} \ketbra{a}{a}}
 \end{align}

 We can now combine each of these terms to offer the full picture of the output of the channel to second order. We make two modifications to the results from each sum: first, we will switch to double index notation to make for easier use in other areas, and secondly we let $\rho = \ketbra{i,j}{k,l}$. We note that the first term in the following equation is provided by $(A) + (B)$, the second through $(E) + (F)$, and the last two through $(C) + (D)$. 
 \begin{align}
     &\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{k,l})\bigg|_{\alpha = 0} dG \\
     &= -\frac{2  e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2} \nonumber \\
     &~+ \sum_{(a,b): \Delta(k,l|a,b) \neq 0} \frac{1 + i \Delta(k,l|a,b) t - e^{i \Delta(k,l|a,b) t}}{\Delta(k,l|a,b)^2} + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) \ketbra{i,j}{k,l} \nonumber \\
    &~ +\delta_{i,k} \delta_{j,l} \frac{2 e^{i \Delta(i,j|k,l)t}}{\dim+1} \parens{ \sum_{(a,b): \Delta(i,j|a,b) \neq 0 } \frac{2(1- \cos (\Delta(i,j|a,b)t))}{\Delta(i,j|a,b)^2} \ketbra{a,b}{a,b} + t^2 \sum_{(a,b) : \Delta(i,j|a,b) = 0} \ketbra{a,b}{a,b}} \label{eq:second_order_output}
 \end{align}
\end{proof}

The goal for the remainder of this section is to simplify the expression for the channel output given by the above lemma. The first thing to note is that our channel does not appear to have large off-diagonal contributions to second order in $\alpha$. 
\begin{corollary}
    Given the inputs to Lemma \ref{lem:the_double_duhamel} and a diagonal input state $\rho = \sum_{i,j} \rho_{i,j} \ketbra{i,j}{i,j}$, then we have $$\int \bra{k,l} \Phi_G(\rho)  \ket{m,n} ~dG~ \in \bigo{\alpha^3},$$ for $(k,l) \neq (m,n)$.
\end{corollary}
\begin{proof}
    We start from the Taylor's series of $\Phi_G$ with respect to $\alpha$:
    \begin{equation}
        \Phi_G(\rho) = \rho + \frac{\alpha^2}{2!} \int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\rho) dG + \bigo{\alpha^3}. \label{eq:off_diagonal_taylors}
    \end{equation}
    First we note that $\bra{k,l}\rho \ket{m,n} =0$, as $\rho$ is diagonal and we assumed that $(k,l) \neq (m,n)$. Substituting Lemma \ref{lem:the_double_duhamel} for the second order derivative and taking matrix elements yields:
    \begin{align}
        &\bra{k,l} \int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\rho)\bigg|_{\alpha = 0} dG ~dG ~\ket{m,n}  \\
     &= -\frac{2 }{\dim + 1} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2} \nonumber \\
     &~+ \sum_{(a,b): \Delta(k,l|a,b) \neq 0} \frac{1 + i \Delta(k,l|a,b) t - e^{i \Delta(k,l|a,b) t}}{\Delta(k,l|a,b)^2} + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) \bra{k,l} \rho \ket{m,n} \nonumber \\
    &~ + \frac{2}{\dim+1}\sum_{i,j} \rho_{i,j} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0 } \frac{2(1- \cos (\Delta(i,j|a,b)t))}{\Delta(i,j|a,b)^2} \braket{k,l}{a,b} \braket{a,b}{m,n} \nonumber \\
    &~ + t^2 \sum_{(a,b) : \Delta(i,j|a,b) = 0} \braket{k,l}{a,b} \braket{a,b}{m,n} \bigg).
    \end{align}
    We see that the factors $\bra{k,l}\rho \ket{m,n}$ and $\braket{k,l}{i,j} \braket{i,j}{m,n}$ can never be non-zero due to the assumption $(k,l) \neq (m,n)$. The only remaining term in Eq. \ref{eq:off_diagonal_taylors} is of $\bigo{\alpha^3}$, yielding the stated result.
\end{proof}

Our last objective in this section is to yield a (relatively) concise statement for transition probabilities among diagonal matrix elements for density matrices. As we will be interested in computing trace distances we will be interested in 
\begin{theorem} \label{thm:second_order_transition_coeffs}
Given the inputs to Lemma \ref{lem:the_double_duhamel}, let 
$$\tau(i,j | k,l) \coloneqq \frac{\alpha^2 }{2} \bra{k,l} \int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{i,j})\bigg|_{\alpha = 0} dG \ket{k,l},$$ denote the second order transition coefficients in $\alpha$. These are expressed in terms of the eigenvalue differences $\Delta(i',j'|k',l')$ and other constants as 
$$\tau(i,j | k,l) = \begin{cases}
    - \frac{\alpha^2 t^2 }{\dim + 1} \parens{\sum_{(a,b) : \Delta(i,j | a,b) \neq 0} \frac{\sinc^2(\Delta(a,b|i,j) t}{2} + (\eta(i,j) - 1)} & (i,j) = (k,l) \\
    \frac{\alpha^2 t^2}{\dim + 1} & (i,j) \neq (k,l), \Delta(i,j | k,l) = 0 \\
    \frac{\alpha^2 t^2 }{\dim + 1} \sinc^2(\Delta t /2) & \Delta(i,j| k,l) \neq 0.
\end{cases}$$
. A summation shows that $\tau(i,j|i,j) = -\sum_{(k,l) \neq (i,j)} \tau(i,j|k,l)$, which as a byproduct shows that the mapping $\Phi$ is trace preserving to order $\bigo{\alpha^2}$.
\end{theorem}
\begin{proof}
    We focus exclusively on diagonal inputs and outputs for density matrices, $\ketbra{a}{a}$ and $\ketbra{b}{b}$. We use Eq. \eqref{eq:second_order_output}, starting with transitions within the degenerate subspace of $a$. For the following we assume $a \neq b$ and $\Delta_{ab} = 0$: 
    \begin{align}
        &\prob{a \to b | a \neq b, \Delta_{ab} = 0} = \trace{\ketbra{b}{b} \int \Phi_G(\ketbra{a}{a}) dG} \\
        &= \braket{b}{a} \braket{a}{b} + \frac{\alpha^2}{2} \bra{b} \int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{a}{a})\bigg|_{\alpha = 0} dG \ket{b} \\
        &= -\frac{\sigma^2 \alpha^2}{\dim + 1} \bigg(\sum_{j: \Delta_{aj} \neq 0} \frac{1 - i \Delta_{aj}t - e^{-i \Delta_{aj} t}}{\Delta_{aj}^2} + \sum_{j: \Delta_{aj} \neq 0} \frac{1 + i \Delta_{aj} t - e^{i \Delta_{aj} t}}{\Delta_{aj}^2} + t^2 \eta(a) \bigg) \braket{b}{a} \braket{a}{b} \nonumber \\
        &~+ \frac{\alpha^2 \sigma^2}{\dim + 1} \parens{\sum_{i: \Delta_{ai} \neq 0} \frac{2(1- \cos(\Delta_{ai} t)}{\Delta_{ai}^2} \braket{b}{i}\braket{i}{b} + t^2 \sum_{i: \Delta_{ai} = 0 } \braket{b}{i} \braket{i}{b}} \label{eq:transition_intermediate} \\
        &= \frac{\sigma^2 \alpha^2 t^2}{\dim + 1}. 
    \end{align}
    We now proceed to the case that $b \neq a$ and $\Delta_{ab} \neq 0$, where we can start from Eq. \eqref{eq:transition_intermediate} and we get
    \begin{equation}
        \prob{a \to b | a \neq b, \Delta_{ab} \neq 0} = \frac{2 \sigma^2 \alpha^2 }{\dim + 1} \frac{1 - \cos (\Delta_{ab} t)}{\Delta_{ab}^2}.
    \end{equation}

    The remaining case to consider is when $b = a$. This involves simplifying the summations in Eq. \eqref{eq:second_order_output} and including the zeroth order $\bigo{\alpha^0}$ term which simply contributes a 1. 
    \begin{align}
        \prob{a \to a} &= 1 -\frac{\sigma^2 \alpha^2 }{\dim + 1} \bigg(\sum_{j: \Delta_{aj} \neq 0} \frac{1 - i \Delta_{aj}t - e^{-i \Delta_{aj} t}}{\Delta_{aj}^2} + \sum_{j: \Delta_{aj} \neq 0} \frac{1 + i \Delta_{aj} t - e^{i \Delta_{aj} t}}{\Delta_{aj}^2} + t^2 \eta(a) \bigg) \nonumber \\
    &~ +\frac{\sigma^2 \alpha^2 }{\dim+1} \parens{ \sum_{i: \Delta_{ai} \neq 0 } \frac{2(1- \cos (\Delta_{ai}t))}{\Delta_{ai}^2} \braket{a}{i} \braket{i}{a} + t^2  \sum_{i : \Delta_{ai} = 0} \braket{a}{i} \braket{i}{a}} \\
    &= 1 - \frac{\sigma^2 \alpha^2}{\dim + 1} \parens{\sum_{i: \Delta_{ai} \neq 0} \frac{2(1 - \cos(\Delta_{ai}t))}{\Delta_{ai}^2} + t^2 (\eta(a) - 1)}.
    \end{align}
    To simplify further we reduce the cosine terms to a sinc function as follows
    \begin{align}
        \frac{2(1 - \cos(\Delta t)}{\Delta^2} &= \frac{2(1 - \cos^2(\Delta t / 2) + \sin^2(\Delta t /2)}{\Delta^2} \\
        &= \frac{t^2}{2} \frac{\sin^2 (\Delta t / 2)}{ (\Delta t/ 2)^2} \\
        &= \frac{t^2}{2} \sinc^2(\Delta t /2).
    \end{align}
    This completes the transition probability computation. It is straightforward to see that $\sum_{b} \prob{a \to b} = 1$ and that $0 \leq \prob{a \to b} \leq 1$ given that $\alpha t \leq \sqrt{\dim + 1}$.
\end{proof}

\subsection{Remainder Bound}
We now aim to bound the spectral norm of the remainder term $R_{\Phi}$. This is rather tedious, as even to third order in $\alpha$ we have 24 terms to bound. For example, looking first at the $(A)$ term resulting from the second order derivative in Eq. \eqref{eq:second_order_deriv_intermediate_a} we have 4 multiplicative factors involving $\alpha$, leading to 4 terms from this single term of the second order derivative. As there are six terms in total, this yeilds 24 terms. We can profit from the fact though that the expressions for the second order derivatives do simplify and the final expression only has 3 terms, as two of the derivatives act similarly. We first will analyze the single term from Eq. \eqref{eq:second_order_deriv_intermediate_a} in detail. 


\begin{theorem}
    Let $\Phi(\rho)$ be the thermalizing channel as defined in Def. \eqref{eq:phi_g_definition} and $R_{\Phi}$ the remainder term 
    \begin{equation}
        R_{\Phi}(\rho, \alpha) = \Phi(\rho, \alpha) - \rho - \frac{\alpha}{1!} \frac{\partial}{\partial \alpha} \Phi(\rho)\bigg|_{\alpha = 0} - \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0}.
    \end{equation}
    Then we have that
    \begin{equation}
        \alpha t \leq \parens{\frac{\epsilon_R}{2 \dim}}^{1/3} \implies \norm{R_{\Phi}} \leq \epsilon_R.
    \end{equation}
\end{theorem}
\begin{proof}
    First I need to write down what exactly we are trying to bound. What we are essentially trying to do is bound the matrix entries:
\begin{equation}
    \Phi(\rho) = \rho + \frac{\alpha}{1} \frac{\partial}{\partial \alpha} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^3}{3!} \frac{\partial^3}{\partial \alpha^3} \Phi(\rho) \bigg|_{\alpha = \alpha^\star},
\end{equation}
where $\alpha^\star \in [0, \alpha]$. Our goal is then to bound the spectral norm of the remainder term for all such $\alpha^\star$.

\begin{align}
    &\norm{\frac{\partial}{\partial \alpha} (it)^2 \int_0^1 \parens{\int_0^1 e^{i s_1 s_2 (H+\alpha G)t} s_1 G e^{i s_1 (1-s_2) (H+\alpha G)t} ds_2} G e^{i(1-s_1) (H+\alpha G)t} ds_1 \rho e^{-i(H+\alpha G) t}} \\
    &=t^3 \norm{\int_0^1 \int_0^1 \int_0^1 e^{i s_1 s_2 s_3(H + \alpha G)t} G e^{i s_1 s_2 (1 - s_3)} ds_3 G e^{i s_1 (1-s_2)(H + \alpha G)t} ds_2 G e^{i(1-s_1)(H + \alpha G)t} ds_1 \rho e^{-i(H + \alpha G)t} }  + t^3 \norm{\ldots}\\
    &\leq t^3 \norm{G}^3 + t^3 \norm{\ldots} \\
    &\leq 4 t^3 \norm{G}^3,
\end{align}
where we used smoothness of the integrand to bring the norm inside the integral via the triangle inequality and submultiplicativity of the operator norm to simply break the norm of the product into the product of each of the norms. The operator norm of the unitary operators and the density matrix are each 1, and the resulting integrals yield only fractional values, which are upper bounded by 1. Now here we see the final simplification that can be made, each term $(B), (C)$, etc., will yield at most a cubic power of $\norm{G}$, so we can upper bound each term in the final sum as $t^3 \norm{G}^3$. In reality we would have terms such as $\norm{G} \norm{G^3}$ and all other polynomials, but we don't care.

Now as $G$ is a random matrix, we can only bound it's operator norm with a probabilistic guarantee. 

This gives
\begin{equation}
    \norm{\Phi(\rho) - \rho - \sum_{i,j,k,l} \ketbra{j}{j} \bra{i} \rho \ket{i} \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} \tau(i,k | j, l) } \leq 4 \alpha^3 t^3 \int \norm{G}^3 dG
\end{equation}

We now need to bound $\int \norm{G}^3 dG$. This is a fun probability question. First we get rid of the Haar integrals
\begin{align}
    \int \norm{G}^3 dG &= \int \int \norm{U_G D U_G^\dagger}^3  dU dD \\
    &= \int \norm{D}^3 dD \\
    &= \expect{\max_i |d_i|^3}
\end{align}

It is pretty straightforward to upper bound this integral by $\int \norm{D}^3 dD \leq 2 \dim$. This then gives
\begin{equation}
    \norm{R_{\Phi}} \leq 8 \alpha^3 t^3 \dim.
\end{equation}
So this says we need $\alpha t \leq \frac{\epsilon_{R}^{1/3}}{2 \dim^{1/3}}$ in order for $\norm{R_{\Phi}} \leq \epsilon_R$.
\end{proof}

\textcolor{red}{TODO: Change this to be the norm of the average instead of the average of the norm. The norm of the average should be reducible to the third moment of the distribution of eigenvalues. Even then though we might pick up dimensionful factors because we have to do a spectral decomposition of each random matrix $D$. This then introduces a sum over all 3 diagonals. We can use independence to get rid of two of them, but the last remaining one I'm not sure how to, as we have projectors interspersed throughout the factors. Using triangle inequality and submultiplicativity gives you a factor of dimension, in which case we are just as good with our dumb bound of the average of the norm cubed.}
\todo{Fix the definitions used here.}

\section{Generalizations to Many Qubits}

This introduction part needs to introduce the setup that will be used for these two proofs.

\begin{lemma}
    \begin{equation}
        \sum_{i,j,l} a(i) b(j) \tau(i,j | k,l) = \sum_{i \neq k} \sum_{j,l} (a(i) b(j) - a(k) b(l)) \tau(i,j |k,l)
    \end{equation}
\end{lemma}
\begin{proof}
    \begin{align}
        \sum_{i,j,l} a(i) b(j) \tau(i,j | k,l) &= \sum_{i \neq k} \sum_{j,l} \parens{a(i) b(j) \tau(i,j|k,l)} + \sum_{j,l} a(k) b(j) \tau(k,j | k,l) \label{eq:n_qubit_fixed_pt_intermediate_1}.
    \end{align}
    We expand the simpler sum on the right:
    \begin{align}
        \sum_{j,l}a(k) b(j) \tau(k,j| k,l) &= a(k) b(0) (\tau(k,0|k,0) + \tau(k,0|k,1)) + a(k) b(1) (\tau(k,1|k,0) + \tau(k,1|k,1)) \\
        &= - a(k) b(0) \sum_{c \neq k}\sum_{l} \tau(k,0 | c, l) - a(k) b(1) \sum_{c \neq k} \sum_{l}\tau(k,1|c,l) \\
        &= - \sum_{c \neq k} \sum_{j,l} a(k) b(j) \tau(k,j |c,l).
    \end{align}
    Plugging this into Eq. \ref{eq:n_qubit_fixed_pt_intermediate_1} allows us to simplify as follows
    \begin{align}
        \sum_{i,j,l} a(i) b(j) \tau(i,j|k,l) &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) \tau(i,j|k,l)) - \sum_{c \neq k} \sum_{j,l} a(k) b(j) \tau(k,j|c,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) \tau(i,j|k,l)) - \sum_{i \neq k} \sum_{j,l} a(k) b(l) \tau(i,j|k,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) - a(k) b(l)) \tau(i,j | k,l).
    \end{align}
\end{proof}

\begin{theorem}
    Using $H_S, H_E, \gamma, G, \epsilon_{\sinc}$ and $\rho_E$ as defined above. If $\epsilon_{\sinc} \leq \frac{1}{4}$ (or alternatively $t \geq \frac{2}{\Delta_{\min}}$), then following statement holds
    \begin{equation}
        \alpha t \leq \sqrt{2} \epsilon_{fix}^{1/4} \sqrt{1 + \frac{1}{\dim}} \implies \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_2^2 \leq \epsilon_{fix}
    \end{equation}
\end{theorem}
\begin{proof}
Our goal is to show that our channel has $\rho_S(\beta_E)$ as a fixed point and that it reduces the distance to the fixed point at different temperatures. We start from Eq. \ref{eq:trace_dist_p_norm} with the goal of showing that $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_2^2 \leq \epsilon$. This reduces to showing
$$\sum_k \abs{\sum_{i,j,l} \frac{e^{-\beta_E \lambda(i,j)}}{\partfun(\beta_E)} \tau(i,j | k,l)}^2 \leq \epsilon.$$ 
\inlinetodo{Need to include bounds on the remainder term now that we have it. It shouldn't be too hard, just set the remainder term error to one half of the error we are trying to bound and then bound the other contribution as one half and then they sum to the error we want.}


The tool we will use to bound this sum is the fact that our map preserves trace to second order, which gives us $\tau(i,j|i,j) = - \sum_{(k,l) \neq (i,j)} \tau(i,j|k,l)$. We look first at a fixed value of $k$ and further let $a(i) = \bra{i} \rho_S \ket{i}$ and $b(j) = \bra{j} \rho_E \ket{j}$, which will allow us to generalize to non-thermal input states. By definition of density matrices we have $0 \leq a(i) \leq 1$ for all $i$ and similarly $0 \leq b(j) \leq 1$ for all $j$. 
\begin{align}
    \sum_{i,j,l} a(i) b(j) \tau(i,j | k,l) &= \sum_{i \neq k} \sum_{j,l} \parens{a(i) b(j) \tau(i,j|k,l)} + \sum_{j,l} a(k) b(j) \tau(k,j | k,l) \label{eq:n_qubit_fixed_pt_intermediate_1}.
\end{align}
We expand the simpler sum on the right:
\begin{align}
    \sum_{j,l}a(k) b(j) \tau(k,j| k,l) &= a(k) b(0) (\tau(k,0|k,0) + \tau(k,0|k,1)) + a(k) b(1) (\tau(k,1|k,0) + \tau(k,1|k,1)) \\
    &= - a(k) b(0) \sum_{c \neq k}\sum_{l} \tau(k,0 | c, l) - a(k) b(1) \sum_{c \neq k} \sum_{l}\tau(k,1|c,l) \\
    &= - \sum_{c \neq k} \sum_{j,l} a(k) b(j) \tau(k,j |c,l).
\end{align}
Plugging this into Eq. \ref{eq:n_qubit_fixed_pt_intermediate_1} allows us to simplify as follows
\begin{align}
    \sum_{i,j,l} a(i) b(j) \tau(i,j|k,l) &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) \tau(i,j|k,l)) - \sum_{c \neq k} \sum_{j,l} a(k) b(j) \tau(k,j|c,l) \\
    &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) \tau(i,j|k,l)) - \sum_{i \neq k} \sum_{j,l} a(k) b(l) \tau(i,j|k,l) \\
    &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) - a(k) b(l)) \tau(i,j | k,l).
\end{align}

We now can bound our desired quantity as follows
\begin{align}
    &\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_2^2 \leq \sum_{k} \abs{\sum_{i \neq k} \sum_{j,l} ((a(i) b(j) -a(k) b(l)) \tau(i,j|k,l)}^2 \\
    &\leq \sum_{j,l,k} \sum_{i \neq k} \tau(i,j|k,l)^2 (a(i) b(j) - a(k) b(l))^2 \\
    &\leq \sum_{j,l,k} \sum_{i \neq k} \tau(i,j|k,l)^2 \\
    &= \sum_{j,l,k} \sum_{i \neq k} \frac{\alpha^4 t^4}{(\dim + 1)^2} \sinc(\Delta(i,j|k,l) t/ 2)^4 \\
    &= \parens{\frac{\alpha^2 t^2}{\dim + 1}}^2 \sum_k \sum_{i \neq k} \parens{2 \sinc\parens{ \frac{\Delta_S(i,k) t}{2}}^2 + \sinc\parens{\frac{(\Delta_S(i,k) - \gamma)t}{2}}^2 + \sinc\parens{\frac{(\Delta_S(i,k) + \gamma)t}{2}}^2} \\
    &\leq \parens{\frac{\alpha^2 t^2}{\dim + 1}}^2 \sum_k \sum_{i \neq k} \parens{3 \epsilon_{\sinc}^2 + \sinc\parens{\frac{(\Delta_S(i,k) - \gamma)t}{2}}^2} \\
    &\leq \parens{\frac{\alpha^2 t^2}{\dim + 1}}^2 (3 \epsilon_{\sinc}^2 \dim_S (\dim_S - 1) + \sum_{k} \sum_{i \neq k} \sinc\parens{\frac{(\Delta_S(i,k) - \gamma)t}{2}}^2.
\end{align}
Now given a specific value of $\gamma$ we can define a ``near-degeneracy" set of indices $S_\gamma$ that contains all indices $i,k$ such that their energy difference is close enough to $\gamma$:
\begin{equation}
    S_\gamma \coloneqq \set{(i,k) : i,k \in \set{1, 2, \ldots, \dim_S}, i \neq k, |\Delta_S(i,k) - \gamma| \leq \Delta_{\min}}.
\end{equation}
The size of this set can vary drastically depending on the eigenvalues of the system Hamiltonian. For example, take the harmonic oscillator with energy difference of 1 and set $\gamma = 1$. Then $S_{\gamma}$ in this case would be of size $|S_{1}| = \dim_S - 1$. On the other hand, take $H_S$ to be the identity, then $S_\gamma$ for any nonzero $\gamma$ is empty. 

Now picking back up from where we left off:
\begin{align}
    &= \parens{\frac{\alpha^2 t^2}{\dim + 1}}^2 (3 \epsilon_{\sinc}^2 \dim_S (\dim_S - 1) + \sum_{(i,k) \in S_{\gamma}} \sinc\parens{\frac{(\Delta_S(i,k) - \gamma)t}{2}}^2 + \epsilon_{\sinc}^2 (\dim_S^2 - |S_{\gamma}|)) \\
    &\leq \parens{\frac{\alpha^2 t^2}{\dim + 1}}^2 \parens{\epsilon_{\sinc}^2 (4 \dim_S^2 - 3 \dim_S - |S_{\gamma}|) + |S_{\gamma}|} \\
    &\leq \parens{\frac{\alpha^2 t^2}{\dim + 1}}^{2} \parens{\epsilon_{\sinc}^2 (4 \dim_S^2 - 3 \dim_S) + \dim_S(\dim_S - 1)} \\
    &\leq \parens{\frac{\alpha^2 t^2}{\dim + 1}}^2 \dim_S^2 (4 \epsilon_{\sinc} + 1). \label{eq:fixed_pt_bound_1}
\end{align}
Now we simply enforce that 
\begin{equation}
    \epsilon_{\sinc} \leq \frac{1}{4}
\end{equation}
so that we can say 
\begin{equation}
    \alpha t \leq \epsilon_{fix}^{1/4} \sqrt{\frac{\dim + 1}{\dim_S}} \implies \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_2^2 \leq \epsilon_{fix}.
\end{equation}
\end{proof}

The next step we need to prove is to find the range of temperatures for which the system is \emph{ not} in an approximate fixed state. This means
\begin{claim}
   If $\beta \geq \beta_{idk}$, then it is not an approximate fixed point. 
\end{claim}
\begin{proof}
    We need to show that there is no way to lower bound the distance of the actual channel, instead we have to lower bound the distance of $\rho_S(\beta_E)$ and the second order approximation to the channel. 
    The following is what we want to show. 
    \begin{align}
        \norm{\rho_S(\beta_E) - \Phi^{(2)}(\rho_S(\beta))}_2^2 \geq \epsilon_{fix}.
    \end{align}
    We use Lemma (insert schatten deconstruction lemma here) to get
    \begin{align}
        \norm{\rho_S(\beta_E) - \Phi^{(2)}(\rho_S(\beta))}_2^2 &= \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} - \sum_{i,j,l} \frac{e^{-\beta \lambda_S(i) - \beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|k ,l)}^2 \\
        &\geq \abs{\frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} - \sum_{i,j,l} \frac{e^{-\beta \lambda_S(i) - \beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)}\tau(i,j|0,l)}^2 \\
        &= \parens{\frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)}}^2 - 2 \parens{\frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)}} \nonumber \\
        &\quad - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \sum_{i,j,l} \frac{e^{-\beta \lambda_S(i) - \beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|0,l) \nonumber \\
        &\quad + \parens{\sum_{i,j,l} \frac{e^{-\beta \lambda_S(i) - \beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|0,l)}^2.
    \end{align}
    We now need to simplify
    \begin{align}
        \sum_{i,j,l} \frac{e^{-\beta \lambda_S(i) - \beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|0,l) &= \sum_{i \neq 0} (a(i) b(j) - a(k) b(l)) \tau(i,j|0,l) \\
        &\geq \sum_{i \neq 0} \parens{(a(i) b(0) - a(0) b(1)) \tau(i, 0 | 0, 1) - 3 \epsilon_{\sinc}}
    \end{align}
    Now we assume that there exists one and only one index $\kappa$ such that $\abs{\Delta_S(\kappa, 0) - \gamma} \leq \Delta_{\min}$. This ensures that if $i \neq \kappa$ then $\tau(i,j | 0, l) \leq \epsilon_{\sinc}$ and that $\epsilon_{\sinc} \leq \tau(\kappa, 0 | 0, 1) \leq 1$. This allows us to simplify the sum as
    \begin{align}
        \sum_{i \neq 0} \parens{(a(i) b(0) - a(0) b(1)) \tau(i, 0 | 0, 1) - 3 \epsilon_{\sinc}} &= \parens{(a(\kappa) b(0) - a(0) b(1)) \tau(\kappa, 0 | 0, 1) - 3 \epsilon_{\sinc}} \nonumber \\
        &\quad + \sum_{i \neq 0, \kappa} \parens{(a(i) b(0) - a(0) b(1)) \tau(i, 0 | 0, 1) - 3 \epsilon_{\sinc}} \\
        &\geq (a(\kappa) b(0) - a(0) b(1)) \tau(\kappa, 0 | 0, 1) - \epsilon_{\sinc}(3 + 4 (\dim - 2))
    \end{align}
\end{proof}

\begin{theorem}
    This is the actual legit one, so work here. 
\end{theorem}
\begin{proof}
    First we want to define the (multi)set
    \begin{equation}
        S = \set{\Delta_S(i,j) : 0 \leq i < j \leq \dim_S}.
    \end{equation}
    We assume that if $(i,j) \neq (i', j')$ then $\Delta_S(i,j) \neq \Delta_S(i', j')$. This is a significant restriction, but it greatly simplifies the following proofs. 
    
    Our next goal is to study the distance reduction achieved by an application of our repeated interactions channel. The most convenient distance to study is the square of the Sch\"{a}tten 2-norm, $\norm{\cdot}_2^2$. Although this does not have a nice operational interpretation, as the trace distance does in regards to state distinguishability, it can be used to give a loose upper bound on the trace distance. We break down our distance into two terms based on the second order decomposition of the channel
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi_{\gamma}(\rho_S)}_2^2 &= \norm{\rho_S(\beta_E) - \rho_S - \sum_{i, j, k, l} \bra{i} \rho_S \ket{i} \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)}   \tau(i, j| k,l) \ketbra{k}{k} + R(\rho_S)}_2^2.
\end{align}
For simplicity, denote the operator $\rho_S(\beta_E) - \rho_S \eqqcolon A$ and $\sum_{i, j, k, l} \bra{i} \rho_S \ket{i} \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} \tau(i, j| k,l) \ketbra{k}{k} \eqqcolon B$. This allows us to isolate the remainder part as follows
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi_\gamma(\rho_S)}_2^2 &= \norm{A - B + R(\rho_S)}_2^2 \\
    &\leq \parens{\norm{A - B}_2 + \norm{R(\rho_S)}}^2 \\
    &\leq \norm{A - B}_2^2 + 2 \norm{A - B}_2 \norm{R(\rho_S(\beta))}_2 + \norm{R(\rho_S)}_2^2
\end{align}
We see that since we have a term linear in $\norm{R}_2$ we should only analyze $\norm{A - B}_2^2$ to order $\bigo{\alpha^2}$. We will now simplify this to a linear expression in $\norm{R}_2$ by using simple upper bounds on norms of $A$ and $B$. \textcolor{red}{NOTE: Need to bound the norm of the Taylor remainder of the channel for all states by some epsilon.} 
\begin{align}
    \norm{A - B}_2 &\leq \norm{A - B}_1 \\
    &\leq \norm{A}_1 + \norm{B}_1 \\
    &= \sum_{i} \abs{\bra{i}\rho_S(\beta_E) \ket{i} - \bra{i} \rho_S \ket{i}} + \sum_k \abs{\sum_{i,j,l} \bra{i} \rho_S \ket{i} \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} \tau(i,j | k,l)} \\
    &\leq \sum_i 2 + \sum_k \sum_{i,j,l} 1 \\
    &\leq \dim^2 + 2 \dim \\
    &\leq 2 \dim^2.
\end{align}
This then gives our remainder term as
\begin{equation}
    \norm{\rho_S(\beta_E) - \Phi_{\gamma}(\rho_S)}_2^2 \leq \norm{A - B}_2^2 + (4 \dim^2 + 1) \norm{R_{\Phi}}_2.
\end{equation}
Now we can bound $\norm{R_{\Phi}}_2 \leq \sqrt{\dim} \norm{R_{\Phi}} \leq \epsilon_{R} \sqrt{\dim}$. This gives the final inequality as
\begin{equation}
    \norm{\rho_S(\beta_E) - \Phi_{\gamma}(\rho_S)}_2^2 \leq \norm{A - B}_2^2 + 5 \dim^{5/2} \epsilon_{R}
\end{equation}

Note we can improve this bound with improvements to the $\norm{B}_2$ bound. 
\end{proof}


Shifting our attention to $\norm{A-B}_2^2$, we decompose this into trace calculations straightforwardly
\begin{align}
    \norm{A-B}_2^2 &= \trace{(A-B)^\dagger (A-B)} \\
    &= \trace{A^2} - 2 \trace{AB} + \trace{B^2}
\end{align}
In order to bound each of these traces it will be helpful to organize the indices of our system into two sets, one of indices which are ``active" in the transition and those that are ``dead". The active set is denoted as $S_{\gamma}$ and is defined as
\begin{equation}
    S_{\gamma} \coloneqq \set{(i,j) : \lambda_S(i) \leq \lambda_S(j), |\Delta_S(i,j) - \gamma| \leq \Delta_{\min}},
\end{equation}
whereas the ``dead" set is denoted and defined as
\begin{equation}
T_{\gamma} \coloneqq \set{i : \forall j, |\Delta_S(i,j) - \gamma| \geq \Delta_{\min}}. 
\end{equation}


\noindent \textbf{Bounding: }$\trace{B^2}$


We first bound $\norm{B}_2^2 = \trace{B^2}$. We split this trace into sums over $S_{\gamma}$ and $T_{\gamma}$. 
\begin{align}
    k \in T_{\gamma} \implies B(k) &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) - a(k) b(l)) \tau(i,j|k,l) \\
    &\leq \sum_{i \neq k} \sum_{j,l} \tau(i,j|k,l) \\
    &\leq 4 (\dim_S - 1) \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} \\
    &\leq 2 \alpha^2 t^2 \epsilon_{\sinc}.
\end{align}


To perform the sum over the remaining indices we introduce a function $f_U$ that maps a given index to a subset of indices that satisfy
\begin{align}
    f_U(k | \gamma) \coloneqq \set{k' : \abs{\Delta_S(k, k') - \gamma} \leq \Delta_{\min} \text{ and } \Delta_S(k,k') < 0} \\
    f_L(k | \gamma) \coloneqq \set{k' : \abs{\Delta_S(k, k') - \gamma} \leq \Delta_{\min} \text{ and } \Delta_S(k, k') > 0}.
\end{align}
We then simplify a term for $\trace{B^2}$ as
\begin{align}
    B(k) &= \sum_{i \neq k} \sum_{j,l} (a(i) b(j) - a(k) b(l)) \tau(i,j|k,l) \\
    &\leq \sum_{i \in T_{\gamma}(k)} \sum_{j,l} \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} + \sum_{k' \in f_U(k)} \parens{\frac{\alpha^2 t^2}{\dim + 1} 3 \epsilon_{\sinc} + \tau(k', 0 | k, 1)} + \sum_{k'' \in f_L(k)} \parens{\tau(k'', 1 | k, 0) + 3\frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}} \\
    &\leq \epsilon_{\sinc}\frac{\alpha^2 t^2}{\dim + 1} \parens{4 |T_{\gamma}(k)| + 3 | f_U(k)| + 3 |f_L(k)|} + \sum_{k' \in f_U(k)} (a(k')b(0) - a(k) b(1))\tau(k', 0| k, 1) \nonumber \\
    &\quad + \sum_{k'' \in f_L(k)} (a(k'') b(1) - a(k) b(0))\tau(k'', 1| k, 0) \\
    &\leq \epsilon_{\sinc} \dim \frac{\alpha^2 t^2}{\dim + 1} + \sum_{k' \in f_U(k)} (a(k')b(0) - a(k) b(1))\tau(k', 0| k, 1) + \sum_{k'' \in f_L(k)} (a(k'') b(1) - a(k) b(0))\tau(k'', 1| k, 0) \\
    &= \epsilon_{\sinc}\alpha^2 t^2 + b(0) \parens{\sum_{k' \in f_U(k)} a(k') \tau(k', 0 | k, 1) - \sum_{k'' \in f_L(k)} a(k'') \tau(k'', 1| k, 0)} \nonumber \\
    &\quad + b(1) \parens{-\sum_{k' \in f_U(k)} a(k') \tau(k', 0 | k, 1) + \sum_{k'' \in f_L(k)} a(k'') \tau(k'', 1| k, 0)} \\
    &\eqqcolon \widetilde{B}.
\end{align}
We let $\widetilde{B}$ denote the upper bound on $B(k)$. 

this gives the final bound for $\trace{B^2}$ as
\begin{equation}
    \trace{B^2} \leq 4 |T_{\gamma}| (\alpha^2 t^2 \epsilon_{\sinc})^2 + |S_{\gamma}| \widetilde{B}^2
\end{equation}



\newpage
\noindent\rule{\textwidth}{1pt}
\noindent\rule{\textwidth}{1pt}

\begin{claim}
    Given inputs $H_S$, $\rho_S$, $\beta_E$, and $\gamma$, we compute an upper bound on the distance of the thermalizing channel as
    \begin{align}
        \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_2^2 \leq \norm{\rho_S(\beta_E) - \rho_S}_2^2 - \epsilon
    \end{align}
    
\end{claim}


Starting from the beginning:
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_2^2 &= \norm{\rho_S(\beta_E) - \rho_S(\beta) - \sum_{i,j,k,l} \frac{e^{-\beta_E \lambda_E(j) - \beta \lambda_S(i)}}{\partfun_E(\beta_E) \partfun_S(\beta)} \tau(i,j|k,l) \ketbra{k}{k} }_2^2 + \bigo{\alpha^6} \\
    &\approx \norm{A - B}_2^2 \\
    &= \trace{(A - B)^\dagger (A - B)} \\
    &= \trace{A^2} - 2 \trace{A B} + \trace{B^2},
\end{align}
where $A \coloneqq \rho_S(\beta_E) - \rho_S(\beta)$ and $B \coloneqq \sum_{i,j,k,l} \frac{e^{-\beta_E \lambda_E(j) - \beta \lambda_S(i)}}{\partfun_E(\beta_E) \partfun_S(\beta)} \tau(i,j|k,l) \ketbra{k}{k} $ are Hermitian and diagonal in the system's Hamiltonian basis. As $\trace{A^2}$ represents the distance we are trying to reduce, the goal is to show that $2 \trace{AB} \geq \trace{B^2} + \epsilon$. 

We use this sorting to create the (ordered) set of indices as before,

This means that each pair of indices $(i,j)$, such that their eigenvalue difference is close to $\gamma$, is stored only once in the set. This allows us to define the function $S_\gamma(i) = j$, as we have assumed non-degeneracy of the system eigenvalues. In a similar spirit, we say $i \in S_{\gamma}$ if it is the first index of a pair $(i,j) \in S_{\gamma}$. We can further define the set of ``dead" indices, those indices that do not have another eigenvalue that such that their difference is close to $\gamma$. This gives:

Now our goal is to compute $\trace{A B} = \sum_{k} A(k) B(k)$. We first compute $A(k) B(k)$ for $k \in T_{\gamma}$ and then for $k \in S_{\gamma}$. 



Now we move on to bounding the paired indices. Let $(k_1, k_2) \in S_{\gamma}$ be a single pair. We look at the sum of $A(k_1) B(k_1) + A(k_2) B(k_2)$. 
\begin{align}
    B(k_1) &\leq  \parens{\sum_{i \neq k_1, k_2} \sum_{j,l} (a(i) b(j) - a(k_1) b(l)) \tau(i,j|k_1, l) + 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + (a(k_2) b(0) - a(k_1) b(1)) \tau(k_2, 0 | k_1, 1)} \\
    &\leq \parens{\sum_{i \neq k_1, k_2} \sum_{j,l} \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} + 3 \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} + (a(k_2) b(0) - a(k_1) b(1)) \tau(k_2, 0 | k_1, 1)} \\
    &= (4 \dim_S - 5) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + (a(k_2) b(0) - a(k_1) b(1)) \tau(k_2, 0 | k_1, 1) \\
    &\leq 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + \tau(k_2, 0| k_1, 1)
\end{align}



and similarly we compute
\begin{align}
    B(k_2) &\leq  \parens{\sum_{i \neq k_1, k_2} \sum_{j,l} (a(i) b(j) - a(k_1) b(l)) \tau(i,j|k_2, l) + 3 \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} + (a(k_1) b(1) - a(k_2) b(0)) \tau(k_1, 1 | k_2, 0)} \\
    &\leq \parens{\sum_{i \neq k_1, k_2} \sum_{j,l} \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + (a(k_1) b(1) - a(k_2) b(0)) \tau(k_1, 1 | k_2, 0)} \\
    &= (4 \dim_S - 5) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + (a(k_1) b(1) - a(k_2) b(0)) \tau(k_1, 1 | k_2, 0) \\
    &\leq 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + \tau(k_1, 1| k_2, 0).
\end{align}
Given that $\tau$ is symmetric about input and output, we have $B(k_1) - (4 \dim_S - 5) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} = - (B(k_2) - (4\dim_S - 5) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc})$. This allows us to compute the sum
\begin{align}
    A(k_1) B(k_1) + A(k_2) B(k_2) &\leq (4 \dim_S - 5) \epsilon_{\sinc} \parens{A(k_1) + A(k_2)} + (a(k_2) b(0) - a(k_1) b(1)) \tau(k_1, 1| k_2, 0) (A(k_1) - A(k_2)) \\
    &\leq 2 (4 \dim_S - 5) \epsilon_{\sinc} + (a(k_2) b(0) - a(k_1) b(1)) \tau(k_1, 1| k_2, 0) (A(k_1) - A(k_2)) \\
    &= 2 (4 \dim_S - 5) \epsilon_{\sinc} + \frac{e^{-\beta \lambda_S(k_2) - \beta_E \lambda_E(0)} - e^{-\beta \lambda_S(k_1) - \beta_E \lambda_E(1)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(k_1, 1| k_2, 0) (A(k_1) - A(k_2)).
\end{align}
As we want this to be negative, we require
\begin{align}
    e^{-\beta \lambda_S(k_2) - \beta_E \lambda_E(0)} - e^{-\beta \lambda_S(k_1) - \beta_E \lambda_E(1)} &\leq 0 \\
    \beta \lambda_S(k_2) &\geq \beta \lambda_S(k_1) + \beta_E\gamma \\
    \beta \Delta_S(k_2, k_1) &\geq \beta_E \gamma.
\end{align}

We will need an upper bound on $\trace{B^2}$
\begin{align}
    \trace{B^2} &= \sum_k B(k)^2 \\
    &= \sum_{k \in T_{\gamma}} B(k)^2 + \sum_{(k_1, k_2) \in S_{\gamma}} B(k_1)^2 + B(k_2)^2 \\
    &\leq 16 |T_{\gamma}| (\dim_S - 1)^2 \frac{\alpha^4 t^4}{(\dim + 1)^2}\epsilon_{\sinc}^2 + \sum_{(k_1, k_2) \in S_{\gamma}} B(k_1)^2 + B(k_2)^2
\end{align}
In order to compute the above we need an upper bound on $B(k_1)^2$ and $B(k_2)^2$. For ease of notation, we define the following variables:
\begin{align}
    S_1 &\coloneqq \sum_{i \neq k_1, k_2} \sum_{j,l} (a(i) b(j) - a(k_1) b(l)) \tau(i, j|k_1, l) \\
    S_2 &\coloneqq \sum_{(j,l) \neq (0, 1)} (a(k_2) b(j) - a(k_1) b(l)) \tau(k_2, j| k_1, l) \\
    r &\coloneqq a(k_2) b(0) - a(k_1) b(1) 
\end{align}
We can bound the absolute value of these as:
\begin{align}
    \abs{S_1} &\leq \sum_{i \neq k_1, k_2} \sum_{j,l} |a(i) b(j) - a(k_1) b(l)| |\tau(i, j|k_1, l)| \\
    &\leq \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \sum_{i \neq k_1, k_2} \sum_{j,l} 1\\
    &\leq 4 (\dim_S - 2) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    \abs{S_2} &\leq \sum_{(j,l) \neq (0,1)} |a(k_1) b(j) - a(k_2) b(j)| |\tau(k_2, j | k_1, l)| \\
    &\leq \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \sum_{(j,l) \neq (0,1)} 1 \\
    &\leq 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}
\end{align}
This allows us to compute bounds on $B(k_1)^2$ and $B(k_2)^2$ as:
\begin{align}
    B(k_1)^2 &= \bigg( S_1 + S_2 + r \tau(k_2, 0| k_1, 1) \bigg)^2 \\
    &\leq (|S_1| + |S_2|)^2 + 2(S_1 + S_2) r \tau(k_2, 0| k_1, 1) + r^2 \tau(k_2, 0| k_1, 1)^2 \\
    B(k_2)^2 &= \bigg( S_1 + S_2 - r \tau(k_2, 0 | k_1, 1) \bigg)^2 \\
    &\leq (|S_1| + |S_2|)^2 - 2 (S_1 + S_2) r \tau(k_2, 0 | k_1, 1) + r^2 \tau(k_2, 0 | k_1, 1)^2 \\
    B(k_1)^2 + B(k_2)^2 &\leq 2 (|S_1| + |S_2|)^2 + 2 r^2 \tau(k_2, 0 | k_1, 1)^2 \\
    &\leq 32 \dim_S^2 \frac{\alpha^4 t^4}{(\dim + 1)^2} \epsilon_{\sinc}^2 + 2 r^2 \tau(k_2, 0 | k_1, 1)^2.
\end{align}
We can now bound $\trace{B^2}$ as
\begin{align}
    \trace{B^2} &= \sum_{k \in T_{\gamma}} B(k)^2 + \sum_{(k_1, k_2) \in S_{\gamma}} B(k_1)^2 + B(k_2)^2 \\
    &\leq 16 |T_{\gamma}| (\dim_S - 1)^2 \frac{\alpha^4 t^4}{(\dim + 1)^2} \epsilon_{\sinc}^2 + 16 \dim_S^2 (2 |S_{\gamma}|) \frac{\alpha^4 t^4}{(\dim + 1)^2} \epsilon_{\sinc}^2 + 2 \sum_{(k_1, k_2) \in S_{\gamma}} r^2 \tau(k_1, 1 | k_2, 0)^2 \\
    &\leq 16 \dim_S^3 \frac{\alpha^4 t^4}{(\dim + 1)^2} \epsilon_{\sinc}^2 + 2 \sum_{(k_1, k_2) \in S_{\gamma}} r^2 \tau(k_1, 1 | k_2, 0)^2
\end{align}

Now our goal is to upper bound $-2\trace{AB}$. We make use of the fact that $|A(k)| \leq 1$ for all $k$ and that $k \in T_{\gamma}$ implies that $|B(k)| \leq 4 (\dim_S - 1) \epsilon_{\sinc}$.
\begin{align}
    -2 \trace{AB} &= -2 \sum_{k \in T_{\gamma}} A(k) B(k) - 2 \sum_{(k_1, k_2) \in S_{\gamma}} A(k_1) B(k_1) + A(k_2) B(k_2) \\
    &\leq + 2 \sum_{k \in T_{\gamma}} |A(k)| |B(k)| - 2 \sum_{(k_1, k_2) \in S_{\gamma}} A(k_1) B(k_1) + A(k_2) B(k_2) \\
    &\leq 8 \dim_S |T_{\gamma}| \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} - 2 \sum_{(k_1, k_2) \in S_{\gamma}} A(k_1) B(k_1) + A(k_2) B(k_2).
\end{align}
We now investigate the right term in the above. Using the notation from before
\begin{align}
    A(k_1) B(k_1) &= A(k_1) (S_1(k_1) + S_2(k_1) + r \tau(k_1, 1 | k_2, 0)) \\
    A(k_2) B(k_2) &= A(k_2) (S_1(k_2) + S_2(k_2) - r \tau(k_1, 1 | k_2, 0)) \\
    -2A(k_1) B(k_1) -2 A(k_2) B(k_2) & = -2 A(k_1)(S_1(k_1) + S_2(k_1))  - 2A(k_2)( S_1(k_2) + S_2(k_2)) + 2 r \tau(k_1, 1 | k_2, 0)( A(k_2) - A(k_1)) \\
    &\leq 2 |A(k_1)|(|S_1(k_1)| + |S_2(k_1)|) + 2 |A(k_2)|( |S_1(k_2)| + |S_2(k_2)|) \nonumber \\
    &+ 2 r \tau(k_1, 1 | k_2, 0)(A(k_2) - A(k_1)) \\
    &\leq 16 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 2 r \tau(k_1, 1 | k_2, 0)( A(k_2) - A(k_1)).
\end{align}
This allows us to plug in to our sum:
\begin{align}
    -2 \trace{AB} &\leq 8 \dim_S(|T_{\gamma}| + 2 |S_{\gamma}|) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 2 \sum_{(k_1, k_2) \in S_{\gamma}} r \tau(k_1, 1 | k_2, 0) ( A(k_2) - A(k_1)) \\
    &= 8 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 2 \sum_{(k_1, k_2) \in S_{\gamma}} r \tau(k_1, 1 | k_2, 0) ( A(k_2) - A(k_1)).
\end{align}
Combining with the bound for $\trace{B^2}$ we get the following
\begin{align}
    \trace{B^2} - 2 \trace{AB} &\leq 16 \dim_S^3 \frac{\alpha^4 t^4}{(\dim + 1)^2} \epsilon_{\sinc}^2 + 8 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 2 \sum_{(k_1, k_2) \in S_{\gamma}} \parens{r^2 \tau(k_1, 1 |k_2, 0)^2 + r \tau(k_1, 1| k_2, 0) (A(k_2) - A(k_1))}.
\end{align}
To prove the required bound, we will require the following two inequalities:
\begin{align}
    16 \dim_S^3 \frac{\alpha^4 t^4}{(\dim + 1)^2}\epsilon_{\sinc}^2 + 8 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} &\leq \frac{\epsilon}{2} \\
    2 \sum_{(k_1, k_2) \in S_{\gamma}} r \tau(k_1, 1 | k_2, 0) (r \tau(k_1, 1 | k_2, 0)  + A(k_2) - A(k_1)) &\leq - \frac{3\epsilon}{2}.
\end{align}

Everything below this line has not been updated with the fix to $\tau(i,j|k,l) \leq \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}$. 

\noindent\rule{\textwidth}{1pt}

The first inequality is rather straightforward. We use the intuition that the left term (ignoring constant factors) is nearly the square of the right term, and since we want to bound their sum with something small we can then deduce that the right term should be dominant. This yields the intuition that
\begin{align}
    16 \dim_S^3 \frac{\alpha^4 t^4}{(\dim + 1)^2} \epsilon_{\sinc}^2 &\leq 8 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    \epsilon_{\sinc} &\leq \frac{\dim + 1}{2 \dim_S} \frac{1}{(\alpha^2 t^2)} \\
    &= \parens{1 + \frac{1}{\dim}} \frac{1}{\alpha^2 t^2}.
\end{align}
We then note that because the bound $\epsilon_{\sinc} \geq 1/(\Delta_{\min}^2 t^2)$ has to be satisfied, this yields a ``sandwich'' of acceptable ranges for $\epsilon_{\sinc}$ when combined with the upper bound above. To make this sandwich reasonable we require
\begin{align}
    \frac{1}{\Delta_{\min}^2 t^2} &\leq \epsilon_{\sinc} \leq \parens{1 + \frac{1}{\dim}} \frac{1}{\alpha^2 t^2} \\
    \alpha^2 &\leq \Delta_{\min}^2 \parens{1 + \frac{1}{\dim}}.
\end{align}
Curiously, there are no requirements on the value of $t$. To complete this argument, we now simplify the lower bound on $\epsilon$ by noting,
\begin{align}
    16 \dim_S^3 \frac{\alpha^4 t^4}{(\dim + 1)^2} \epsilon_{\sinc}^2 + 8 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} &\leq 16 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc},
\end{align}
which yields $\epsilon \geq 32 \dim_S^2 \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1}$.

The second inequality is a lot trickier. We will work with this as a quadratic expression in $\tau$ and find values of $\tau$ such that it is satisfied. We plug in for $\tau$ and rewrite the inequality as 
\begin{align}
    &\frac{(\alpha t)^4}{(\dim + 1)^2} \parens{2  \sum_{(k_1, k_2) \in S_{\gamma}} r^2 \sinc((\Delta_S(k_1, k_2) - \gamma) t/2)^4} \nonumber \\
    &+ \frac{(\alpha t)^2}{\dim + 1} \parens{2  \sum_{(k_1, k_2) \in S_{\gamma}}r \sinc((\Delta_S(k_1, k_2) - \gamma) t/2)^2 (A(k_2) - A(k_1))} \nonumber \\
    &+ \parens{\frac{\alpha t}{\dim + 1}}^0 \frac{3 \epsilon}{2} \leq 0
\end{align}
We first simplify the coefficient for $\alpha^4$ as 
\begin{align}
    2 \sum_{(k_1, k_2) \in S_{\gamma}} r^2 \sinc((\Delta_S(k_1, k_2) - \gamma) t/2)^4 &\leq 2 |S_{\gamma}|.
\end{align}
Since everything is positive, if this simplified inequality holds then so does the original. 

For simplicity we denote the summation in the coefficient for $\alpha^2$ as
$$d \coloneqq 2 \sum_{(k_1, k_2) \in S_{\gamma}} r \sinc((\Delta_S(k_1, k_2) - \gamma) t/2)^2 (A(k_2) - A(k_1)).$$
Letting $x$ play the role of $(\alpha t)^2 / (\dim + 1)$ gives us a simple quadratic expression:
\begin{equation}
    2 x^2 |S_{\gamma}| + x d + \frac{3 \epsilon}{2} \leq 0.
\end{equation}
As $(\alpha t)^2 / (\dim + 1) \geq 0$ we have $x$ must be positive as well. This requires for not only $d$ to be negative, but that the roots of the quadratic must both be real and at least one of them must be positive. Denote the roots $x_{\pm}$, which can be seen as:
\begin{align}
    x_{\pm} &= \frac{-d}{4 |S_{\gamma}|} \pm \frac{1}{4 |S_{\gamma}|} \sqrt{d^2 - 12 |S_{\gamma}| \epsilon} \\
    &= \frac{-d}{4 |S_{\gamma}|} \parens{1 \mp \sqrt{1 - \frac{12 |S_{\gamma}| \epsilon}{d^2}}}.
\end{align}
We want to guarantee that these roots are real, so we require 
\begin{equation}
    d^2 \geq 12 |S_{\gamma}| \epsilon. \label{eq:d_squared_epsilon_bound}
\end{equation}
Note that inside the radical, $12  |S_{\gamma}| \epsilon / d^2$ is always positive, implying that the radical is always going to be less than 1. This leads to both roots being positive so long as $d \leq 0$ and $d^2$ is lower bounded as mentioned.

If these two conditions are met, then we can set $\alpha^2$ to be the average of the two roots and the distance reduction claim is satisfied. Ideally we would like the lower root to be as close to 0 as possible, so that way we can simply reduce $\alpha$ to get our desired distance reduction. Regardless, our last objective still is to produce bounds on $d$ that satisfy the given inequalities, we therefore turn our attention to the sum in question. To simplify $d$ we first investigate the sign of $r$, assuming it to be positive we see what conditions result
\begin{align}
    r = a(k_2) b(0) - a(k_1) b(1) &\geq 0 \\
    \bra{k_2} \rho_S \ket{k_2} \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)} - \bra{k_1} \rho_S \ket{k_1} \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} &\geq 0 \\
    \frac{\bra{k_2} \rho_S \ket{k_2}}{\bra{k_1} \rho_S \ket{k_1}} &\geq e^{-\beta_E \gamma}. \label{eq:bound_on_rho_s_for_r}
\end{align}
If we were using thermal states, $\rho_S = e^{-\beta H_S} / \partfun_S(\beta)$, the left hand side of the above would be $e^{-\beta \Delta_S(k_2, k_1)}$. Simplifying would result in $\beta \leq \frac{\gamma}{\Delta_S(k_2, k_1)} \beta_E$. Since we expect the distance moved by the thermalizing channel to be greatest as $\beta_E \to \infty$ and $\beta \to 0$, requiring this inequality to be true, and therefore $r \geq 0$, seems to be a reasonable condition to impose.

With the sign of $r$ sorted out, we now have to bound the summation for $d$. Since we require $d \leq 0$ in order to get positive values for $(\alpha t)^2 $, we require
\begin{align}
    \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc((\Delta_S(k_2, k_1) - \gamma)t/2)^2}{\dim + 1} (A(k_2) - A(k_1)) &\leq 0.
\end{align}
To prove this we bound the following, note we surpress the arguments to $\sinc$ to save space, and we make use of the bound required in Eq. \eqref{eq:bound_on_rho_s_for_r}
\begin{align}
    \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} A(k_2) &= \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} \parens{\frac{e^{-\beta_E \lambda_S(k_2)}}{\partfun_S(\beta_E)}  - \bra{k_2} \rho_S \ket{k_2}} \\
    &\leq \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} \parens{\frac{e^{-\beta_E \lambda_S(k_2)}}{\partfun_S(\beta_E)}  - e^{-\beta_E \gamma} \bra{k_1} \rho_S \ket{k_1}} \\
    &= \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} \parens{\frac{e^{-\beta_E (\lambda_S(k_1) - \lambda_S(k_1) + \lambda_S(k_2) )}}{\partfun_S(\beta_E)}  - e^{-\beta_E \gamma} \bra{k_1} \rho_S \ket{k_1}} \\
    &= \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} \parens{\frac{e^{-\beta_E \lambda_S(k_1)}}{\partfun_S(\beta_E)} e^{-\beta_E \Delta_S(k_2, k_1)}  - e^{-\beta_E \gamma} \bra{k_1} \rho_S \ket{k_1}}.
\end{align}
We note right away that as $\beta_E \to 0$ this yields $\sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} (A(k_2) - A(k_1)) \leq 0$. 

Now subtracting the sum with $A(k_1)$ and simplifying yields
\begin{align}
    \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} (A(k_2) - A(k_1)) &\leq \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} \parens{\frac{e^{-\beta_E \lambda_S(k_1)}}{\partfun_S(\beta_E)} (e^{-\beta_E \Delta_S(k_2, k_1)} - 1) - \bra{k_1} \rho_S \ket{k_1} (e^{-\beta_E \gamma} - 1) } .
\end{align}
Since we have to upper bound this summation by 0, a good first step would be to understand when a given term is positive or negative. Taking a generic term, disregarding the prefactor of $\frac{\sinc^2}{\dim + 1}$ and simplifying leads to
\begin{align}
    \bra{k_1} \rho_S \ket{k_1}& \parens{1 - e^{-\beta_E \gamma}} -\frac{e^{-\beta_E \lambda_S(k_1)}}{\partfun_S(\beta_E)}\parens{1 - e^{-\beta_E \Delta_S}} \leq 0 \\
    \bra{k_1} \rho_S \ket{k_1}&  \leq \frac{e^{-\beta_E \lambda_S(k_1)}}{ \partfun_S(\beta_E)}\frac{1 - e^{-\beta_E \Delta_S}}{1 - e^{-\beta_E \gamma}}.
\end{align}
In order to proceed with the analysis we must impose some kind of structure onto $\rho_S$. We will investigate a few limits, one in which $\rho_S$ is a thermal state with $\beta$ close enough to $\beta_E$, one in which $\beta_E \to \infty$, and another in which we bound the operator distance of $\rho_S$ from $\rho_S(\beta_E)$. 

The condition in which we expect the most rapid thermalization is one in which the environment is in it's ground state (temperature of 0 or $\beta_E \to \infty$) and the system is in the maximally mixed state (temperature to infinity or $\beta \to 0$). In this situation, the factors $e^{-\beta_E \Delta_S} \to 0$ and $e^{-\beta_E \gamma} \to 0$. Further, the Boltzmann factors approach 0 if the eigenvalue is not a minimal eigenvalue or 1 if it is a minimal eigenvalue (ground state energy). In this situation, if $(0, i) \notin S_{\gamma}$, for all $i$, then we have that the Boltzmann factors $e^{-\beta_E \lambda_S(k_1)} = 0$. In this case we can also directly evaluate the summation $A(k_2) - A(k_1)$, as this is 
\begin{equation}
    A(k_2) - A(k_1) = \frac{e^{-\beta_E \lambda_S(k_2)}}{\partfun_S(\beta_E)} - \frac{1}{\dim_S} - \frac{e^{-\beta_E \lambda_S(k_1)}}{\partfun_S(\beta_E)} + \frac{1}{\dim_S} = 0.
\end{equation}
From Eq. \eqref{eq:d_squared_epsilon_bound} we see that this implies that $\epsilon = 0$ and we do not have any distance reduction to the ground state possible. This makes intuitive sense, as any probability mass that gets shuffled from high energy states to lower energy states does not get us any closer to the ground state. 
However, whenever the 0 temperature environment and infinite temperature system are coupled when $\gamma$ is close enough to a transition between a system ground state and an excited state we can get distance reduction. We will analyze this situation now. In the case in which there exists a pair $(0, i) \in S_{\gamma}$, meaning $e^{-\beta_E \lambda_S(k)} / \partfun_S(\beta_E) = \delta_{k,0}$ and $|\Delta_S(i, 0) - \gamma| \leq \Delta_{\min}$. In this case, if $(k_1, k_2) \in S_{\gamma}$ and $k_1 \neq 0$, then $A(k_2) - A(k_1) = 0$. However, for $(0, k)$ $A(k) - A(0) = 0 - 1/\dim_S - 1 + 1/\dim_S = -1$. Therefore, the total sum is then 
\begin{align}
    \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} (A(k_2) - A(k_1)) &= - \sum_{(0, k) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1} \\
    &\leq \frac{- \epsilon_{\sinc}}{\dim + 1}.
\end{align}
In addition, we can provide the simplistic bound 
\begin{equation}
    \sum_{(k_1, k_2) \in S_{\gamma}} \frac{\sinc^2}{\dim + 1}(A(k_2) - A(k_1)) \geq \frac{-1}{\dim + 1}.
\end{equation}
This allows us to argue that $\frac{\epsilon_{\sinc}^2}{(\dim + 1)^2} \leq d^2 \leq \frac{1}{(\dim + 1)^2}$. Propagating this through to $\epsilon$ via Eq. \eqref{eq:d_squared_epsilon_bound} yields 
\begin{align}
    d^2 &\geq \frac{12 t^4 |S_{\gamma}| \epsilon}{(\dim + 1)^2} \\
    \epsilon &\leq \frac{1}{12 t^4 |S_{\gamma}|}.
\end{align}
Now we look at the lower bound for $\epsilon$, which is given by 
\begin{align}
    \epsilon \geq 32 \epsilon_{\sinc} \alpha^2 t^2 \frac{\dim_S^2}{\dim + 1}
\end{align}


Without adding any structure to $\rho_S$ this is essentially a requirement we must impose on the state. We will go on to investigate for what ranges of $\beta$ this holds in the case that $\rho_S$ is a thermal state. We see as $\beta_E \to \infty$ that this inequality is trivially satisfied for $k_1$ being the ground state, the RHS approaches 1.

As we can see that this bound is pretty much dependent on the structure of the state, we now move on to bounding the value of $\epsilon$ that can be achieved. This comes from the bound in Eq. \eqref{eq:d_squared_epsilon_bound}, where we note that since $d$ is negative (from $r$ being positive) this amounts to an upper bound on $d$ (or a lower bound on $\epsilon$).

\bibliographystyle{unsrt}
\bibliography{bib}

\appendix

\section{Detailed Balance}
Given that we are able to rotate the basis we work with into the System Hamiltonian's eigenbasis, and moreover we previously showed that off-diagonal coherences in this basis are negligible, we can treat this process as a stochastic one. Explicitly, if we give a diagonal state as input to the channel $\Phi$, to order $\bigo{\alpha^2}$ we get a diagonal state back. Since diagonal states can be thought of as probability distributions, the map $\Phi$ can roughly be thought of as a Markov process on the eigenstates of the Hamiltonian $H_S$. Since our goal is to show that this map produces a thermal state,
we would need to show that this probability distribution converges to the Boltzmann distribution $e^{-\beta_E \lambda_S(i)} / \partfun_S(\beta_E)$, for state $i$. In classical probability sampling literature this is done through a Detailed Balance calculation, which shows that the desired distribution is a fixed point. Ergodicity arguments of the desired Markov chain are then used to claim that the fixed point is unique and that therefore the process produces samples of the desired distribution.

Given that our map is only approximate, we cannot make rigorous claims regarding detailed balance at fixed $\alpha, t,$ or $\gamma$. However, in this section we show that in the appropriate limits that our mapping satisfies Detailed Balance exactly. This is enough to give solid evidence that for appropriate regimes our channel should converge approximately to this goal distribution.



\begin{align}
    &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \nonumber \\
    &\approx \frac{\alpha^2 t^2}{\dim + 1} \parens {\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}  \sinc^2((\Delta(i,j) + \gamma)t) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 ((\Delta(i,j) - \gamma)t) + 2 \sinc^2(\Delta(i,j) t)} \nonumber
\end{align}

\begin{align}
    &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \nonumber \\
    &= \bra{j} \Phi_{\gamma}(\ketbra{i}{i}) \ket{j} \\
    &= \braket{j}{i}\braket{i}{j} + \sum_{k,l} \tau(i, k | j, l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} + \bra{j} R_{\Phi}(\ketbra{i}{i})\ket{j}
\end{align}
Now as we are studying Detailed Balance, we assume that $i \neq j$. For our purposes, without loss of generality we let $\lambda_S(i) \geq \lambda_S(j)$, so by transitioning from $i \to j$ we are losing energy to the environment. We simplify the non-trivial term from above as
\begin{align}
    &\sum_{k,l} \tau(i,k| j,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} = \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)}(\tau(i,0|j,0) + \tau(i,0|j,1)) + \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} (\tau(i,1|j,0) + \tau(i,1|j,1) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} (\sinc^2(\Delta_S(i,j)t/2) + \sinc^2((\Delta_S(i,j) - \gamma)t/2)) \nonumber \\
    &\quad \quad \quad \quad \quad \quad  +\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} (\sinc^2((\Delta_S(i,j) + \gamma)t/2) + \sinc^2(\Delta_S(i,j) t/2)) \bigg) \\
    &\frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) + \sinc^2(\Delta_S(i,j)t/2)
\end{align}
As $\Delta_S(i,j) \geq 0 $ we expect that only the $\sinc^2((\Delta_S(i,j) - \gamma) t/2)$ term will contribute significantly to this sum. We can similarly write down the probability of the state to transition from $j \to i$ as
\begin{align}
    \prob{\text{System transition } j \to i | \text{ env at } \beta_E} = \sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} + \bra{i} R_{\Phi}(\ketbra{j}{j})\ket{i},
\end{align}
where we write the non-trivial term as
\begin{align}
    \sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} = \frac{\alpha^2 t^2}{\dim + 1} \bigg(&\frac{1}{1 + e^{-\beta_E \gamma}} (\sinc^2(\Delta_S(j,i)t/2) + \sinc^2((\Delta_S(j,i) - \gamma)t/2)) \nonumber \\
    &+\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} (\sinc^2((\Delta_S(j,i) + \gamma)t/2) + \sinc^2(\Delta_S(j,i) t/2)) \bigg).
\end{align}
We simplify this by noting $\Delta_S(i,j) = - \Delta_S(j,i)$ and that $\sinc^2(x) = \sinc^2(-x)$ to get
\begin{align}
&\sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} \\
&= \frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) + \sinc^2(\Delta_S(i,j)t/2)
\end{align}

Now we would like to show some form of Detailed Balance, in appropriate limits, for the thermal state of the system. We will show that Detailed Balance holds in expectation, or with some non-zero probability. The thermal state give $\prob{\text{state in } i} = \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}$. We then need to expand differences
\begin{align}
    \chi(i,j) \coloneqq &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \prob{\text{state in } i} \nonumber \\
    &- \prob{\text{System transition } j \to i | \text{ env at } \beta_E} \prob{\text{state in } j}.
\end{align}
This expression is written in full glory as
\begin{align}
    \chi(i,j) &= \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \bra{j} R_{\Phi}(\ketbra{i}{i})\ket{j} \nonumber \\
    &+ \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \nonumber \\
    &+\frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma) t/2) \nonumber \\
    &+ \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\Delta_S(i,j)t/2) \nonumber \\
    &-\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \bra{i} R_{\Phi}(\ketbra{j}{j})\ket{i} \nonumber \\
    &- \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma) t/2) \nonumber \\
    &-\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \nonumber \\
    &- \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\Delta_S(i,j)t/2).
\end{align}
In order to simplify this, we will group these expressions with an end goal in mind. As we would like this to hold for all $i \neq j$ but we have a fixed $\gamma$, we need to randomly choose a $\gamma$ and show that this holds in expectation. Further, we would like to bound the absolute value of these differences. So we want $\mathbb{E}_{\gamma}\abs{\chi(i,j)}$. We can then use the triangle inequality, the fact that $\abs{\bra{j}R_{\Phi}(\ketbra{i}{i})\ket{j}} \leq \norm{R_{\Phi}}$, $\abs{\frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}} \leq 1$, 

along with the fact that transitions such as $\sinc^2(\Delta_S(i,j)t/2)$ are suppressed by $\epsilon_{\sinc}$ to get a simplified upper bound:
\begin{align}
    \mathbb{E}_{\gamma} \abs{\chi(i,j)} &\leq \mathbb{E}_{\gamma} 2 \norm{R_{\Phi}} \nonumber \\
    &+ \mathbb{E}_{\gamma} \frac{\alpha^2 t^2}{\dim + 1} 4 \epsilon_{\sinc}  \nonumber \\
    &+ \mathbb{E}_{\gamma} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \frac{1}{\partfun_S(\beta_E)} \abs{\parens{e^{-\beta_E \lambda_S(i)} - e^{-\beta_E \gamma} e^{-\beta_E \lambda_S(j)}} \sinc^2((\Delta_S(i,j) - \gamma) t/2)} \\
    &\leq 2 \norm{R_{\Phi}} + 4 \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \nonumber \\
    &+ \frac{\alpha^2 t^2}{\dim + 1} \mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}}. \label{eq:detailed_balance_upper_bound}
\end{align}
This is where we have to introduce our distribution over $\gamma$. We note that if $\gamma$ is far away from $\Delta_S(i,j)$, then as $t \to \infty$ we have that this product is trivially 0. We want to show that Detailed Balance is satisfied even when $\gamma \to \Delta_S(i,j)$. To do so we upper bound $\sinc^2 \leq 1$ whenever $\abs{\Delta_S(i,j) - \gamma} \leq \Delta_{\min}$ and $\sinc^2 \leq \epsilon_{\sinc}$ whenever $\abs{\Delta_S(i,j) - \gamma} \geq \Delta_{\min}$. 

We now are at an impasse. Our goal for this argument is to show that if our channel does anything non-trivial, then in the appropriate limits ($t \to \infty, \alpha \to 0, \alpha t \to c_{small}$) it should satisfy detailed balance conditions, or at least get arbitrarily close to it. We see that when our channel has a $\gamma$ that is not close to any $\Delta_S(i,j)$ we do not induce any transitions among states (in the $t \to \infty$ limit). This then trivially satisfies Detailed balance, as $\prob{i \to j} = \prob{j \to i} = 0$ gives $0 = 0$ for Detailed Balance. We would like to show that all $i \neq j$ can get arbitrarily close to satisfying Detailed Balance by choosing a $\gamma$ randomly. To do so there are three obvious candidates for distributions of $\gamma$ that we could analyze theoretically:
\begin{enumerate}
    \item A maximum entropy prior, or choosing $\gamma$ uniformly from 0 to $\norm{H}$,
    \item A minimal entropy prior, or exact knowledge of $\Delta_S(i,j)$ for all $i,j$, where we choose indices or differences uniformly,
    \item Choose a difference $\Delta_S(i,j)$ uniformly and then add in noise, either in the form of a Gaussian or a simple uniform box centered about the gap $\Delta_S(i,j)$.
\end{enumerate}

The simplest for us to look at first is the maximum entropy prior, or the uniform distribution of $\gamma$ from 0 to $\norm{H}$. This then gives us two regimes, $|\Delta_S(i,j) - \gamma| \leq \Delta_{\min}$ and $|\Delta_S(i,j) - \gamma| > \Delta_{\min}$. The $\sinc^2$ term is upper bounded by 1 in the former and $\epsilon_{\sinc}$ in the latter. 
\begin{align}
    &\mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} \nonumber \\
    &= \frac{1}{\norm{H}} \int_0^{\norm{H}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \\
    &= \frac{1}{\norm{H}} \int_0^{\Delta_S(i,j) - \Delta_{\min}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\quad + \frac{1}{\norm{H}} \int_{\Delta_S(i,j) - \Delta_{\min}}^{\Delta_S(i,j) + \Delta_{\min}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\quad + \int_{\Delta_S(i,j) + \Delta_{\min}}^{\norm{H}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma.
\end{align}
We will simplify these integrals separately. Starting with the first
\begin{align}
    &\frac{1}{\norm{H}} \int_0^{\Delta_S(i,j) - \Delta_{\min}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\leq \frac{\epsilon_{\sinc}}{\norm{H}} \int_0^{\Delta_S(i,j) - \Delta_{\min}} \left(e^{\beta_E(\Delta_S(i,j) - \gamma)} - 1\right) d\gamma \\
    &= \frac{\epsilon_{\sinc}}{\norm{H}}\parens{\Delta_{\min} - \Delta_S(i,j) + \frac{e^{\beta_E \Delta_S(i,j)}}{\beta_E}\left( 1 - e^{-\beta_E(\Delta_S(i,j) - \Delta_{\min})} \right)}.
\end{align}
We then compute the third, as it is more similar to the first as
\begin{align}
    &\frac{1}{\norm{H}} \int_{\Delta_S(i,j) + \Delta_{\min}}^{\norm{H}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\leq \frac{\epsilon_{\sinc}}{\norm{H}} \int_{\Delta_S(i,j) + \Delta_{\min}}^{\norm{H}} \left(1 - e^{\beta_E (\Delta_S(i,j) - \gamma)} \right) d\gamma \\
    &=\frac{\epsilon_{\sinc}}{\norm{H}}\parens{\norm{H} - (\Delta_S(i,j) + \Delta_{\min}) + \frac{1}{\beta_E} \left(e^{-\beta_E(\norm{H} - \Delta_S(i,j))} - e^{-\beta_E \Delta_{\min}} \right) }.
\end{align}
Adding the results of these two integrals yields
\begin{align}
    \frac{\epsilon_{\sinc}}{\norm{H}}\parens{\norm{H} - 2 \Delta_S(i,j) +  \frac{1}{\beta_E} e^{\beta_E \Delta_S(i,j)}(2 + e^{-\beta_E \norm{H}}) + \frac{e^{\beta_E \Delta_{\min}} - e^{-\beta_E \Delta_{\min}}}{\beta_E}}.
\end{align}
We make two observations. First that this is positive given that $\norm{H} \geq 2 \Delta_S(i,j)$ and tends towards infinity as $\beta_E \to \infty$. The second is that there is no time dependence on the factor within the parenthesis, meaning that for fixed $\beta_E$ we can make this quantity arbitrarily small by reducing $\epsilon_{\sinc} \propto 1/t^2$. 

The more reasonable distribution to analyze is the minimal entropy, or perfect knowledge distribution. In this distribution we pick an eigenvalue difference uniformly at random. We denote the set of eigenvalue gaps as $G_{\gamma}$. Then the expectation can be split into two: $S_{\gamma}$ being the set of gaps that are close to $\gamma$ and $T_{\gamma}$ as those that are far apart. Let $N_{diff}$ denote the number of differences. Specifically, let $S_{\gamma} = \set{\lambda_S(k) - \lambda_S(l) = \Delta_S(k,l) : \sinc^2((\Delta_S(i,j) - \Delta_S(k,l))t/2) \geq \epsilon_{\sinc}}$, and $T_{\gamma} = \set{\Delta_S(k,l) : \sinc^2((\Delta_S(i,j) - \Delta_S(k,l))t/2) < \epsilon_{\sinc}}$. Then the expected value over $\gamma$ becomes
\begin{align}
    &\mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \nonumber \\
    &=\frac{1}{N_{diff}}\sum_{\gamma \in S_{\gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} + \frac{1}{N_{diff}} \sum_{\gamma \in T_{\gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \\
    &\leq \frac{1}{N_{diff}} \sum_{\gamma \in S_{\gamma}} \abs{1 - e^{\beta_E(\Delta_S(i,j) -\gamma)}} + \frac{1}{N_{diff}} \epsilon_{\sinc} \sum_{\gamma \in T_{\gamma}}\abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}}.
\end{align}
We see that for the right hand sum the factor of $\epsilon_{\sinc}$, which vanishes as $t \to \infty$, causes the total sum to vanish as there are no explicit $t$ dependent terms. The set $T_{\gamma}$ does change with $t$, but it is upper bounded by a finite value and so is each possible summand. Now the real kicker is what happens to the leftmost summation. We investigate when an eigenvalue gap $\Delta_S(k,l)$ can be included in $S_{\gamma}$ as $t \to \infty$. 
\begin{align}
    \lim_{t \to \infty} \sinc^2((\Delta_S(i,j) - \Delta_S(k,l)) t/2) = \begin{cases}
        0 & \Delta_S(i,j) \neq \Delta_S(k,l) \\
        1 & \Delta_S(i,j) = \Delta_S(k,l).
    \end{cases}
\end{align}
Because of this $S_{\gamma} = \set{\Delta_S(i,j)}$ becomes a multiset consisting solely of $\Delta_S(i,j)$ with the number of degeneracies of the eigenvalue $S_{\gamma}$. This means that any term in the summation then becomes $\abs{1 - e^{\beta_E(\Delta_S(i,j) - \Delta_S(i,j)}} = 0$. Then given that $\epsilon_{\sinc} = 1/(\Delta_(\min)^2 t^2) \to 0$, we have that $\lim_{t \to \infty} \mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} = 0$. Looking at Eq. \eqref{eq:detailed_balance_upper_bound}, along with $\epsilon_{\sinc} = 1/(\Delta_{\min}^2 t^2)$ and $\alpha = \epsilon_{\alpha} / t$, we see that 
\begin{align}
    &\lim_{t \to \infty} \mathbb{E}_{\gamma} \abs{\chi(i,j)} \leq 2 \lim_{t \to \infty} \norm{R_{\Phi}} + 4 \lim_{t \to \infty} \frac{\epsilon_{\alpha}^2}{t^2 \Delta_{\min}^2 (\dim + 1)} \nonumber \\
    &+ \frac{\epsilon_{\alpha}^2}{\dim + 1} \lim_{t \to \infty} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \\
    &= 2 \lim_{t \to \infty} \norm{R_{\Phi}}.
\end{align}
As 



\begin{align}
    \frac{\prob{i \to j}}{\prob{j \to i}} &\approx \frac{\frac{1}{1 + e^{-\beta_E \gamma}}\sinc^2 ((\Delta(i,j) - \gamma)t)+ 3 \epsilon_{\sinc}}{\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((-\Delta(i,j) + \gamma)t) + 3 \epsilon_{\sinc}}  \nonumber \\
    \lim_{t \to \infty} &\implies e^{\beta_E \gamma} = e^{-\beta_E \Delta(i,j)} \nonumber \\
    &= \frac{\prob{\text{System in } j}}{ \prob{\text{System in } i}} \nonumber
\end{align}

\section{Proof of Lemma \ref{lem:the_double_duhamel}}


\section{Single Qubit System \& Environment}
We now will analyze the effects of $\Phi$ on thermalizing a system state with only a single qubit environment. The single qubit case makes the effect of tracing out the environment tractable. For absolute simplicity, we first study the case of two qubits. After this situation is analyzed in detail we extend the results to arbitrary system Hamiltonians. For the remainder of this section we assume a a system Hamiltonian $H_S = \begin{bmatrix}
    0 & 0 \\ 0 & \Delta_S
\end{bmatrix}$ and an environment Hamiltonian $H_E = \begin{bmatrix}
    0 & 0 \\ 0 & \gamma
\end{bmatrix}$. Further we will use an environment prepared in the state $\rho_E(\beta) = \frac{e^{-\beta H_E}}{\partfun_E(\beta)}$. Another restriction we will make is that we assume that $t \geq \frac{2}{\Delta_{\min} \sqrt{\epsilon_{\sinc}}}$, where $\lambda(i,j) = \lambda(k,l)$ or $\lambda(i,j) - \lambda(k,l) \geq \Delta_{\min}$, per Lemma \ref{lem:sinc_poly_approx}. 

Given the two qubit setup as mentioned, we want to compute the trace distance 
\begin{equation}
    \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1.
\end{equation}
We first start with the trace distance and reduce it to a computation of transition coefficients. 
\begin{align}
    \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1  &= \norm{\frac{\alpha^2}{2} \partrace{\hilb_E}{\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\rho_S(\beta) \otimes \rho_E(\beta)) \bigg|_{\alpha = 0} ~dG}}_1 + \bigo{\alpha^3} \\
    &\approx \norm{\sum_{i,j} e^{-\beta \lambda(i,j)} \partfun(\beta)^{-1}\partrace{\hilb_E}{ \frac{\alpha^2}{2} \int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{i,j})\bigg|_{\alpha=0} dG} }_1 \\
    &= \norm{\sum_{i,j} e^{-\beta \lambda(i,j)} \partfun(\beta)^{-1} \partrace{\hilb_E}{\sum_{k,l} \tau(i,j | k,l) \ketbra{k,l}{k,l}}}_1 \\
    &= \norm{\sum_{i,j} e^{-\beta \lambda(i,j)} \partfun(\beta)^{-1} \sum_{k,l} \tau(i,j |k,l) \ketbra{k}{k}}_1 \\
    &= \sum_k \abs{\sum_{i,j} e^{-\beta \lambda(i,j)} \partfun(\beta)^{-1} \sum_{l} \tau(i,j |k,l)} \\
    &= \frac{1}{|\partfun(\beta)|} \sum_k \abs{\sum_{i,j} e^{-\beta \lambda(i,j)} \sum_{l} \tau(i,j |k,l)} \label{eq:fixed_point_as_transition_coeffs}
\end{align}
\begin{align}
&\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_p^p \\
&= \norm{\rho_S(\beta_E) - \rho_S(\beta) - \frac{\alpha^2}{2} \partrace{\hilb_E}{\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\rho_S(\beta) \otimes \rho_E(\beta_E)) \bigg|_{\alpha=0} dG}}_p^p + \bigo{\alpha^3} \\
&\approx \norm{\rho_S(\beta_E) - \rho_S(\beta) - \sum_{i,j} \frac{e^{-\beta \lambda_S(i) -\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \frac{\alpha^2}{2}\partrace{\hilb_E}{\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{i,j}) \bigg|_{\alpha=0} dG} }_p^p \\
&= \norm{\rho_S(\beta_E) - \rho_S(\beta) - \sum_{i,j} \frac{e^{-\beta \lambda_S(i) -\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \partrace{\hilb_E}{\sum_{k,l} \tau(i,j|k,l) \ketbra{k,l}{k,l}} }_p^p \\
&= \norm{\rho_S(\beta_E) - \rho_S(\beta) - \sum_{i,j} \sum_{k,l} \frac{e^{-\beta \lambda_S(i) -\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|k,l) \ketbra{k}{k} }_p^p \\
&= \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} - \sum_{i,j,l} \frac{e^{-\beta \lambda_S(i) - \beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|k,l)}^p. \label{eq:trace_dist_p_norm}
\end{align}
We note that the last step is due to the fact that $\rho_S(x)$ is diagonal in the eigenbasis that we are working over. We drop the $\bigo{\alpha^3}$ term, we could possibly upper bound it by some additional error contribution $\epsilon_{\alpha}$ or by stating that since it is less than the $\alpha^2$ contribution we could simply add a factor of 2 in front of the norm. These details should be cleaned up. Our goal is to now compute these sums for fixed $k$ in 0,1.

\subsection{Fixed Points}

In the following we set $p=1$ and $\beta = \beta_E$.
    We make heavy use of Lemma \ref{thm:second_order_transition_coeffs}. 
    We now compute the term for $k=0, i=1$
    \begin{align}
        \sum_{j, l} e^{-\beta \lambda(1,j)} \tau(1,j| 0, l) &=  e^{-\beta \Delta_S} \parens{\tau(1, 0| 0, 0) + \tau(1, 0 | 0, 1)} +  e^{-\beta (\Delta_S + \gamma)} \parens{\tau(1, 1 | 0, 0) + \tau(1, 1| 0, 1)} \label{eq:single_qubit_fixed_pt_1} 
    \end{align}
    Now we compute the term for $k=0, i= 0$, which involves using the self-transition substitution for $\tau(a,b|a,b)$.
    \begin{align}
        \sum_{j,l} e^{-\beta \lambda(0,j)} \tau(0,j|0,l) &= e^{-\beta \cdot 0} \parens{\tau(0,0| 0,0) + \tau(0,0 | 0,1)} + e^{-\beta \gamma} \parens{\tau(0,1 | 0,0) + \tau(0,1|0,1)} \\
        &= -(\tau(0,0| 0,1) + \tau(0,0| 1,0) + \tau(0,0|1,1)) + \tau(0,0|0,1) \nonumber \\
        &~ + e^{-\beta \gamma} \parens{\tau(0,1|0,0) - (\tau(0,1|0,0) + \tau(0,1| 1,0) + \tau(0,1|1,1))} \\
        &= - \tau(0,0|1,0)  - \tau(0,0|1,1) - e^{-\beta \gamma} \tau(0,1|1,0) - e^{-\beta \gamma} \tau(0,1|1,1) \label{eq:single_qubit_fixed_pt_2}
    \end{align}
    Using the fact that $\tau(a,b|c,d) = \tau(c,d|a,b)$ we can add Eqs. \ref{eq:single_qubit_fixed_pt_1} and \ref{eq:single_qubit_fixed_pt_2} to get:
    \begin{align}
        \sum_{i,j} e^{-\beta \lambda(i,j)} \sum_l \tau(i,j|0,l) &= \tau(0,0|1,0)(e^{-\beta \Delta_S} -1) + \tau(0,1|1,0)(e^{-\beta \Delta_S} - e^{-\beta \gamma}) \nonumber \\
        &\quad + \tau(0,0,| 1,1) (e^{-\beta (\Delta_S + \gamma)} - 1) + \tau(0,1|1,1) (e^{-\beta (\Delta_S + \gamma)} - e^{-\beta \gamma})
    \end{align}
    We can now use the fact that $\tau(a,b|c,d)$ is positive for $(a,b) \neq (c,d)$ and that $\beta > 0, \Delta_S > 0$ and $\gamma > 0$ to rewrite the above as
    \begin{align}
        \sum_{i,j} e^{-\beta \lambda(i,j)} \sum_l \tau(i,j|0,l) &= \tau(0,1|1,0)(e^{-\beta \Delta_S} - e^{-\beta \gamma}) - \tau(0,0|1,0)(1 - e^{-\beta \Delta_S})  \nonumber \\
        &\quad - \tau(0,0,| 1,1) (1 - e^{-\beta (\Delta_S + \gamma)}) - \tau(0,1|1,1) e^{-\beta \gamma} (1 - e^{-\beta \Delta_S})
    \end{align}
    Now using the intution that non-degenerate transitions are going to be significantly suppressed, we use the triangle inequality. 
    \begin{align}
        &\bigg| \tau(0,1|1,0)(e^{-\beta \Delta_S} - e^{-\beta \gamma}) - \tau(0,0|1,0)(1 - e^{-\beta \Delta_S})  \nonumber \\
        &\quad - \tau(0,0,| 1,1) (1 - e^{-\beta (\Delta_S + \gamma)}) - \tau(0,1|1,1) e^{-\beta \gamma} (1 - e^{-\beta \Delta_S})\bigg| \\
        &\leq \abs{\tau(0,1|1,0) (e^{-\beta \Delta_S} - e^{-\beta \gamma})} + \abs{\tau(0,0|1,0)} + |\tau(0,0|1,1)| + |\tau(0,1|1,1)| \\
        &\leq \abs{\tau(0,1|1,0) (e^{-\beta \Delta_S} - e^{-\beta \gamma})} + 3 \frac{\epsilon_{\sinc} \alpha^2 t^2}{2(\dim + 1)} \\
        &= \frac{\alpha^2 t^2}{2(\dim + 1)} \parens{\sinc^2((\Delta_S - \gamma)t/2) |e^{-\beta \Delta_S} - e^{-\beta \gamma}| + 3 \epsilon_{\sinc}}\\
        &= \frac{\alpha^2 t^2}{2 (\dim + 1)} \parens{\sinc^2((\Delta_S - \gamma)t/2) e^{-\beta \Delta_S} |1 - e^{\beta (\Delta_S - \gamma)}| + 3 \epsilon_{\sinc}} \\
        &=: \frac{\alpha^2 t^2}{2(\dim + 1)} (e^{-\beta \Delta_S} g(\Delta_S - \gamma) + 3 \epsilon_{\sinc})
    \end{align}

    Now when computing the summation $|\sum_{i,j} e^{-\beta \lambda(i,j)} \sum_l \tau(i,j |1,l)|$, for $k=1$, the same result as $k=0$ is found. This allows us to say that the trace distance is upper bounded by
    \begin{equation}
        \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 \leq \frac{\alpha^2 t^2}{(\dim + 1)\partfun} (e^{-\beta \Delta_S} g(\Delta_S - \gamma) + 3 \epsilon_{\sinc}).
    \end{equation}
    We note that the following upper bound is sufficient to show that the trace distance is bounded:
    \begin{align}
        g(\Delta_S - \gamma) &= \sinc^2((\Delta_S - \gamma)t/2) \abs{1 - e^{\beta(\Delta_S - \gamma)}} \\
        &\leq \abs{e^{\beta(\Delta_S - \gamma)} - 1} \\
        &\leq e^{\beta \Delta_S} - 1,
    \end{align}
    where we assumed $\gamma \geq 0$ in the last step. This yields the following upper bound on the trace distance
    \begin{equation}
        \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 \leq \frac{\alpha^2 t^2}{\partfun (\dim+1)} (1 - e^{-\beta \Delta_S} + 3 \epsilon_{\sinc}).
    \end{equation}
    We can even drop the $e^{-\beta \Delta_S}$ term and have 
    $$\alpha^2 \leq \epsilon_{\beta} \frac{ \partfun (\dim + 1)}{t^2 (1 - e^{-\beta \Delta_S} + 3 \epsilon_{\sinc})}$$
    to imply $\norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 \leq \epsilon_{\beta}$.

    This is not the complete picture, however. We also have to satisfy the inequality $\alpha^2 t^2 / (\dim + 1) \leq 1$ in order for $\tau$ to represent valid transition probabilities. 


    Another weird thing is that as $\gamma \to \Delta_S$, aka we have a degenerate transition possible, it seems that the contribution of this transition to the trace distance vanishes? If you look at the temperature contributions we have $\tau(0,1|1,0)|e^{-\beta \Delta_S} - e^{-\beta \gamma}|$, which goes to zero as $\gamma \to \Delta_S$. This is seemingly contradictory, as we are getting probability mass shuffling between the energy levels. 
    
    Our goal is to produce an upper bound on $g(\Delta_S - \gamma)$. To do so there are two different approaches we explore. The first is to sample $\gamma$ from a probability distribution and look at the expected value of the trace distance. The easiest distribution to start with is a uniform distribution from $\Delta_S - \frac{1}{2} \epsilon \Delta_{\text{min}}$ to $\Delta_S + \frac{1}{2} \epsilon \Delta_{\text{min}}$. Within this range we can approximate $\sinc^2 ((\Delta_S - \gamma)t/2)$ as $1$ with only an error of at most $\epsilon$. 
    \begin{align}
        \int_{\Delta_S - \epsilon \Delta_{\text{min}}/2}^{\Delta_S + \epsilon \Delta_{\text{min}}/2} g(\Delta_S - \gamma) \prob{\gamma} d\gamma &= \int_{\epsilon \Delta_{\text{min}}/2}^{-\epsilon \Delta_{\text{min}}/2} g(u) \prob{u(\gamma)} (-du) \\
        &=\frac{1}{\epsilon \Delta_{\text{min}}} \int_{-\epsilon \Delta_{\text{min}}/2}^{\epsilon \Delta_{\text{min}}/2} \sinc^2(u t /2) |1 - e^{\beta u}| du \\
        &\leq  \parens{\frac{1}{\epsilon \Delta_{\text{min}}} \int_{-\epsilon \Delta_{\text{min}}/2}^{\epsilon \Delta_{\text{min}}/2} |1 - \epsilon| |1 - e^{\beta u}| du + 3 \epsilon_{\sinc}} \\
        &= \parens{\frac{1}{ \Delta_{\text{min}}} \parens{\frac{1}{\epsilon } - 1}\frac{2(\cosh (\beta \epsilon \Delta_{\text{min}}/2) - 1)  }{\beta} + 3 \epsilon_{\sinc}}
    \end{align}
    We see that in the limit as $\epsilon \to 0$, $\cosh(\beta \epsilon \Delta_{\min} /2) - 1 \in \bigo{\epsilon^2}$, so the total contribution is of order $\bigo{\epsilon}$. Similar logic holds for the limit as $\Delta_{\min} \to 0$. As $\beta \to \infty$, the $\cosh$ term simply approaches an exponential $e^{\beta \epsilon \Delta_{\min} /2} / \beta$. When looking at this as a contribution to the trace distance, we see that $e^{-\beta \Delta_S} g(\Delta_S - \gamma) \to e^{-\beta (\Delta_S - \epsilon \Delta_{\min}/2)} / \beta$, and since $\Delta_{\min} \leq \Delta_S$ by definition we have a vanishing trace distance.

    One thing to note is that if $\gamma$ is too large then we fall into the ``near-zero" approximation regime due to the large value of $t$. We had set $t$ such that for $\Delta \geq \Delta_{\min}$, then $\sinc^2(\Delta t/2) \leq \epsilon_{\sinc}$. This means that we require $\Delta_S - \gamma \leq \Delta_{\min}$ to fall within the window for any reasonable contribution from the $\sinc$. This gives us a window of $\pm \Delta_{\min}$ around $\Delta_S$ that we should have Now we let $\epsilon$ be a constant factor, say $\epsilon = 1/2$. This seems sufficient for now. Plugging in for the upper bound on the trace distance yields:
    \begin{equation}
        \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 \leq \frac{\alpha^2 t^2}{\partfun (\dim + 1)} \parens{ \frac{2 e^{-\beta \Delta_S}}{\beta \Delta_{\min}} (\cosh(\beta \Delta_{\min} / 4) - 1) + 3 \epsilon_{\sinc} }
    \end{equation}

    What if instead of integrating really close to $\Delta_S$, we actually integrate the function over some much larger interval? Would this work? 
    \begin{align}
        \int_{\Delta_{\min}}^{\Delta_{\max}} g(\Delta_S - \gamma) \prob{\gamma} d\gamma &= \int_{\Delta_{\min}}^{\Delta_{\max}} \sinc^2((\Delta_S - \gamma)t/2)  \frac{\abs{1 - e^{\beta (\Delta_S - \gamma)}}}{\Delta_{\max} - \Delta_{\min}} d\gamma \\
        &= \int_{\Delta_{\min}}^{\Delta_S} \sinc^2((\Delta_S - \gamma)t/2) \frac{e^{\beta (\Delta_S - \gamma)} - 1}{\Delta_{\max} - \Delta_{\min} } d\gamma + \int_{\Delta_S}^{\Delta_{\max}} \sinc^2((\Delta_S - \gamma)t/2) \frac{1 - e^{- \beta (\gamma - \Delta_S)}}{\Delta_{\max} - \Delta_{\min} } d\gamma
    \end{align}
 
\begin{align}
    \norm{A}_1 &= \trace{\sqrt{A A^\dagger}} \\
    &= \trace{\sqrt{\sum_i |a_i|^2 \ketbra{i}{i}}} \\
    &= \trace{\sum_i |a_i| \ketbra{i}{i}} \\
    &= \sum_i |a_i|.
\end{align}

\subsection{Distance Reduction}
We now would like to show that some distance metric between an input state $\rho_S(\beta)$ and the desired output state $\rho_S(\beta_E)$ is decreasing upon application of $\Phi$. The most straightforward metric to use is the $\norm{\cdot}_2^2$ distance. Again, we restrict ourselves to the single qubit system as defined at the beginning of the section.

\begin{align}
    \norm{\rho_s(\beta_E) - \Phi(\rho_S(\beta))}_2^2 &= \norm{\rho_S(\beta_E) - \rho_S(\beta) - \sum_{i,j,k,l} \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)} \tau(i,j|k,l) \ketbra{k}{k} + R_3(\Phi)} _2^2
\end{align}
To simplify notation, we note that $\norm{R_3(\Phi)}_2^2 \leq \norm{R_3(\Phi)}_1^2 \in \bigo{\alpha^6}$ and introduce $A = \rho_S(\beta_E) - \rho_S(\beta)$ and $B = \sum_{i,j,k,l} \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)} \tau(i,j|k,l) \ketbra{k}{k}$. By dropping terms of order $\bigo{\alpha^6}$ for now we can compute
\begin{align}
    \norm{\rho_s(\beta_E) - \Phi(\rho_S(\beta))}_2^2 &\approx \norm{A - B}_2^2 \\
    &= \trace{(A-B)^\dagger (A - B)} \\
    &= \trace{A^2} - 2 \trace{A B} + \trace{B^2} \\
    &= \norm{A}_2^2 + \norm{B}_2^2 - 2 \trace{A B}.
\end{align}
Now in order to show that $\norm{\rho_s(\beta_E) - \Phi(\rho_S(\beta))}_2^2 \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_2^2$, we need to show that $2 \trace{AB} \geq \trace{B^2}$.

First we note that since $A$ and $B$ are diagonal, we have $\trace{AB} = \sum_{k} A(k) B(k) = A(0)B(0) + A(1) B(1)$. We first compute the $k=0$ term and then $k=1$. For brevity, allow the following variables:
\begin{equation}
    a(i) = \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)}, \quad b(j) = \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)}.
\end{equation}

We first compute $B$, starting with $k=0$. We note that $\tau(i,j| k,l) = \tau(k,l |i,j)$ and that $\tau(i,j|i,j) = -\sum_{(a,b) \neq (i,j)} \tau(i,j| a,b)$.
\begin{align}
    B(0) &= \sum_{i,j,l} a(i) b(j) \tau(i,j|0,l) \\
    &= a(0) b(0) (\tau(0,0|0,0) + \tau(0,0|0,1)) + a(0)b(1) (\tau(0,1| 0,0) + \tau(0,1| 0,1)) \nonumber \\
    &\quad + a(1) b(0) (\tau(1,0|0,0) + \tau(1,0|0,1)) + a(1) b(1) (\tau(1,1|0,0) + \tau(1,1|0,1)) \\
    &= a(0) b(0) (-\tau(0,0|1,0) - \tau(0,0|1,1)) + a(0)b(1) (- \tau(0,1|1,0) -\tau(0,1|1,1)) \nonumber \\
    &\quad + a(1) b(0) (\tau(1,0|0,0) + \tau(1,0|0,1)) + a(1) b(1) (\tau(1,1|0,0) + \tau(1,1|0,1)) \\
    &= \tau(0,0|1,0) (-b(0) a(0) + b(0) a(1)) + \tau(0,0|1,1)(-b(0) a(0) + b(1) a(1)) \nonumber \\
    &\quad + \tau(0,1|1,0) (-b(1) a(0) + b(0) a(1)) + \tau(0,1|1,1)(-b(1) a(0) + b(1) a(1))
\end{align}
Next we compute $B(1)$ as
\begin{align}
    B(1) &= \sum_{i,j,l} \tau(i,j|1,l) \\
    &= a(0) b(0) (\tau(0,0|1,0) + \tau(0,0|1,1)) + a(0)b(1) (\tau(0,1| 1,0) + \tau(0,1| 1,1)) \nonumber \\
    &\quad + a(1) b(0) (\tau(1,0|1,0) + \tau(1,0|1,1)) + a(1) b(1) (\tau(1,1|1,0) + \tau(1,1|1,1)) \\
    &= a(0) b(0) (\tau(0,0|1,0) + \tau(0,0|1,1)) + a(0)b(1) (\tau(0,1| 1,0) + \tau(0,1| 1,1)) \nonumber \\
    &\quad + a(1) b(0) (- \tau(1,0|0,1) - \tau(1,0|0,0)) + a(1) b(1) (-\tau(1,1|0,0) - \tau(1,1|0,1)) \\
    &= \tau(0,0|1,0) (b(0) a(0) - b(0) a(1)) + \tau(0,0|1,1)(b(0) a(0) - b(1) a(1)) \nonumber \\
    &\quad + \tau(0,1|1,0) (b(1) a(0) - b(0) a(1)) + \tau(0,1|1,1)(b(1) a(0) - b(1) a(1)) \\
    &= - B(0).
\end{align}
This then leads to the equality $\trace{AB} = A(0) B(0) + A(1)B(1) = B(0) (A(0) - A(1))$. We investigate when each of these can be negative. 

\begin{align}
    A(0) - A(1) &= \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} - \frac{e^{-\beta_E \lambda_S(1)}}{\partfun_S(\beta_E)} + \frac{e^{-\beta \lambda_S(1)}}{\partfun_S(\beta)} \\
    &= \frac{e^{-\beta_E \lambda_S(0)} - e^{-\beta_E \lambda_S(1)}}{e^{-\beta_E \lambda_S(0)} + e^{-\beta_E \lambda_S(1)}} - \frac{e^{-\beta \lambda_S(0)} - e^{-\beta \lambda_S(1)}}{e^{-\beta \lambda_S(0)} + e^{-\beta \lambda_S(1)}} \\
    &= \frac{e^{\beta_E \Delta_S} - 1}{e^{\beta_E \Delta_S} + 1} - \frac{e^{\beta \Delta_S} - 1}{e^{\beta \Delta_S} + 1} \\
    &= \tanh(\beta_E \Delta_S / 2) - \tanh(\beta \Delta_S / 2).
\end{align}
Since $\tanh$ is a monotonic function, $\beta_E \geq \beta$ implies $A(0) - A(1) \geq 0$. For ease of notation, we define $T(\delta) \coloneqq \tanh(\beta_E \Delta_S /2) - \tanh((\beta_E -\delta)\Delta_S/2)$, where $\beta = \beta_E - \delta$.

 In order to determine what kind of bound should be used, we first simplify the overall expression:
\begin{align}
    \trace{A^2} - 2 \trace{AB} + \trace{B^2} &= \trace{A^2} - 2 (A(0) B(0) + A(1)B(1)) + B(0)^2 + B(1)^2 \\
    &= \trace{A^2} - 2 B(0)(A(0) - A(1)) + 2 B(0)^2 \\
    &= \trace{A^2} + 2 B(0)(B(0) - A(0) + A(1)) \label{eq:dist_decrease_a_and_b}
\end{align}
As our goal is to show $\norm{A - B}_2^2 \leq \norm{A}_2^2 - \epsilon$, we want to show $2 B(0) (B(0) -A(0) + A(1)) \leq - \epsilon$.

Now we work on bounding $B(0)$. We note that most of the non-energy preserving transitions (i.e all other than $\tau(0,1|1,0)$) can be bounded by a trivial bound of 0. Working term by term, along with the facts: $a(i) \geq 0$, $b(i) \geq 0$, and $i \leq j \implies a(i) \geq a(j) \& b(i) \geq b(j)$, $\tau(i,j|k,l) \geq 0$ if $(i,j) \neq (k,l)$ we get the following
\begin{align}
    \tau(0,0|1,0) b(0)(a(1) - a(0)) &\leq 0 \\
    \tau(0,0|1,1) (-b(0) a(0) + b(1) a(1)) &\leq \tau(0,0|1,1) b(0) (-a(0) + a(1)) \leq 0 \\
    \tau(0,1|1,1) b(1) (-a(0) + a(1)) &\leq 0.
\end{align}
This produces the bound $B(0) \leq \tau(0,1|1,0) (a(1)b(0) - a(0)b(1))$. Our goal is to further reduce this upper bound:
\begin{align}
    B(0) &\leq \tau(0,1|1,) \frac{1}{\partfun_E(\beta_E) \partfun_S(\beta)}\parens{e^{-\beta \lambda_S(1)} e^{-\beta_E \lambda_E(0)} - e^{-\beta \lambda_S(0)} e^{-\beta_E \lambda_E(1)}} \\
    &= \tau(0,1|1,0) \frac{e^{-\beta \lambda_S(1)}}{\partfun_E(\beta_E) \partfun_S(\beta)} (1 - e^{\beta \Delta_S - \beta_E \gamma })
\end{align}
Now we see that if $\beta \Delta_S \geq \beta_E \gamma$ then our bound becomes negative. If we let $\beta = \beta_E - \delta$ we have $\beta \Delta_S - \beta_E \gamma = \beta_E o$.
However, we could simply use the dumb bound:
\begin{align}
    B(0) &\leq \tau(0,1|1,0) (a(1) b(0) - a(0) b(1)) \\
    &\leq \tau(0,1|1,0) a(1) b(0) \\
    &\leq \frac{\alpha^2 t^2}{\dim + 1} \sinc^2((\Delta_S -\gamma)t/2)
\end{align}
We will use this for now.


The next tast is to construct a lower bound for $B(0)$. For this we repeat a similar term-by-term process as above. For a lower bound, however, we can take advantage that the non-degenerate transitions are upper bounded by $\epsilon_{\sinc}$. We also have $a(i),b(j) \leq 1$ as they are Boltzmann factors.
\begin{align}
    \tau(0,0|1,0) b(0) (a(1) - a(0)) \geq - \tau(0,0|1,0) a(0) b(0) \geq - \epsilon_{\sinc} \\
    \tau(0,0|1,1)(-b(0) a(0) + b(1)a(1)) \geq - \tau(0,0|1,1) a(0) b(0) \geq -\epsilon_{\sinc} \\
    \tau(0,1|1,1) b(1)(-a(0) + a(1) \geq - \tau(0,1|1,1) a(0)b(1) \geq - \epsilon_{\sinc}
\end{align}
The one remaining term is
\begin{align}
    \tau(0,1|1,0) (a(1)b(0) - a(0)b(1)) &= \frac{\alpha^2 t^2 \sinc^2((\Delta_S - \gamma) t/2)}{\dim + 1} \frac{1}{\partfun_E(\beta_E) \partfun_S(\beta)}(e^{-\beta \lambda_S(1) - \beta_E \lambda_E(0)} - e^{-\beta \lambda_S(0) - \beta_E \lambda_E(1)}) \\
    &= \frac{\alpha^2 t^2 \sinc^2((\Delta_S - \gamma) t/2)}{\dim + 1} \frac{e^{-\beta \lambda_S(1)}}{\partfun_E(\beta_E) \partfun_S(\beta)} (1 - e^{\beta \Delta_S - \beta_E \gamma})
\end{align}
Now we let $\beta = \beta_E - \delta$ and $\gamma = \Delta_S + \widetilde{\gamma}$. We get
\begin{align}
    1 - e^{(\beta_E - \delta)\Delta_S - \beta_E (\Delta_S + \widetilde{\gamma})} &= 1 - e^{-\delta \Delta_S - \beta_E \Delta_S - \beta_E \widetilde{\gamma}} \\
    &= 1 - e^{-\beta_E \Delta_S (1 - \delta / \beta_E + \widetilde{\gamma} / \Delta_S)}
\end{align}
We see that this is positive so long as $\widetilde{\gamma} \geq -\Delta_S(1 - \frac{\delta}{\beta_E})$. This is a fairly significant range if $\Delta_S$ is much larger than $\Delta_{\min}$. 

However, we can get a much simpler bound if we simply ignore these prefactors. We could bound it by:
\begin{align}
    \tau(0,1|1,0)(a(1) b(0) - a(0) b(1)) &\geq - \tau(0,1|1,0) a(0) b(1) \\
    &\geq - \frac{\alpha^2 t^2}{\dim + 1} \sinc^2((\Delta_S - \gamma)t/2)
\end{align}
Which yields $B(0) \geq -\parens{3 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} \sinc^2((\Delta_S - \gamma)t/2)}$.

Now that we have upper and lower bounds, we return to the original task of bounding the distance decrease. Our first goal is to upper bound $2 B(0)^2$. The issue we have is that $B(0)$ could be negative, in which case $\text{LB} \leq B(0) ~\&~ B(0) \leq \text{UB} \implies B(0)^2 \leq \min \set{\text{LB}^2, \text{UB}^2}$. So given the bounds we have computed and noting that $\epsilon_{\sinc} \geq 0$, we have $B(0)^2 \leq \frac{\alpha^2 t^2}{\dim  + 1} \sinc^2((\Delta_S - \gamma)t/2)$. 
\begin{align}
    &2 B(0)^2 - 2 B(0) (A(0) - A(1)) \\
    &\leq \parens{\frac{2 \alpha^2 t^2}{\dim + 1} \sinc^2 ((\Delta_S -\gamma)t/2)}^2 - 2\parens{3 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} \sinc^2 ((\Delta_S -\gamma)t/2)} T(\delta) \\
    &= - \epsilon
\end{align}
As we expect the $\alpha^4$ term to be dominated by the $\alpha^2$ term, we have that 
$$\epsilon \leq 2 (3 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} \sinc^2((\Delta_S - \gamma)t/2) T(\delta) - \parens{\frac{2 \alpha^2 t^2}{\dim + 1} \sinc^2 ((\Delta_S -\gamma)t/2)}^2.$$
Now we see the importance of knowing $\Delta_S$ to high precision, if $\Delta_S - \gamma \geq \Delta_{\min}$, then we see that $\sinc^2((\Delta_S - \gamma)t/2) \leq \epsilon_{\sinc}$ and we have that $\epsilon \in \bigo{\epsilon_{\sinc}}$. Since $\epsilon_{\sinc}$ is a user-defined paramater that we would like to take as small as possible, this would severely limit the distance that we converge to. Further, note that as $\delta \to 0$ $T(\delta) \to 0$, and we have that $\epsilon$ becomes negative, implying that our bounds break down at some point. 

We can then say that $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_2^2 \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_2^2 - \epsilon$. By setting $\epsilon = \epsilon' \norm{\rho_S(\beta_E) - \rho_S(\beta)}_2^2$, we have
\begin{equation}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_2 \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_2 \sqrt{1 - \epsilon'}
\end{equation}
It follows from standard Sch\"atten norm inequalities that $\norm{X}_2 \leq \sqrt{\dim} \norm{X}_{\infty} \leq \sqrt{\dim} \norm{X}_1$, which we can use to say:
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 &\leq  \sqrt{\dim} \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_2 \\
    &\leq \parens{\dim \sqrt{1 - \epsilon'}} \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1
\end{align}

\end{document}
