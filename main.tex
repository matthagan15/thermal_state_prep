\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amsthm, amssymb}
\usepackage[margin=1.5cm]{geometry}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}
\usepackage{todonotes}
\usepackage{nicefrac}
\usepackage{mathrsfs}
\usepackage{tikz}

%%%%%%%%    THEOREM DEFINITIONS AND RESTATABLE
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{todonotes}

\newcommand{\matt}[1]{\todo[color=red!50, prepend, caption={Matt}, tickmarkheight=0.25cm]{#1}}
\newcommand{\note}[1]{\emph{Note: #1}}
\newcommand{\conjecture}[1]{ \noindent\emph{\textbf{Conjecture:}} \emph{ #1 }}




%%%%%%%%    NOTATION DEFINITIONS FOR EASIER WRITING
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\ketbra}[2]{| #1\rangle\! \langle #2|}
\newcommand{\parens}[1]{\left( #1 \right)}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left| \left| #1 \right| \right|}
\newcommand{\diamondnorm}[1]{\left| \left| #1 \right| \right|_\diamond}
\newcommand{\anglebrackets}[1]{\left< #1 \right>}
\newcommand{\overlap}[2]{\anglebrackets{#1 , #2 }}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\openone}{\mathds{1}}
\newcommand{\expect}[1]{\mathbb{E}\brackets{#1}}
\newcommand{\prob}[1]{\text{Pr}\left[ #1 \right]}
\newcommand{\bigo}[1]{\mathcal{O}\left( #1 \right)}
\newcommand{\bigotilde}[1]{\widetilde{\mathcal{O}} \left( #1 \right)}
\newcommand{\ts}{\textsuperscript}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\trace}[1]{\Tr \brackets{ #1 }}
\newcommand{\partrace}[2]{\Tr_{#1} \brackets{ #2 }}
\newcommand{\complex}{\mathbb{C}}

%%%%% COMMONLY USED OBJECTS
\newcommand{\hilb}{\mathscr{H}}
\newcommand{\partfun}{\mathcal{Z}}
\newcommand{\gue}{\rm GUE}
\DeclareMathOperator{\hermMathOp}{Herm}
\newcommand{\herm}[1]{\hermMathOp\parens{#1}}


\title{Thermal State Prep}
\author{Nathan Wiebe, Matthew Hagan}
\date{May 2022}

\begin{document}

\maketitle

\section{Introduction}
The computational task of preparing states of the form $\frac{e^{-\beta H}}{\partfun}$ in a usable approximation on a quantum computer is both incredibly useful and difficult. The closely related problem of estimating the free energy, defined as $F = -(1/\beta) \log \partfun$, up to additive error is known to be QMA-Hard for 2-local Hamiltonians \cite{bravyi_complexity_2021}. Intuitively, finding the ground state of a $k$-local Hamiltonian is known to be QMA-Hard \cite{} and the thermal state $e^{-\beta H} \partfun^{-1}$ can have arbitrarily high overlap with the ground state as $\beta \to \infty$ or the temperature approaches 0, indicating the problem for all computationally accessible $H$ and $\beta$ seems to be an incredibly difficult problem. 

The difficulty of preparing thermal states is offset by their incredible utility. Whenever low energy states are needed, such as when training Boltzmann machines \cite{}, preparing ground states for quantum error correcting codes \cite{}, or starting states for chemistry simulations \cite{}, thermal states correspond to natural choices for starting states. Even more broadly, thermal states can be used as resource states for SemiDefinite Program (SDP) solvers via a Quantum Aurora-Kale algorithm given by Brandao et al. \cite{}. 

Want to include here the main methods for preparing thermal states that currently exist, mainly the Poulin and Wocjan paper and Quantum-Quantum Metropolis Hastings. Where should more niche/advanced methods go?

\section{Preliminaries}
We denote the Hilbert space of the system as $\hilb_{sys}$ and the environment as $\hilb_{env}$. The algorithm we propose for preparing thermal states for a Hamiltonian $H_{sys} \in \herm{\hilb_{sys}}$ involves direct time simulation of the system coupled to an environment governed by $H_{env} \in \herm{\hilb_{env}}$. We denote the interaction $H_{int} \in \herm{\hilb_{sys} \otimes \hilb_{env}}$. At a broad level, the algorithm starts from some initial state (what is the distribution over inputs?) we denote as $\rho$, undergoes time evolution with $H = H_{sys} + H_{env} + H_{int}$ for some time $t$, after which we trace out the environment leaving
\begin{equation}
    \mathcal{P}(\rho) = \partrace{Env}{e^{i H t} \rho e^{-i H t}}.
\end{equation}
The goal of this paper is to provide subsets of possible systems, environments, and interactions that can lead to efficient thermalization. 


\section{Harmonic Oscillators}
The first system we want to study is the two coupled Harmonic oscillators at first to get the machinery working and then extend one of the systems to something more general. We consider two systems, $\hilb_1 \otimes \hilb_2$, with two "base" hamiltonians $H_1 \otimes \openone$ and $\openone \otimes H_2$, where $H_1, H_2$ are finite approximations to a harmonic oscillator. For notation, we write $H_1 = \sum_{i = 0}^{n-1} \lambda_i u_i u_i^*$ and $H_2 = \sum_{j = 0}^{m} \nu_j v_j v_j^*$. Then we let $H = H_1 \otimes \openone + \openone \otimes H_2$, which has eigenvalue decomposition $H = \sum_{i,j} (\lambda_i + \nu_j) (u_i \otimes v_j)(u_i \otimes v_j)^*$. For simplicity, we will also use the notation $\eta_{i,j} := \lambda_i + \nu_j$ and $\Pi_{i,j} := (u_i \otimes v_j) (u_i \otimes v_j)^*$. We then consider adding in a perturbation $\alpha G$, where $\alpha \in [0, \infty)$ is a real coupling constant and $G$ is sampled from the Gaussian Unitary Ensemble (GUE) $G \sim GUE$. We denote perturbed expressions with tildes, so our perturbed hamiltonian is $\widetilde{H} = H + \alpha G$ with eigenvalues $\widetilde{\eta}_{i,j}$ and eigenspace projectors $\widetilde{\Pi}_{i,j}$. 

The channel we would like to replicate is
\begin{equation}
    \Phi(\rho) = \partrace{\hilb_2}{\int dG e^{+i \widetilde{H} t} \rho e^{- i \widetilde{H} t}}.
\end{equation}
Our goal is to show that for some performance metric this channel takes thermal states defined for $H_1, H_2$ and outputs a thermal state on $\hilb_1$ with a lower temperature than it began with. In other words, if we let $d(\rho, \sigma)$ represent the metric of interest then we would like $d \parens{ \partfun_3^{-1} e^{-\beta_3 H_1}, \Phi\parens{\partfun_1^{-1} e^{-\beta_1 H_1} \otimes \partfun_2^{-1}e^{-\beta_2 H_2}} }$ to be small (if we think of it as a distance, high if it's some kind of overlap). There are a few various metrics we could look at: the trace distance, fidelity, difference in overlap on a single observable, or the difference in overlap averaged over a distribution over all possible observables. 

\begin{itemize}
    \item Fidelity: $F(P, Q) := \trace{\sqrt{\sqrt{P} Q \sqrt{P}}}$. The biggest issue I could see here is the square root, it seems like the thing in the square root might be fairly straightforward to compute. 
    \item Trace distance: $\norm{P - Q}_1 = \trace{\sqrt{(P-Q)^*(P-Q)}}$
    \item Overlap: $\anglebrackets{P, Q} = \trace{P^* Q}$, similar to dot product for vectors. This is not super useful, note if $P = Q$ then the overlap isn't even one and is more a measure of the purity of the state. 
    \item Measurement statistics for a single observable: Let $\mu(i)$ denote a measurement, could correspond to eigenspace projectors for some Hermitian observable. Then the vector defined by $\overlap{\mu(i)}{P}$ denotes the probability vector for the measurement $\mu$. We could take the Kullback-Leibler divergence of these vectors for $P, Q$.
    \item Measurement statistics for an ensemble of observables: The idea behind this is that the trace distance corresponds to an upper bound on the distinguishability between $P,Q$ as defined by their measurement statistics. If instead of looking at the maximum, what if we specify a distribution over possible observables and then look at the measurement statistic differences? 
\end{itemize}

\subsection{Simplifying $\Phi(\rho)$ with Perturbation Theory}
The goal is to use perturbation theory to write the output of the channel in terms of powers of the coupling strength $\alpha$. It remains to be seen if first order perturbation theory will be enough. We also are specifically interested when $\rho$ is a product state of thermal states, in other words $\rho_{PS} = \frac{e^{-\beta_1 H_1}}{\partfun_1} \otimes \frac{e^{-\beta_2 H_2}}{\partfun_2}$. We also denote the action of our channel with respect to a particular choice of $G$ as $\Phi_G$, which combined with linearity of partial traces gives $\Phi = \int dG \Phi_G$. 
\begin{align}
    \Phi_G(\rho_{PS}) &= \partrace{2}{e^{+i \widetilde{H} t} \rho_{PS} e^{-i \widetilde{H} t}} \\
    &= \sum_{k} \openone \otimes v_k^* \parens{\sum_{i, j} e^{i \widetilde{\eta}_{i, j} t} \widetilde{\Pi}_{i,j}} \rho_{PS} \parens{ \sum_{m, n} e^{i \widetilde{\eta}_{m, n} t} \widetilde{\Pi}_{m,n} }
\end{align}
Our next goal is to analyze $e^{i \widetilde{\eta}_{i,j} t} \widetilde{\Pi}_{i,j}$ using time-independent perturbation theory. Our goal is to write $e^{i \widetilde{\eta}_{i,j} t}$ as a power series in terms of $\alpha$, which is given by perturbation theory for $\widetilde{H} = H + \alpha G$. We first write the perturbation series for $\widetilde{\eta}$ as 
\begin{equation}
    \widetilde{\eta}_{i,j} = \eta_{i,j} + \alpha (u_i \otimes v_j)^* G (u_i \otimes v_j) + \alpha^2 \sum_{(i',j') \neq (i,j)}  \frac{\abs{(u_i \otimes v_j)^* G (u_{i'} \otimes v_{j'})}^{2}}{\eta_{i,j} - \eta_{i',j'}} + \bigo{\alpha^3}.
\end{equation}
To simplify the notation, we introduce the indexing $G(i,j,k,l) := (u_i \otimes v_j)^* G (u_k \otimes v_l)$. $G(i,j)$ without a second pair of indices denotes the diagonal matrix element $(u_i \otimes v_j)^* G (u_i \otimes v_j)$. The Taylor's Series for $e^{i \widetilde{\eta}_{i,j} t}$ is
\begin{align}
    e^{i \widetilde{\eta}_{i,j} t} &= e^{i \eta_{i,j} t} \\
    &\quad + \frac{\partial \widetilde{\eta}_{i,j}}{\partial \alpha} e^{i \widetilde{\eta}_{i,j}t} \bigg|_{\alpha = 0} i \alpha t \\
    &\quad + \frac{ i\alpha^2 t}{2!} \frac{\partial^2 \widetilde{\eta}_{i,j}}{\partial \alpha^2} e^{i \widetilde{\eta}_{i,j}t}\bigg|_{\alpha=0} \\
    &\quad - \frac{\alpha^2 t^2}{2!} \parens{\frac{\partial \widetilde{\eta}_{i,j}}{\partial \alpha}}^2 e^{i \widetilde{\eta}_{i,j} t} \bigg|_{\alpha=0} + \bigo{\alpha^3} .
\end{align}

\subsection{Simplifying $\Phi(\rho)$ with Trotter}
The idea here is to try and see what very short time evolution does to our channel. At the very least this might give some intuition as to what is happening?
\begin{align}
    \Phi_G(\rho_{PS}) &= \partrace{2}{e^{+i\widetilde{H}t} \rho_{PS} e^{-i \widetilde{H} t}} \\
    &= \partrace{2}{\parens{e^{i G \alpha t} e^{i H t} + \bigo{t^2}} \rho_{PS} \parens{e^{-iHt} e^{-i G \alpha t} + \bigo{t^2}}} \\
    &= 
\end{align}


\bibliographystyle{unsrt}
\bibliography{bib}

\end{document}
