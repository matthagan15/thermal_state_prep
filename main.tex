\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amsthm, amssymb}
\usepackage[margin=3cm]{geometry}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}
\usepackage{todonotes}
\usepackage{nicefrac}
\usepackage{mathrsfs}
\usepackage{tikz}
\usepackage{thm-restate}
\usepackage{hyperref}

\usepackage{etoc}

%%%%%%%%    THEOREM DEFINITIONS AND RESTATABLE
% \newcounter{claim}
% \setcounter{claim}{0}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{dependency}{Dependency}
\newtheorem{definition}{Definition}

\newcommand{\matt}[1]{\todo[color=red!50, prepend, caption={Matt}, tickmarkheight=0.25cm]{#1}}
\newcommand{\inlinetodo}[1]{\textcolor{red}{{\Large TODO:} #1}}




%%%%%%%%    NOTATION DEFINITIONS FOR EASIER WRITING
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\ketbra}[2]{| #1\rangle\! \langle #2|}
\newcommand{\parens}[1]{\left( #1 \right)}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left| \left| #1 \right| \right|}
\newcommand{\diamondnorm}[1]{\left| \left| #1 \right| \right|_\diamond}
\newcommand{\anglebrackets}[1]{\left< #1 \right>}
\newcommand{\overlap}[2]{\anglebrackets{#1 , #2 }}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\openone}{\mathds{1}}
\newcommand{\expect}[1]{\mathbb{E}\brackets{#1}}
\newcommand{\variance}[1]{\textit{Var} \brackets{ #1 }}
\newcommand{\prob}[1]{\text{Pr}\left[ #1 \right]}
\newcommand{\bigo}[1]{O\left( #1 \right)}
\newcommand{\bigotilde}[1]{\widetilde{O} \left( #1 \right)}
\newcommand{\ts}{\textsuperscript}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\trace}[1]{\Tr \brackets{ #1 }}
\newcommand{\partrace}[2]{\Tr_{#1} \brackets{ #2 }}
\newcommand{\complex}{\mathbb{C}}

%%%%% COMMONLY USED OBJECTS
\newcommand{\hilb}{\mathcal{H}}
\newcommand{\partfun}{\mathcal{Z}}
\newcommand{\identity}{\mathds{1}}
\newcommand{\gue}{\rm GUE}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\hermMathOp}{Herm}
\DeclareMathOperator{\im}{Im}
\newcommand{\herm}[1]{\hermMathOp\parens{#1}}


\title{Thermal State Prep}
\author{Matthew Hagan, Nathan Wiebe}
\date{May 2022}

\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Going to leave this blank for now. \cite{shiraishi_undecidability_2021}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
We denote the Hilbert space of the system as $\hilb_{S}$ and the environment as $\hilb_{E}$, with the Hamiltonians governing each as $H_{S}$ and $H_{E}$. We will assume without loss of generality that the system's Hilbert space can be encoded with $n$ qubits, giving $\dim_S = 2^{n}$, and the environment's Hilbert space can be encoded with $m$ qubits giving $\dim_E = 2^{m}$. The Hamiltonian for the joint system on $\hilb_{S} \otimes \hilb_{E}$ is then $H = H_{S} \otimes \identity + \identity \otimes H_{E}$. The Hilbert space of the combined system and environment is of dimension $\dim = \dim_E \cdot \dim_S = 2^{n + m}$. 

We will primarily work in the eigenbasis for each Hamiltonian:
\begin{equation}
    H_{S} = \sum_{i = 0}^{2^n - 1} \lambda_S(i) \ketbra{s_i}{s_i} ~,~ H_{E} = \sum_{j=0}^{2^m - 1} \lambda_E(j) \ketbra{e_j}{e_j} ~,~ H = \sum_{i=0}^{2^n - 1} \sum_{j=0}^{2^m - 1} \lambda(i,j) (\ket{s_i} \otimes \ket{e_j})(\bra{s_i} \otimes \bra{e_j}),
\end{equation}
for convenience we will denote the tensor product of eigenvectors simply by their indices $\ket{i,j} \coloneqq \ket{s_i} \otimes \ket{e_j}$. For convenience we define $\lambda(i,j) \coloneqq \lambda_S(i) + \lambda_E(j)$. We also make use of the following notation for the energy differences of the system-environment Hamiltonian
$$\Delta(i,j|k,l) \coloneqq \lambda(i,j) - \lambda(k,l),$$
and will use $\Delta_S(i,j) \coloneqq \lambda_S(i) - \lambda_S(j)$ for just the system differences. We use the notation $\delta(i,j|k,l)$ to denote the product of Kronecker delta functions $\delta(i,j|k,l) = \delta_{i,k} \delta_{j,l}$.

For input states we will typically assume thermal states of the form $\rho_S(\beta) = \frac{e^{-\beta H_S}}{\partfun_S}$, where $\partfun_S = \trace{e^{-\beta H_S}}$, where the inverse temperature $\beta$ of the partition function will typically be assumed but written explicitly if need be. We will assume environment states of the form $\rho_E(\beta) = \frac{e^{-\beta H_E}}{\partfun_E}$.


Overall one application of our channel is represented as
\begin{equation}
    \Phi(\rho ; \alpha, \beta_E, t) :=  \partrace{\hilb_E}{\int e^{+i(H + \alpha G)t} \rho \otimes \rho_E(\beta_E) e^{-i(H + \alpha G) t} dG},
\end{equation}
and we will typically let the parameters $\alpha, \beta_E,$ and $t$ for the channel be implicit. It will prove convenient to study the time evolution of a fixed interaction as a map from the total system-environment Hilbert space to itself. We denote this channel for a fixed interaction $G$ as
\begin{equation}
    \Phi_G(\rho_S \otimes \rho_E(\beta_E) := e^{+i (H+ \alpha G) t} \rho_S \otimes \rho_E(\beta_E) e^{-i (H + \alpha G) t}. \label{eq:phi_g_definition}
\end{equation}
Clearly then $\Phi(\rho_S) = \partrace{\hilb_E}{\int \Phi_G (\rho_S \otimes \rho_E) dG}$. We use $G$ to denote the randomized interaction term, where $G = U_G D U_G^\dagger$. The measure we choose for the eigenbasis of $G$ is $U_G \sim Haar$ and the eigenvalues are i.i.d with mean 0 and variance $1$.  This gives the overall interaction measure as the decomposition $\int dG = \int \int dD ~ dU_G$. The interaction strength of $G$ will be controlled through the coupling coefficient $\alpha$.

\begin{restatable}{lemma}{haar_two_moment} \label{lem:haar_two_moment}
    Let $\int (\cdot) dU$ denote the average distributed according to the Haar measure over $\dim$-dimensional unitary matrices $U$. Then for $\ket{i_1},\ket{i_2},\ldots,\ket{k_2}$ drawn from an orthonormal basis
    \begin{align}
        &\int \bra{i_1} U \ket{j_1} \bra{i_2} U \ket{j_2} \bra{k_1} U^\dagger \ket{l_1} ~ \bra{k_2} U^\dagger \ket{l_2} dU \nonumber \\
        &= ~\frac{1}{\dim^2 - 1} \parens{\delta_{i_1, l_1} \delta_{j_1, k_1} \delta_{i_2, l_2} \delta_{j_2, k_2} + \delta_{i_1, l_2} \delta_{j_1, k_2} \delta_{i_2, l_1} \delta_{j_2, k_1}} \nonumber \\
        &\quad - \frac{1}{\dim(\dim^2 - 1)} \parens{\delta_{i_1, l_2} \delta_{j_1, k_1} \delta_{i_2, l_1} \delta_{j_2, k_2} + \delta_{i_1, l_1} \delta_{j_1, k_2} \delta_{i_2, l_2} \delta_{j_2, k_1}}. \label{eq:haar_two_moment_integral}
    \end{align}
\end{restatable}

\begin{lemma}[Sinc Function Bounds] \label{lem:sinc_poly_approx}
    The following implications hold 
    \begin{align}
        |x| \leq \sqrt{10 \epsilon_{\sinc}/7} &\implies \sinc^2(x) \geq 1 - \epsilon_{\sinc} \label{eq:sinc_lower_bound}\\
        |x| \geq 1 / \sqrt{\epsilon_{\sinc}} &\implies \sinc^2(x) \leq \epsilon_{\sinc}. \label{eq:sinc_upper_bound}
    \end{align}
    % The constant approximation $f(x) = 1$ has error $|f(x) - 1| \leq \widetilde{\epsilon}_{\sinc}$ if $|x| \leq \sqrt{2 \widetilde{\epsilon}_{\sinc}}$. This leads to the observation that $\widetilde{\epsilon}_{\sinc}$ acts as a lower bound for $f(x)$, as $|x| \leq \sqrt{2 \widetilde{\epsilon}_{\sinc}}$ implies $f(x) \geq 1 - \widetilde{\epsilon}_{\sinc}$. We denote this upper bound with $\Delta_{\sinc} \coloneqq \sqrt{2 \widetilde{\epsilon}_{\sinc}}$. 
    % We also require a lower bound $\epsilon_{\sinc}$, such that $|x| \geq \Delta_{\min} \implies f(x) \leq \epsilon_{\sinc}$. Setting $\Delta_{\min} = 1 / \sqrt{\epsilon_{\sinc}}$ guarantees this to hold. 
\end{lemma}
\begin{proof}
    We start with a Taylor Series for $\sinc^2$, which we compute using the expression of $\sinc$ as $\sinc(x) = \frac{\sin x}{x} = \int_0^1 \cos(sx) ds$. We compute the first two derivatives as
    \begin{align}
        \frac{d \sinc^2(x)}{dx} &= -2 \int_0^1 \sin(sx) s ds \int_0^1 \cos(sx) ds \\
        \frac{d^2 \sinc^2(x)}{dx^2} &= -2 \int_0^1 \cos(sx)s^2 ds \int_0^1 \cos(sx) ds + 2\int_0^1 \sin(sx) s ~ds \int_0^1 \sin(sx) s ~ds.
    \end{align}
    We can evaluate each of these derivatives about the origin using continuity of the derivatives along with the limits $\lim_{x \to 0} \cos(sx) = 1$ and $\lim_{x \to 0} \sin(sx) = 0$. We can now compute the Maclaurin Series for some $x_{\star} \in [0,1]$ as
    \begin{align}
        f(x) &= f(0) + x \frac{df}{dx}\bigg|_{x = 0} + \frac{x^2}{2!} \frac{d^2f}{dx^2}\bigg|_{x = x_{\star}}.
    \end{align}
    Plugging in $\sinc^2(0) = 1$ and $\frac{d\sinc^2(x)}{dx}\big|_{x = 0} = 0$ then yields $|\sinc^2(x) - 1| = \frac{|x|^2}{2} \abs{\frac{d^2\sinc^2(x)}{dx^2}(x_{\star})}$. We make use of the rather simplistic bound
    \begin{align}
        \abs{\frac{d^2\sinc^2(x)}{dx^2}(x_{\star})} &\leq 2 \abs{\int_0^1 \cos(sx) s^2 ds \int_0^1 \cos(sx) ds} + 2\abs{\int_0^1 \sin(sx) s ds \int_0^1 \sin(sx) s ds} \\
        &\leq 2 \int_0^1 \abs{\cos(sx)} s^2 ds \int_0^1 \abs{\cos(sx)} ds + 2\parens{\int_0^1 \abs{\sin(sx)} |s| ds}^2 \\
        &\leq 2 \int_0^1 s^2 ds + 2\parens{\int_0^1 s ds}^2 \\
        &\leq 2/3 + 1/2 = 7/6.
    \end{align}
    This yields the final inequality $|\sinc^2(x) - 1| \leq \frac{7|x|^2}{10}$. We then see that $|x| \leq \sqrt{10 \widetilde{\epsilon}_{\sinc}/7}$ implies $|f(x) - 1| \leq \widetilde{\epsilon}_{\sinc}$. 

    The upper bound of $\epsilon_{\sinc}$ for large $|x|$ is relatively straightforward:
    \begin{align}
        f(x) &= \frac{\sin^2(x)}{x^2} \\
            &\leq \frac{1}{|x|^2},
    \end{align}
    where we see that $|x| \geq 1 / \sqrt{\epsilon_{\sinc}}$ implies $\sinc^2(x) \leq \epsilon_{\sinc}$.
\end{proof}

We will often rely on a particularly parametrized form of $f(x)$ which is worth investigating on it's own right. Note we can get a square root improvement of the dependence of $|\Delta_S(i,j) - \gamma|$ on $\epsilon_{\sinc}$ in the below Corallary if we only require $f(x) \geq 1 - \widetilde{\epsilon}_{\sinc}$. This then requires $|\Delta_S(i,j) - \gamma| \in \bigo{\sqrt{\epsilon_{\sinc} \widetilde{\epsilon}_{\sinc}}}$, however this will not prove significantly useful for us so we use the looser bound.
\begin{corollary} \label{cor:gamma_difference_reqs}
    The statements 
    $$|x| \geq \Delta_{\min} \implies \sinc^2\parens{xt / 2} \leq \epsilon_{\sinc}$$
    and
    $$|x| = |\Delta_S(i,j) - \gamma| \leq \sqrt{2} \Delta_{\min} \epsilon_{\sinc} \implies f(xt/2) \geq 1 - \epsilon_{\sinc}$$
    hold for $t = \frac{2}{\Delta_{\min} \sqrt{\epsilon_{\sinc}}}$. This gives $\epsilon_{\sinc} = \frac{4}{\Delta_{\min}^2 t^2}$. We denote the barrier $\Delta_{\sinc} = \sqrt{2} \Delta_{\min} \epsilon_{\sinc}$. 
\end{corollary}
\begin{proof}
    Throughout this proof we can think of $0 \leq \Delta_{\min} \leq \Delta_S(i,j)$ as a constant, so we avoid writing it as function arguments.
    We first want to provide a bound on $t$ such that $|x| \geq \Delta_{\min}$ implies $f(xt/2) \leq \epsilon_{\sinc}$. This is provided through Eq. \eqref{eq:sinc_upper_bound}
    \begin{align}
        \left| \frac{x t }{ 2} \right| = \frac{|x| t}{2} \geq \frac{\Delta_{\min}t}{2}.
    \end{align}
    We see that setting $t$ such that $\Delta_{\min} t / 2 = 1 /\sqrt{\epsilon_{\sinc}}$, which can be rewritten as $t = \frac{2}{\Delta_{min} \sqrt{\epsilon_{\sinc}}}$, yields the implication $|x| \geq \Delta_{\min} \implies f(xt/2) \leq \epsilon_{\sinc}$. 

    We now want to investigate what values of $x = \Delta_S(i,j) - \gamma$, for the given $t$ as above, yields $f(xt/2) \geq 1 - \epsilon_{\sinc}$. We see that the inequality required for this is
    \begin{align}
        \frac{|x| t}{2} &\leq \sqrt{2 \epsilon_{\sinc}} \\
        \iff  |\Delta_S(i,j) - \gamma| \frac{2}{2 \Delta_{\min} \sqrt{\epsilon_{\sinc}}} &\leq \sqrt{2 \epsilon_{\sinc}} \\
        \iff \abs{\Delta_S(i,j) - \gamma} &\leq \sqrt{2} \Delta_{\min} \epsilon_{\sinc}
    \end{align}
\end{proof}


We now see that if we want there to be unique $(i,j)$ such that $|\Delta_S(i,j) - \gamma| \leq \Delta_{\sinc}$ and for $(i',j') \neq (i,j) \implies |\Delta_S(i',j') - \gamma| \geq \Delta_{\min}$, then we require $|\Delta_S(i,j) - \Delta_S(k,l)| \geq \Delta_{\min} + \Delta_{\sinc}$. 

Suppose $|\Delta_S(i,j) - \gamma| \leq \Delta_{\sinc}$ and $|\Delta_S(i,j) - \Delta_S(k,l)| \geq \Delta_{\sinc} + \Delta_{\min}$ for $(k,l) \neq (i,j)$. We would like to show then that $|\Delta_S(k,l) - \gamma| \geq \Delta_{\min}$. We see that given three real numbers $\gamma, \Delta_S(i,j), \Delta_S(k,l)$ we have four relevant orderings:
\begin{align}
    \gamma \leq \Delta_S(i,j) \leq \Delta_S(k,l) \\
    \Delta_S(k,l) \leq \Delta_S(i,j) \leq \gamma \\
    \Delta_S(i,j) \leq \gamma \leq \Delta_S(k,l) \\
    \Delta_S(k,l) \leq \gamma \leq \Delta_S(i,j).
\end{align}
The scenario $\gamma \leq \Delta_S(i,j) \leq \Delta_S(k,l)$ yields
\begin{align}
    |\Delta_S(k,l) - \gamma| &= \Delta_S(k,l) - \gamma \\
    &= \Delta_S(k,l) - \Delta_S(i,j) + \Delta_S(i,j) - \gamma \\
    &\geq \Delta_S(k,l) - \Delta_S(i,j) \\
    &= |\Delta_S(k,l) - \Delta_S(i,j)| \\
    &\geq \Delta_{\min} + \Delta_{\sinc} \\
    &\geq \Delta_{\min}.
\end{align}
The other direction ($\Delta_S(k,l) \leq \Delta_S(i,j) \leq \gamma$) holds similarly. 

The scenario $\Delta_S(k,l) \leq \gamma \leq \Delta_S(i,j)$ holds through the following computation
\begin{align}
    |\Delta_S(k,l) - \gamma| &= \gamma - \Delta_S(k,l) \\
    &= \gamma + \Delta_S(i,j) - \Delta_S(i,j) - \Delta_S(k,l) \\
    &= \gamma - \Delta_S(i,j) + |\Delta_S(i,j) - \Delta_S(k,l)| \\
    &= -|\gamma - \Delta_S(i,j)| + |\Delta_S(i,j) - \Delta_S(k,l)| \\
    &\geq -\Delta_{\sinc} + \Delta_{\min} + \Delta_{\sinc} \\
    &= \Delta_{\min}.
\end{align}
The other direction ($\Delta_S(i,j) \leq \gamma \leq \Delta_S(k,l)$ ) holds similarly.


% \begin{lemma}[Sinc function parameters]
%     Let $f(x) = \frac{\sin^2(x)}{x^2}$. The constant approximation $f(x) = 1$ has error $|f(x) - 1| \leq \epsilon_{1}$ if $|x| \leq \sqrt{2 \epsilon_{1}}$. We denote $\epsilon_{\sinc} \coloneqq \frac{1}{\Delta_{\min}^2 t^2}$, which leads to the claim that if $\Delta \geq \Delta_{\min}$, then $f(\Delta) \leq \epsilon_{\sinc}$. Further, if $x = \Delta t$, with $\Delta \geq \Delta_{\min}$, we note that $t \geq (\Delta_{\min} \sqrt{\epsilon_{\sinc} } )^{-1}$ suffices for $f(\Delta t) \leq \epsilon_{\sinc}$.
% \end{lemma}
% \begin{proof}
%     We use the form of $\sinc$ as $\frac{\sin(x)}{x} = \int_0^1 \cos(sx) ds$. The first 3 derivatives are computed using straightforward calculus
%     \begin{align}
%         \frac{df}{dx} &= -2 \int_0^1 \sin(sx) s ds \int_0^1 \cos(sx) ds \\
%         \frac{d^2f}{dx^2} &= -2 \int_0^1 \cos(sx)s^2 ds \int_0^1 \cos(sx) ds + \int_0^1 \sin(sx) s ~ds \int_0^1 \sin(sx) s ~ds.
%         % \frac{d^3 f}{dx^3} &= 2 \int_0^1 \sin(sx)s^3 ds \int_0^1 \cos(sx) ds + 4 \int_0^1 \cos(sx) s^2 ds \int_0^1 \sin(sx) s ds .
%     \end{align}
%     By using the fact that $\cos(sx) \to 1$ and $\sin(sx) \to 0$ as $x \to 0$ we can evaluate the Taylor's series to $f(x)$ directly. We also make use of the inequality $\abs{\int_a^b f(x) dx} \leq \int_a^b \abs{f(x)} dx$ and that sine and cosine are bounded by 1.
%     \begin{align}
%         f(x) &= \frac{\sin^2(x)}{x^2} \bigg|_{x = 0} + x \frac{df}{dx}\bigg|_{x = 0} + \frac{x^2}{2!} \frac{d^2f}{dx^2}\bigg|_{x=c} \\
%         f(x) &= 1 + \frac{x^2}{2} \frac{d^2f}{dx^2}\bigg|_{x=c}  \\
%         \abs{f(x) - 1} &= \frac{\abs{x}^2}{2} \abs{\frac{d^2f}{dx^2}(x=c)} \\
%         &\leq \frac{|x|^2}{3}
%     \end{align}
%     Requiring $|x|^2/3 \leq \epsilon$ yields the statement.
    
%     We will also have a need for bounding $f(x)$ when $x$ is of the form $x = \Delta t$. We would like to choose $t$ large enough so that $\Delta \geq \Delta_{\text{min}}$ implies that $f(\Delta t) \leq \epsilon_{\sinc}$. This can be given using the fact that $\sin(x) \leq 1$:
%     \begin{align}
%         f(\Delta t) &= \frac{\sin^2(\Delta t)}{\Delta^2 t^2} \\
%         &\leq \frac{1}{\Delta^2 t^2} \\
%         \frac{1}{\Delta_{\text{min}}^2 t^2} &\leq \frac{1}{\Delta^2 t^2} \leq \epsilon_{\sinc} \\
%         \frac{1}{\Delta_{\text{min}} \sqrt{\epsilon_{\sinc}}} &\leq t.
%     \end{align}
%     We now use this bound on $t$ to investigate when the polynomial approximation given above holds for inputs $x = (\Delta - \gamma)t$. We assume $\gamma \geq 0 $.
%     \begin{align}
%         |(\Delta - \gamma)t| &\leq \sqrt[3]{6\epsilon} \\
%         \abs{\Delta - \gamma} &\leq \frac{\sqrt[3]{6 \epsilon}}{t} \\
%         &\leq \Delta_{\text{min}} \sqrt{\epsilon_{\sinc}}\sqrt[3]{6 \epsilon}.
%     \end{align}
%     We also would like to note the differences if one uses the constant term for $f(x)$ as opposed to a quadratic.
%     $$f(x) = 1 + x \frac{df}{dx}\bigg|_{x=c}$$
%     Using the fact that the first derivative is 0 at $x= 0$ and the second is bounded by $\abs{\frac{d^2f}{dx^2}} \leq 1$, we get
%     $$f(x) = 1 + R(x)$$
%     and $\abs{R(x)} \leq |x|^2 / 2$ implies that $|x| \leq \sqrt{2\epsilon}$ suffices for $\abs{f(x) - 1} \leq \epsilon$. Using this bound with $f((\Delta - \gamma)t)$, we get $\abs{\Delta - \gamma} \leq \Delta_{\text{min}} \sqrt{2 \epsilon_{\sinc} \epsilon} $.
% \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Taylor's Series for $\Phi$} \label{sec:taylor_series_phi}

The easiest way for us to understand the thermalizing channel $\Phi$ is through a Taylor's series with respect to the coupling constant $\alpha$, which will turn out to give us a series in terms of $\alpha t$ instead. This can be thought of as the weak interaction regime, which is extensively studied in open quantum systems. This section provides results about the first and second order terms in the Taylor's Series for $\Phi$ and we give an upper bound on the norm of the remainder term $R_{\Phi}$. As many of the proofs for the statements in this section tend to be rather technical and do not lend themselves to much insight they can be found in the appendix. 

$\Phi$ can be written with the mean-value version of Taylor's theorem as:
\begin{equation}
    \Phi(\rho_S; \alpha) = \Phi(\rho_S; \alpha = 0) + \alpha \frac{\partial}{\partial \alpha} \Phi(\rho_S; \alpha) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho_S; \alpha) \bigg|_{\alpha = 0} + R_{\Phi}(\rho_S, \alpha_{\star}).
\end{equation}
We will denote the second order approximation as
\begin{equation}
    \Phi^{(2)}(\rho_S) \coloneqq \Phi(\rho_S) - R_{\Phi}(\rho_S, \alpha_\star) = \Phi(\rho_S;\alpha= 0) + \frac{\partial}{\partial \alpha} \Phi(\rho_S, \alpha) \bigg|_{\alpha = 0} + \frac{1}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho_S, \alpha) \bigg|_{\alpha = 0}. \label{def:second_order_approx}
\end{equation}
We will also find it helpful to denote each of the terms as
\begin{equation}
    T^{(k)}(X, \alpha) \coloneqq \frac{\alpha^k}{k!} \frac{\partial^k}{\partial \alpha^k} \Phi(X, \alpha)\bigg|_{\alpha = 0}.\label{def:taylor_series_terms}
\end{equation}
If we were to write out the channel $\Phi$ as a Taylor's Series it would be $\Phi(\rho_S) = \sum_{k = 0}^{\infty} T^{(k)}(\rho_S)$.

We now go through and compute the correction terms $T^{(0)}$, $T^{(1)}$, and $T^{(2)}$. The first one is nearly trivial
\begin{lemma}
    The zeroth order correction $T^{(0)}$ to the thermalizing channel $\Phi$ is the Heisenberg evolved input 
    \begin{equation}
        T^{(0)}(\rho_S) = e^{i H_S t} \rho_S e^{-i H_S t},
    \end{equation}
    where this expression holds for all matrix inputs and not just density operators.
\end{lemma}
\begin{proof}
This is a straightforward computation after plugging in the definitions
    \begin{align}
        T^{(0)}(\rho_S) &= \partrace{\hilb_E}{\int e^{i(H + 0 G)t} \rho_S \otimes \rho_E(\beta_E) e^{-i(H + 0 G)t} dG } \\
        &= \partrace{\hilb_E}{e^{i (H_S \otimes \identity + \identity \otimes H_E)t} \rho_S \otimes \rho_E(\beta_E) e^{-i (H_S \otimes \identity + \identity \otimes H_E)t}} \\
        &= e^{i H_S t} \rho_S e^{-i H_S t}.
    \end{align}
\end{proof}
It will prove useful that this result holds even if we extend the $\rho_S$ input to an arbitrary matrix input, as nowhere in the proof did we rely on the properties of density matrices. This will be necessary in arguing that coherences, or off-diagonal matrix elements in a density matrix, do not accumulate with repeated uses of our channel.

The next order correction shows that to $O(\alpha)$ the effects of the environment on the system are zero. This shows that higher order corrections are necessary to compute nontrivial environmental effects. We leave this proof in this section to give the reader a taste for how these arguments work in the higher order calculations. This proof in particular solely relies on the randomly chosen eigenvalues of the interaction to be mean 0.
\begin{lemma}
   The first order correction $T^{(1)}$ to the thermalizing channel $\Phi$, with randomized interactions $G = U_G D U_G^\dagger $ such that the average of each eigenvalue satisfies $\mathbb{E}[d_{i,i}] = 0$, is zero:
   \begin{equation}
        T^{(1)}(\rho_S) = 0.
   \end{equation}
\end{lemma}
\begin{proof}
    We start by using linearity of derivatives, integration, and partial trace to compute the action of the $\alpha$ derivative on $\Phi_G$ as
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi(\rho_S) \bigg|_{\alpha = 0} &= \frac{\partial}{\partial \alpha} \partrace{\mathcal{H}_E}{\int \Phi_G(\rho_S) dG} \bigg|_{\alpha = 0} \\
         &= \partrace{\mathcal{H}_E}{\int \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) dG \bigg|_{\alpha = 0} } .
    \end{align}
    Now we use the expression for $\Phi_G$ in Eq. \eqref{eq:phi_g_definition} to compute the derivatives, and we use the compact notation $\rho = \rho_S \otimes \rho_E(\beta_E)$ to represent the entire system-environment input,
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi_G (\rho_S) &= \parens{\frac{\partial}{\partial \alpha} e^{+ i (H + \alpha G)t}} \rho e^{-i (H + \alpha G) t} + e^{+i (H + \alpha G)t} \rho \parens{\frac{\partial}{\partial \alpha} e^{- i (H + \alpha G)t}} \\
        &= \parens{\int_{0}^{1} e^{i s (H+\alpha G)t} (i t G) e^{i (1-s) (H+\alpha G)t} ds} \rho e^{-i(H+\alpha G)t} \nonumber \\
    &~ ~+ e^{i(H+\alpha G)t} \rho \parens{\int_{0}^1 e^{-i s (H+\alpha G) t} (- i t G) e^{-i (1-s) (H+\alpha G)t} ds}. \label{eq:first_order_alpha_derivative}
    \end{align}
    We can further simplify this by bringing in the evaluation of $\alpha = 0$ through the partial trace and integration, as they are uniformly convergent over $\alpha$ (is that the correct notion that allows us to switch orders?)
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) \bigg|_{\alpha = 0} &= i t \int_0^1 e^{i s H t} G e^{-i s H t} ds e^{i H t} \rho e^{-i H t} - i t e^{+i H t} \rho \int_0^1 e^{-is H t} G e^{-i(1-s) Ht} ds \\
        &= i t \parens{\int_0^1 G(s t) ds} \rho(t) - it \rho(t) \parens{\int_0^1 G(s t) ds} \\
        &= i t \int_0^1 [G(s t), \rho(t)] ds,
    \end{align}
    where we have used the Heisenberg picture $\rho(t) = e^{i H t} \rho e^{-i H t}$ to simplify the notation.

    This expression is now amenable to computing the correction to the total channel. We do so by performing the integration over the randomized interactions. We take advantage of the structure of our interaction measure, that is $G = U_G D U_G^\dagger$ and $dG = dU_G dD$, which allows us to write
    \begin{align}
        \int \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) \bigg|_{\alpha = 0} dG &= it \int \int_0^1 \left[ e^{i H s t} G e^{-i H s t}, \rho(t) \right] ds ~dG \\
        &= it \int_0^1 \left[ e^{i H s t} \parens{\int \int U_G D U_G^\dagger ~dU_G ~ dD} e^{-i H s t}, \rho(t)  \right] ds \\
        &= i t \int_0^1 \left[ e^{i H s t} \parens{\int U_G \parens{\int D ~ dD} ~ U_G^\dagger dU_G } e^{-i H s t}, \rho(t) \right] ds \\
        &= 0.
    \end{align}
    This last step relies on the use of random eigenvalues with mean 0, implying $\int D ~dD = 0$ which shows that $\frac{\partial}{\partial \alpha} \Phi(\rho_S) \big|_{\alpha = 0 } = 0$.
\end{proof}

Now we move on to calculating the second order correction $T^{(2)}$. This is a significantly more tedious computation, so we move the proof of this result to the appendices. First we compute the "pre-trace" matrix elements of the second order correction $T^{(2)}$, but for off-diagonal elements first and then diagonal elements.

These can be seen from Lemma \ref{lem:big_one}.

\begin{lemma}
     For the following we are investigating off-diagonal elements, so we assume $i' \neq k'$ and $j' \neq l'$.
    \begin{align}
    &\int \bra{i', j'}  T^{(2)}_G \left( \ketbra{i, j}{k, l} \right) \ket{k', l'} ~dG \\
    &=\begin{cases}
        -\frac{\alpha^2 e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg( \sigma(i,j) + \sigma(k,l) + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) & (i, j) = (i', j') \text{ and } (k, l) = (k', l') \\
        0 & (i, j) \neq (i', j') \text{ and } (k, l) \neq (k', l')
    \end{cases}
    \end{align}
    where we let $\sigma$ denote
    \begin{equation}
        \sigma(i,j) \coloneqq \sum_{a,b: \Delta(i,j,|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2}
    \end{equation}
    and we remind the reader that $\eta(i,j)$ denotes the degeneracy of eigenvalue $\lambda_S(i,j)$ of the joint system-environment Hamiltonian. 
\end{lemma}
The real utility of this Lemma is that it shows that to $O(\alpha^2)$ there is no contribution to off-diagonal elements from other off-diagonal elements or from diagonal elements.
\begin{lemma}
    The diagonal elements in the pre-trace second order correction are given as
    \begin{align}
        &\int \bra{i', j'} T^{(2)}_G \left( \ketbra{i, j}{i, j} \right) \ket{i', j'} ~dG \\
        &= \begin{cases}
            - \frac{\alpha^2 t^2 }{\dim + 1} \sum_{(a,b) \neq (i, j)} \sinc^2(\Delta(a,b|i,j) t / 2) & (i,j) = (i', j') \\
    \frac{\alpha^2 t^2 }{\dim + 1} \sinc^2(\Delta(i,j | i', j') t /2) & (i, j) \neq (i', j'),
        \end{cases} \label{eq:second_order_transitions_final_final}
    \end{align}
    where we use the fact that $\lim_{x \to 0} \sinc(x) = 1$ to compute the degenerate contributions (when $\Delta(i, j | i', j') = 0$)
\end{lemma}
However, if we want to compute the effects of the channel on a given input, we have to perform the partial trace over the environment, which clearly depends on initial state of the environment we choose. To proceed, we choose a simple two-level environment $H_E = \begin{bmatrix}
    0 & 0 \\ 0 & \gamma
\end{bmatrix}$, where the eigenvectors $\ket{0}$ and $\ket{1}$ have eigenvalues 0 and $\gamma$ respectively. Here, and throughout the rest of the paper, $\gamma$ should be thought of as a user defined parameter that can be adjusted, or chosen probablistically, throughout the cooling procedure. We use a thermal input state $\rho_E(\beta_E)$, as defined in the preliminaries. 

\begin{lemma} \label{lem:t_2_system_only}
    The second order correction to the channel $\Phi$ with two level environment as described above, differs if the output state $j$ is less than, equal to, or greater than $i$. For $i < j$, meaning we are transitioning the systme from a lower energy state to a higher energy and the environment is losing energy, we have
    \begin{equation}
        \left| \bra{j}T^{(2)}(\ketbra{i}{i})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \parens{(\Delta_S(i,j) + \gamma) t/ 2} \right| \leq 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
    \end{equation}
    For $i > j$, meaning we are transitioning the system from a high energy state to a lower energy state, we have
    \begin{equation}
        \left| \bra{j}T^{(2)}(\ketbra{i}{i})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 \parens{(\Delta_S(i,j) - \gamma) t/ 2} \right| \leq 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
    \end{equation}
    For $i = j$, where the state does not gain or lose energy, we get contributions from all other system states that are weighted based on the gain or loss of energy and is given by
    \begin{align}
        &\left| \bra{i} T^{(2)}(\ketbra{i}{i}) \ket{i} + \frac{\alpha^2 t^2}{\dim + 1} \parens{\frac{1}{1 + e^{-\beta_E \gamma}} \sum_{a < i} \sinc^2 ((\Delta_S(a, i) + \gamma) t/ 2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sum_{a > i} \sinc^2((\Delta_S(a, i) - \gamma)t/ 2)} \right| \nonumber \\
        &\leq 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc},
    \end{align}
    where we point out the difference in sign for the $i = j$ terms and the $i \neq j $ terms.
\end{lemma}
\begin{proof}
    For $i < j$ we use Eq. \eqref{eq:second_order_transitions_final_final} straightforwardly
    \begin{align}
        &\bra{j} T^{(2)} (\ketbra{i}{i}) \ket{j} \nonumber \\
        &= \frac{1}{1 + e^{-\beta_E \gamma}} \int \parens{\bra{j, 0} T^{(2)}_G(\ketbra{i,0}{i, 0}) \ket{j, 0} + \bra{j, 1} T^{(2)}_G (\ketbra{i, 0}{i ,0}) \ket{j, 1} } dG \nonumber \\
        &~+ \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \int \parens{\bra{j, 0} T^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{j, 0} + \bra{j, 1} T^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{j, 1}} dG \\
        &= \frac{1}{1 + e^{-\beta_E \gamma}} \frac{\alpha^2 t^2}{\dim + 1}\parens{\sinc^2(\Delta_S(i, j) t/ 2) + \sinc^2 (\Delta_S(i, j) - \gamma)t /2} \nonumber \\
        &~+ \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \frac{\alpha^2 t^2 }{\dim + 1} \parens{\sinc^2((\Delta_S(i,j) + \gamma) t/ 2) + \sinc^2(\Delta_S(i,j) t/ 2)}.
    \end{align}
    From this expression, we see that since $i < j \implies \Delta_S(i,j) < 0$ we have that only the sinc function with input $\Delta_S(i,j) + \gamma$ is possibly larger than $\epsilon_{\sinc}$. This leads to the norm of the difference as
    \begin{equation}
        \left| \bra{j}T^{(2)}(\ketbra{i}{i})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \parens{(\Delta_S(i,j) + \gamma) t/ 2} \right| \leq 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
    \end{equation}

    The $i > j$ calculation is very similar, except that since $i > j$ this implies $\Delta_S(i,j) > 0$ and the only surviving term in the decomposition is the $\sinc$ function with input $\Delta_S(i,j) - \gamma$.
    
    For the $i = j$ calculation we have
    \begin{align}
        \bra{i} T^{(2)}(\ketbra{i}{i}) \ket{i} &= \frac{1}{1 + e^{-\beta_E \gamma}} \bra{i, 0} T^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i, 0} + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \bra{i, 0} T^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i, 0} \nonumber \\
        &~+ \frac{1}{1 + e^{-\beta_E \gamma}} \bra{i, 1} T^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i, 1} + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \bra{i, 1} T^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i, 1}. \label{eq:same_state_transition_with_env}
    \end{align}
    Now we see that there are two contributions of the same-state transitions of $\ket{i, 0} \to \ket{i, 0}$ and $\ket{i, 1} \to \ket{i,1}$, which we can ues Eq. \eqref{eq:second_order_transitions_final_final} to expand in terms of a summation over all other states. The other two terms in the equation above, $\ket{i, 0} \to \ket{i, 1}$ and $\ket{i, 1} \to \ket{i,0}$ we can suppress due to the following calculation
    \begin{align}
        \abs{\bra{i, 0} T^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i,0}} &= \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\gamma t /2) \\
        &\leq \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc},
    \end{align}
    so we only pay the price of an $\epsilon_{\sinc}$ term. The other off-diagonal transition $\ket{i, 0} \to \ket{i, 1}$ can be similarly bounded. We focus on expanding the same-state contributions in Eq. \eqref{eq:same_state_transition_with_env}
    \begin{align}
        &\bra{i,0} T^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i,0} \nonumber \\
        &= -\frac{\alpha^2 t^2}{\dim + 1} \sum_{(a,b) \neq (i, 0)} \sinc^2 \parens{\Delta_S(a, b | i, 0) t / 2} \\ 
        &= - \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a \neq i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} } \\
        &=- \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a < i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} + \sum_{a > i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} } \\
        &= - \frac{\alpha^2 t^2}{\dim + 1} \sinc^2 (\gamma t / 2) \nonumber \\
        &~ - \frac{\alpha^2 t^2}{\dim + 1} \sum_{a > i} \sum_{b = 0, 1} \sinc^2 \parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} \nonumber \\
        &~ -\frac{\alpha^2 t^2}{\dim + 1} \parens{\sum_{a < i} \sinc^2 \parens{\frac{\Delta_S(a, i) t}{2}} + \sum_{a < i} \sinc^2 \parens{\frac{(\Delta_S(a, i) + \gamma)t}{2}} }. \label{eq:same_state_transition_1}
    \end{align}
    Each of these terms, other than those in the very last term of the form $\sinc^2((\Delta_S(a,i) + \gamma)/2)$, will be upper bounded by $\epsilon_{\sinc}$ as the input arguments are above $\Delta_{\min}$. So $\sinc^2(\gamma t /2) \leq \epsilon_{\sinc}$, $\sinc^2((\Delta_S(a,i) + \lambda_E(b))t/2) \leq \epsilon_{\sinc}$ for all $b$ and $a > i$, and $\sinc^2(\Delta_S(a,i)t/2) \leq \epsilon_{\sinc}$. The only surviving term from this contribution is $- \frac{\alpha^2 t^2}{\dim +1} \sum_{a < i} \sinc^2 ((\Delta_S(a,i) + \gamma)t/2)$. 

    The other equal state transition contribution is computed similarly as
    \begin{align}
        &\bra{i,1} T^{(2)}_G(\ketbra{i, 1}{i,1}) \ket{i, 1} \nonumber \\
        &= - \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\gamma t/ 2) \nonumber \\
        &~- \frac{\alpha^2 t^2}{\dim + 1} \sum_{a < i} \sum_{b=0,1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 1))t}{2}} \nonumber \\
        &~- \frac{\alpha^2 t^2}{\dim + 1} \parens{ \sum_{a > i} \sinc^2\parens{\frac{\Delta_S(a, i) t}{2}} + \sum_{a > i} \sinc^2 \parens{\frac{(\Delta_S(a, i) - \gamma)t}{2}}}. \label{eq:same_state_transition_2}
    \end{align}
    Now we can count up the number of terms that are upper bounded by $\epsilon_{\sinc}$. In Eq. \eqref{eq:same_state_transition_2} we have $1 + 2 * ( \dim_S - i) + (i - 1) = 2 \dim_S - i$ and in Eq. \eqref{eq:same_state_transition_2} we have $1 + 2*(i - 1) + (\dim_S - i) = \dim_S + i$, giving a total count of $3 \dim_S$ from these terms. In addition we have the two off-diagonal state contributions from the first decomposition, which if we require $2 \leq \dim_S$ we can upper bound by $\dim_S$ with a minor constant factor increase. We can now state the final result as
    \begin{align}
        &\left| \bra{i} T^{(2)}(\ketbra{i}{i}) \ket{i} + \frac{\alpha^2 t^2}{\dim + 1} \parens{\frac{1}{1 + e^{-\beta_E \gamma}} \sum_{a < i} \sinc^2 ((\Delta_S(a, i) + \gamma) t/ 2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sum_{a > i} \sinc^2((\Delta_S(a, i) - \gamma)t/ 2)} \right| \nonumber \\
        &\leq 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
    \end{align}
 \end{proof}




\begin{restatable}[Second Order Correction]{lemma}{secondOrderChannelHaar} \label{lem:big_one}
    Given a system Hamiltonian $H_{S}$, an environment Hamiltonian $H_{E}$, a simulation time $t$, and coupling coefficient $\alpha$, let $\Phi_G : \hilb_S \otimes \hilb_E \to \hilb_S \otimes \hilb_E$ denote the fixed interaction channel 
    \begin{equation}
        \Phi_G(\rho) = e^{+i (H + \alpha G)t} \rho e^{-i (H + \alpha G)t},
    \end{equation}
    where $H = H_S \otimes \identity + \identity \otimes H_E$. We compute the output of the averaged channel at $\alpha = 0$ for the basis $\ketbra{a}{b}$ of linear operators as:
 \begin{align}
     &\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{k,l})\bigg|_{\alpha = 0} dG \\
     &= -\frac{2  e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2} \nonumber \\
     &~+ \sum_{(a,b): \Delta(k,l|a,b) \neq 0} \frac{1 + i \Delta(k,l|a,b) t - e^{i \Delta(k,l|a,b) t}}{\Delta(k,l|a,b)^2} + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) \ketbra{i,j}{k,l} \nonumber \\
    &~ +\delta_{i,k} \delta_{j,l} \frac{2 e^{i \Delta(i,j|k,l)t}}{\dim+1} \parens{ \sum_{(a,b): \Delta(i,j|a,b) \neq 0 } \frac{2(1- \cos (\Delta(i,j|a,b)t))}{\Delta(i,j|a,b)^2} \ketbra{a,b}{a,b} + t^2 \sum_{(a,b) : \Delta(i,j|a,b) = 0} \ketbra{a,b}{a,b}}
 \end{align}
\end{restatable}
The proof of this is given in Appendix \ref{sec:haar_integral_appendix}. 



\subsection{Remainder Bound}
We now aim to bound the spectral norm of the remainder term $R_{\Phi}$. This is rather tedious, as even to third order in $\alpha$ we have 24 terms to bound. For example, looking first at the $(A)$ term resulting from the second order derivative in Eq. \eqref{eq:second_order_deriv_intermediate_a} we have 4 multiplicative factors involving $\alpha$, leading to 4 terms from this single term of the second order derivative. As there are six terms in total, this yeilds 24 terms. We can profit from the fact though that the expressions for the second order derivatives do simplify and the final expression only has 3 terms, as two of the derivatives act similarly. We first will analyze the single term from Eq. \eqref{eq:second_order_deriv_intermediate_a} in detail. 


\begin{lemma} \label{lem:remainder_bound}
    Let $\Phi(\rho)$ be the thermalizing channel as defined in Def. \eqref{eq:phi_g_definition} and $R_{\Phi}$ the remainder term 
    \begin{equation}
        R_{\Phi}(\rho, \alpha) = \Phi(\rho, \alpha) - \rho - \frac{\alpha}{1!} \frac{\partial}{\partial \alpha} \Phi(\rho)\bigg|_{\alpha = 0} - \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0}.
    \end{equation}
    Then we have that $\norm{R_{\Phi}} \leq \epsilon_R$ for a simulation duration of
    \begin{equation}
        \alpha t \in O\parens{\frac{\epsilon_R^{1/3}}{ \sqrt{\log(\dim_S)}}}.
    \end{equation}
\end{lemma}
\begin{proof}
    First I need to write down what exactly we are trying to bound. What we are essentially trying to do is bound the matrix entries:
\begin{equation}
    \Phi(\rho) = \rho + \frac{\alpha}{1} \frac{\partial}{\partial \alpha} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^3}{3!} \frac{\partial^3}{\partial \alpha^3} \Phi(\rho) \bigg|_{\alpha = \alpha^\star},\label{eq:remExpress}
\end{equation}
where $\alpha^\star \in [0, \alpha]$. Our goal is then to bound the spectral norm of the remainder term for all such $\alpha^\star$.

% \begin{align}
%     &\norm{\frac{\partial}{\partial \alpha} (it)^2 \int_0^1 \parens{\int_0^1 e^{i s_1 s_2 (H+\alpha G)t} s_1 G e^{i s_1 (1-s_2) (H+\alpha G)t} ds_2} G e^{i(1-s_1) (H+\alpha G)t} ds_1 \rho e^{-i(H+\alpha G) t}} \\
%     &=t^3 \norm{\int_0^1 \int_0^1 \int_0^1 e^{i s_1 s_2 s_3(H + \alpha G)t} G e^{i s_1 s_2 (1 - s_3)} ds_3 G e^{i s_1 (1-s_2)(H + \alpha G)t} ds_2 G e^{i(1-s_1)(H + \alpha G)t} ds_1 \rho e^{-i(H + \alpha G)t} }  + t^3 \norm{\ldots}\\
%     &\leq t^3 \norm{G}^3 + t^3 \norm{\ldots} \\
%     &\leq 4 t^3 \norm{G}^3,
% \end{align}
% where we used smoothness of the integrand to bring the norm inside the integral via the triangle inequality and submultiplicativity of the operator norm to simply break the norm of the product into the product of each of the norms. The operator norm of the unitary operators and the density matrix are each 1, and the resulting integrals yield only fractional values, which are upper bounded by 1. Now here we see the final simplification that can be made, each term $(B), (C)$, etc., will yield at most a cubic power of $\norm{G}$, so we can upper bound each term in the final sum as $t^3 \norm{G}^3$. In reality we would have terms such as $\norm{G} \norm{G^3}$ and all other polynomials, but we don't care.

% Now as $G$ is a random matrix, we can only bound it's operator norm with a probabilistic guarantee. 

\begin{align}
    \|\partial_\alpha^3 \rho(\alpha) \| &= \left\| \frac{\partial^3}{\partial \alpha^3} {\rm Tr}_{H_E} \int e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t} dG\right\|\nonumber\\
    &= \left\| \frac{\partial^3}{\partial \alpha^3} {\rm Tr}_{H_E} \int e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t} dG\right\|\nonumber\\
    &\le    \int \left\|{\rm Tr}_{H_E}\frac{\partial^3}{\partial \alpha^3}\left( e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t}\right) \right\| dG\label{eq:3derivBd}
\end{align}
Next we can apply Duhamel's formula to see that
\begin{align}
    \partial_\alpha \left( e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t}\right) &=\int_0^1 e^{i(H+\alpha G)ts} (iGt)e^{i(H+\alpha G)t(1-s)}  \rho_S \otimes \rho_E e^{-i(H+\alpha G)ts}  ds\nonumber\\
    &\quad+\int_0^1  e^{i(H+\alpha G)t}\rho_S \otimes \rho_Ee^{-i(H+\alpha G)ts} (-iGt)e^{-i(H+\alpha G)t(1-s)}  ds
\end{align}
We can recurse this two more times and observe that because $G$ is independent of $\alpha$
that $12$ terms each appear in the derivative depending on the particular locations where the derivatives land from the use of the product rule.  As an example,
consider the first such example that appears in our norm bound which for unitary matrix valued functions $U_1,U_2,U_3 : \mathbb{R}^3 \mapsto L(\hilb_S \otimes \hilb_E)$
\begin{align}
    &\| {\rm Tr_{H_E}} \int_0^1\int_0^{s}\int_0^{s'} U_1(s,s',s'')(iGt)U_2(s,s',s'') (iGt) U_3(s,s',s'') (iGt) U_4(s,s',s'') d^3s\|\nonumber\\
    &\le \int_0^1 \int_0^s \int_0^{s'} \|{\rm Tr_{\hilb_E}}U_1(s,s',s'')(iGt)U_2(s,s',s'') (iGt) U_3(s,s',s'') (iGt) U_4(s,s',s'')\| d^3s
\end{align}
Next we use the fact from Proposition 1 of~\cite{rastegin2012relations} that for any normal operator $A$ acting on a finite dimensional Hilbert space $\hilb_S\otimes \hilb_E$ that the spectral norm obeys
\begin{equation}
    \|{\rm Tr}_{\hilb_E} A\| \le \|A\|.
\end{equation}
Thus from the unitary invariance of the norm and its sub-multiplicative property
\begin{equation}
    \int_0^1 \int_0^s \int_0^{s'} \|{\rm Tr_{\hilb_E}}U_1(s,s',s'')(iGt)U_2(s,s',s'') (iGt) U_3(s,s',s'') (iGt) U_4(s,s',s'')\| d^3s \le (\|G\|t)^3.\label{eq:t3error}
\end{equation}
Next as there are $12$ such terms that need to be considered we find by combining~\eqref{eq:3derivBd} and~\eqref{eq:t3error}
\begin{equation}
    \|\partial_\alpha^3 \phi(\rho) \| \le 12t^3\int \|G\|^3 dG
\end{equation}
Then an application of~\eqref{eq:remExpress} yields
\begin{equation}
    \norm{\Phi(\rho) - \left(\rho + {\alpha} \frac{\partial}{\partial \alpha} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0} \right)} \leq 12 \alpha^3 t^3 \int \norm{G}^3 dG.\label{eq:remTaylorBd}
\end{equation}

At this stage an interesting statistical problem emerges.  The spectral norm is the largest singular value of a matrix but here the matrix $G$ is a random matrix with Gaussian eigenvalues distributed as a standard normal distribution.  The correct property to look at is the largest and smallest order statistic for the eigenvalues cubed of $G$.  The average value of this quantity will correspond to the norm.  Specifically we can use the theory of order statistics to write this average as
\begin{align}
    \int \max_j \lambda_j(G)^3 dG &= \dim_S\int_{-\infty}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \left({\rm Pr}(x\le \lambda)\right)^{\dim_S -1} d\lambda\\
    &= \dim_S\int_{-\infty}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \left(1-\frac{{\rm erfc}(\lambda/\sqrt{2})}{2} \right)^{\dim_S-1} d\lambda.\\
    &=-\dim_S\int_{-\infty}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \sum_{k=1}^{\dim_S-1} \binom{\dim_S-1}{k}\frac{{\rm erfc}(\lambda/\sqrt{2})^{k}}{2^{k}} d\lambda
\end{align}
Note that because the complementary error function approximates a step function about zero and $\lambda^3$ is an odd function, the above integral is positive despite the overall negative sign.
Now let us define a cutoff on the integral of $\Lambda\ge 1$ and aim to set the value of $\Lambda$ such that the integral beyond this cutoff is at most $\epsilon$ large.  Specifically we aim to find $\Lambda$ such that
\begin{equation}
    0\le -\dim_S\int_{\Lambda}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \sum_{k=0}^{\lfloor \dim_S/2-1\rfloor} \binom{\dim_S-1}{2k+1}\frac{{\rm erfc}(\lambda/\sqrt{2})^{2k+1}}{2^{2k+1}} d\lambda \le \epsilon
\end{equation}
The complementary error function is a rapidly decaying function of $\lambda$ for $\lambda>0$.  Thus in order to simplify the sum to allow only the first order term in the expansion to dominate we wish to take $\Lambda$ large enough so that the total error is at most $\epsilon/2$ which is implied by taking
\begin{equation}
    \frac{{\rm erfc}(\Lambda/\sqrt{2})}{2} \le \frac{\epsilon}{4\dim_S \Lambda^3}.
\end{equation}
The complementary error function has the following asymptotic expansion
\begin{equation}
    {\rm erfc}(\lambda/\sqrt{2}) = \frac{\sqrt{2}e^{-\lambda^2/2}}{\lambda \sqrt{\pi}}\left( 1 + O(\lambda^{-2}) \right)
\end{equation}
Thus using the fact that here $\lambda\ge \Lambda$, $\Lambda \ge 1$ an $e^{-\Lambda^2/2}\Lambda^2 \le 1$ it suffices to take
\begin{equation}
    \Lambda \in O\left(\sqrt{\log(\dim_S/\epsilon)} \right)
\end{equation}
Next we wish to show that the leading order term in the integral is also small we have that
\begin{align}
    -\dim_S^2\int_{\Lambda}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \frac{{\rm erfc}(\sqrt{2}\lambda)}{2} d\lambda&\le \dim_S^2\sqrt{\int_{-\infty}^\infty \lambda^6 \frac{e^{-\lambda^2/2}}{\sqrt{2\pi}}  d\lambda}\sqrt{\int_\Lambda^\infty\left(\frac{{\rm erfc}^2(\sqrt{2}\lambda)}{4}\right)\frac{e^{-\lambda^2/2}}{\sqrt{2\pi}}  d\lambda }\nonumber\\
    &=\sqrt{15} \dim_S^2 \sqrt{\int_\Lambda^\infty\left(\frac{{\rm erfc}^2(\sqrt{2}\lambda)}{4}\right)\frac{e^{-\lambda^2/2}}{\sqrt{2\pi}}  d\lambda }\nonumber\\
    &\le \sqrt{\frac{15}{4}} \dim_S^2 {\rm erfc}(\sqrt{2}\Lambda) \le 4 \dim_S^2 \left(\frac{e^{-2\Lambda^2}}{\Lambda \sqrt{2\pi}} \right) (1+O(\Lambda^{-2})).
\end{align}
As before using $\Lambda \ge 1$ and isolating for $\Lambda$ we see for this term as well that we can make the error at most $\epsilon/2$ for a value of $\Lambda$ that scales as
\begin{equation}
    \Lambda \in O\left(\sqrt{\log(\dim_S/\epsilon)} \right).
\end{equation}
We then have that the remaining integral can be bounded by
\begin{align}
    -\dim_S\int_{-\infty}^{\Lambda}  \lambda^3 \frac{e^{-\lambda^2/2}}{\sqrt{2\pi}} \sum_{k=0}^{\lfloor \dim_S/2-1\rfloor} \binom{\dim_S-1}{2k+1}\frac{{\rm erfc}(\lambda/\sqrt{2})^{2k+1}}{2^{2k+1}} d\lambda&=  \int_{-\infty}^\Lambda \lambda^3 {\rm Pr}(\lambda_{max}(G) = \lambda) d\lambda \nonumber\\
    &\le \Lambda
\end{align}
The value of $\Lambda$ is shown above to be logarithmic in the system dimension, so we can choose $\epsilon \in \Theta(1)$ and not significantly change the value of the integral and hence
\begin{equation}
    \int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{max}(G) = \lambda) d\lambda \in O(\log^{3/2}(\dim_S)).
\end{equation}
From the fact that the probability distribution for the eigenvalues is symmetric about $0$ we have that
\begin{equation}
    \int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\min}(G) = \lambda) d\lambda = - \int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\max}(G) = \lambda) d\lambda
\end{equation}
Thus we have 
\begin{align}
    \int_{-\infty}^\infty \|G\|^3 dG &\le \left|\int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\min}(G) = \lambda) d\lambda \right| + \left|\int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\max}(G) = \lambda) d\lambda \right|\nonumber\\
    &\in O\left(\log^{3/2}(\dim_S) \right)
\end{align}
Finally after inserting this into~\eqref{eq:remTaylorBd} we find that the truncation error for the channel at second order in $\alpha$ is
\begin{equation}
    \norm{\Phi(\rho) - \left(\rho + {\alpha} \frac{\partial}{\partial \alpha} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0} \right)} \in O(\alpha^3 t^3 \log^{3/2}({\rm dim}_S)).
\end{equation}
This gives us that the remainder bound, $\|R_\Phi\|\le \epsilon_R$ for 
\begin{equation}
    \alpha t \in O\left(\frac{\epsilon_R^{1/3}}{\sqrt{\log(\dim_S)}} \right).
\end{equation}
\end{proof}

\section{Approximate Detailed Balance}
In this section we show the most mild form of thermalization for all possible system Hamiltonians $H_S$. What we will show is that if $\Phi$ has a fixed point, then it will be arbitrarily close to the fixed point of the $\bigo{(\alpha t)^2}$ approximation to the channel $\Phi$. Then we show that the fixed point of this approximation, in the limit as $t \to \infty$, $\alpha \to 0$, and we have perfect knowledge of the distribution of eigenvalue differences $\Delta_S(i,k)$, is the thermal state $\rho_S(\beta_E)$. We say this is the most mild form of thermalization as it provides no guarantees for how long it takes the system to reach this fixed point, or even how long it takes to get close to it's fixed point. 

Given that we are able to rotate the basis we work with into the System Hamiltonian's eigenbasis, and moreover we previously showed that off-diagonal coherences in this basis are negligible, we can treat this process as a stochastic one. Explicitly, if we give a diagonal state as input to the channel $\Phi$, to order $\bigo{\alpha^2}$ we get a diagonal state back. Since diagonal states can be thought of as probability distributions, the map $\Phi$ can roughly be thought of as a Markov process on the eigenstates of the Hamiltonian $H_S$. Since our goal is to show that this map produces a thermal state,
we would need to show that this probability distribution converges to the Boltzmann distribution $e^{-\beta_E \lambda_S(i)} / \partfun_S(\beta_E)$, for state $i$. In classical probability sampling literature this is done through a Detailed Balance calculation, which shows that the desired distribution is a fixed point. Ergodicity arguments of the desired Markov chain are then used to claim that the fixed point is unique and that therefore the process produces samples of the desired distribution.

Given that our map is only approximate, we cannot make rigorous claims regarding detailed balance at fixed $\alpha, t,$ or $\gamma$. However, in this section we show that in the appropriate limits that our mapping satisfies Detailed Balance exactly. This is enough to give solid evidence that for appropriate regimes our channel should converge approximately to this goal distribution.


First we need to show that the fixed point of the thermalizing channel $\Phi$, if it exists, is close to it's approximation $T^{(2)}$. In this case, let $\rho_{\Phi}$ denote the fixed point of $\Phi$ and similarly for $\rho_T$.
\begin{align}
    \norm{\rho_{\Phi} - \rho_T} &= \norm{\Phi(\rho_{\Phi}) - T^{(2)}(\rho_T)} \\
    &= \norm{\Phi(\rho_{\Phi}) - T^{(2)}(\rho_{\Phi}) + T^{(2)}(\rho_{\Phi}) - T^{(2)}(\rho_T)}
\end{align}


\begin{align}
    &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \nonumber \\
    &\approx \frac{\alpha^2 t^2}{\dim + 1} \parens {\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}  \sinc^2((\Delta(i,j) + \gamma)t) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 ((\Delta(i,j) - \gamma)t) + 2 \sinc^2(\Delta(i,j) t)} \nonumber
\end{align}

\begin{align}
    &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \nonumber \\
    &= \bra{j} \Phi_{\gamma}(\ketbra{i}{i}) \ket{j} \\
    &= \braket{j}{i}\braket{i}{j} + \sum_{k,l} \tau(i, k | j, l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} + \bra{j} R_{\Phi}(\ketbra{i}{i})\ket{j}
\end{align}
Now as we are studying Detailed Balance, we assume that $i \neq j$. For our purposes, without loss of generality we let $\lambda_S(i) \geq \lambda_S(j)$, so by transitioning from $i \to j$ we are losing energy to the environment. We simplify the non-trivial term from above as
\begin{align}
    &\sum_{k,l} \tau(i,k| j,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} = \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)}(\tau(i,0|j,0) + \tau(i,0|j,1)) + \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} (\tau(i,1|j,0) + \tau(i,1|j,1) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} (\sinc^2(\Delta_S(i,j)t/2) + \sinc^2((\Delta_S(i,j) - \gamma)t/2)) \nonumber \\
    &\quad \quad \quad \quad \quad \quad  +\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} (\sinc^2((\Delta_S(i,j) + \gamma)t/2) + \sinc^2(\Delta_S(i,j) t/2)) \bigg) \\
    &\frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) + \sinc^2(\Delta_S(i,j)t/2)
\end{align}
As $\Delta_S(i,j) \geq 0 $ we expect that only the $\sinc^2((\Delta_S(i,j) - \gamma) t/2)$ term will contribute significantly to this sum. We can similarly write down the probability of the state to transition from $j \to i$ as
\begin{align}
    \prob{\text{System transition } j \to i | \text{ env at } \beta_E} = \sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} + \bra{i} R_{\Phi}(\ketbra{j}{j})\ket{i},
\end{align}
where we write the non-trivial term as
\begin{align}
    \sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} = \frac{\alpha^2 t^2}{\dim + 1} \bigg(&\frac{1}{1 + e^{-\beta_E \gamma}} (\sinc^2(\Delta_S(j,i)t/2) + \sinc^2((\Delta_S(j,i) - \gamma)t/2)) \nonumber \\
    &+\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} (\sinc^2((\Delta_S(j,i) + \gamma)t/2) + \sinc^2(\Delta_S(j,i) t/2)) \bigg).
\end{align}
We simplify this by noting $\Delta_S(i,j) = - \Delta_S(j,i)$ and that $\sinc^2(x) = \sinc^2(-x)$ to get
\begin{align}
&\sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} \\
&= \frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) + \sinc^2(\Delta_S(i,j)t/2)
\end{align}

Now we would like to show some form of Detailed Balance, in appropriate limits, for the thermal state of the system. We will show that Detailed Balance holds in expectation, or with some non-zero probability. The thermal state give $\prob{\text{state in } i} = \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}$. We then need to expand differences
\begin{align}
    \chi(i,j) \coloneqq &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \prob{\text{state in } i} \nonumber \\
    &- \prob{\text{System transition } j \to i | \text{ env at } \beta_E} \prob{\text{state in } j}.
\end{align}
This expression is written in full glory as
\begin{align}
    \chi(i,j) &= \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \bra{j} R_{\Phi}(\ketbra{i}{i})\ket{j} \nonumber \\
    &+ \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \nonumber \\
    &+\frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma) t/2) \nonumber \\
    &+ \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\Delta_S(i,j)t/2) \nonumber \\
    &-\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \bra{i} R_{\Phi}(\ketbra{j}{j})\ket{i} \nonumber \\
    &- \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma) t/2) \nonumber \\
    &-\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \nonumber \\
    &- \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\Delta_S(i,j)t/2).
\end{align}
In order to simplify this, we will group these expressions with an end goal in mind. As we would like this to hold for all $i \neq j$ but we have a fixed $\gamma$, we need to randomly choose a $\gamma$ and show that this holds in expectation. Further, we would like to bound the absolute value of these differences. So we want $\mathbb{E}_{\gamma}\abs{\chi(i,j)}$. We can then use the triangle inequality, the fact that $\abs{\bra{j}R_{\Phi}(\ketbra{i}{i})\ket{j}} \leq \norm{R_{\Phi}}$, $\abs{\frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}} \leq 1$, 

along with the fact that transitions such as $\sinc^2(\Delta_S(i,j)t/2)$ are suppressed by $\epsilon_{\sinc}$ to get a simplified upper bound:
\begin{align}
    \mathbb{E}_{\gamma} \abs{\chi(i,j)} &\leq \mathbb{E}_{\gamma} 2 \norm{R_{\Phi}} \nonumber \\
    &+ \mathbb{E}_{\gamma} \frac{\alpha^2 t^2}{\dim + 1} 4 \epsilon_{\sinc}  \nonumber \\
    &+ \mathbb{E}_{\gamma} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \frac{1}{\partfun_S(\beta_E)} \abs{\parens{e^{-\beta_E \lambda_S(i)} - e^{-\beta_E \gamma} e^{-\beta_E \lambda_S(j)}} \sinc^2((\Delta_S(i,j) - \gamma) t/2)} \\
    &\leq 2 \norm{R_{\Phi}} + 4 \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \nonumber \\
    &+ \frac{\alpha^2 t^2}{\dim + 1} \mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}}. \label{eq:detailed_balance_upper_bound}
\end{align}
This is where we have to introduce our distribution over $\gamma$. We note that if $\gamma$ is far away from $\Delta_S(i,j)$, then as $t \to \infty$ we have that this product is trivially 0. We want to show that Detailed Balance is satisfied even when $\gamma \to \Delta_S(i,j)$. To do so we upper bound $\sinc^2 \leq 1$ whenever $\abs{\Delta_S(i,j) - \gamma} \leq \Delta_{\min}$ and $\sinc^2 \leq \epsilon_{\sinc}$ whenever $\abs{\Delta_S(i,j) - \gamma} \geq \Delta_{\min}$. 

We now are at an impasse. Our goal for this argument is to show that if our channel does anything non-trivial, then in the appropriate limits ($t \to \infty, \alpha \to 0, \alpha t \to c_{small}$) it should satisfy detailed balance conditions, or at least get arbitrarily close to it. We see that when our channel has a $\gamma$ that is not close to any $\Delta_S(i,j)$ we do not induce any transitions among states (in the $t \to \infty$ limit). This then trivially satisfies Detailed balance, as $\prob{i \to j} = \prob{j \to i} = 0$ gives $0 = 0$ for Detailed Balance. We would like to show that all $i \neq j$ can get arbitrarily close to satisfying Detailed Balance by choosing a $\gamma$ randomly. To do so there are three obvious candidates for distributions of $\gamma$ that we could analyze theoretically:
\begin{enumerate}
    \item A maximum entropy prior, or choosing $\gamma$ uniformly from 0 to $\norm{H}$,
    \item A minimal entropy prior, or exact knowledge of $\Delta_S(i,j)$ for all $i,j$, where we choose indices or differences uniformly,
    \item Choose a difference $\Delta_S(i,j)$ uniformly and then add in noise, either in the form of a Gaussian or a simple uniform box centered about the gap $\Delta_S(i,j)$.
\end{enumerate}

The simplest for us to look at first is the maximum entropy prior, or the uniform distribution of $\gamma$ from 0 to $\norm{H}$. This then gives us two regimes, $|\Delta_S(i,j) - \gamma| \leq \Delta_{\min}$ and $|\Delta_S(i,j) - \gamma| > \Delta_{\min}$. The $\sinc^2$ term is upper bounded by 1 in the former and $\epsilon_{\sinc}$ in the latter. 
\begin{align}
    &\mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} \nonumber \\
    &= \frac{1}{\norm{H}} \int_0^{\norm{H}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \\
    &= \frac{1}{\norm{H}} \int_0^{\Delta_S(i,j) - \Delta_{\min}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\quad + \frac{1}{\norm{H}} \int_{\Delta_S(i,j) - \Delta_{\min}}^{\Delta_S(i,j) + \Delta_{\min}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\quad + \int_{\Delta_S(i,j) + \Delta_{\min}}^{\norm{H}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma.
\end{align}
We will simplify these integrals separately. Starting with the first
\begin{align}
    &\frac{1}{\norm{H}} \int_0^{\Delta_S(i,j) - \Delta_{\min}} \sinc^2((\Delta_S(i,j) - \gamma)t/2)\abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\leq \frac{\epsilon_{\sinc}}{\norm{H}} \int_0^{\Delta_S(i,j) - \Delta_{\min}} \left(e^{\beta_E(\Delta_S(i,j) - \gamma)} - 1\right) d\gamma \\
    &= \frac{\epsilon_{\sinc}}{\norm{H}}\parens{\Delta_{\min} - \Delta_S(i,j) + \frac{e^{\beta_E \Delta_S(i,j)}}{\beta_E}\left( 1 - e^{-\beta_E(\Delta_S(i,j) - \Delta_{\min})} \right)}.
\end{align}
We then compute the third, as it is more similar to the first as
\begin{align}
    &\frac{1}{\norm{H}} \int_{\Delta_S(i,j) + \Delta_{\min}}^{\norm{H}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} d\gamma \nonumber \\
    &\leq \frac{\epsilon_{\sinc}}{\norm{H}} \int_{\Delta_S(i,j) + \Delta_{\min}}^{\norm{H}} \left(1 - e^{\beta_E (\Delta_S(i,j) - \gamma)} \right) d\gamma \\
    &=\frac{\epsilon_{\sinc}}{\norm{H}}\parens{\norm{H} - (\Delta_S(i,j) + \Delta_{\min}) + \frac{1}{\beta_E} \left(e^{-\beta_E(\norm{H} - \Delta_S(i,j))} - e^{-\beta_E \Delta_{\min}} \right) }.
\end{align}
Adding the results of these two integrals yields
\begin{align}
    \frac{\epsilon_{\sinc}}{\norm{H}}\parens{\norm{H} - 2 \Delta_S(i,j) +  \frac{1}{\beta_E} e^{\beta_E \Delta_S(i,j)}(2 + e^{-\beta_E \norm{H}}) + \frac{e^{\beta_E \Delta_{\min}} - e^{-\beta_E \Delta_{\min}}}{\beta_E}}.
\end{align}
We make two observations. First that this is positive given that $\norm{H} \geq 2 \Delta_S(i,j)$ and tends towards infinity as $\beta_E \to \infty$. The second is that there is no time dependence on the factor within the parenthesis, meaning that for fixed $\beta_E$ we can make this quantity arbitrarily small by reducing $\epsilon_{\sinc} \propto 1/t^2$. 

The more reasonable distribution to analyze is the minimal entropy, or perfect knowledge distribution. In this distribution we pick an eigenvalue difference uniformly at random. We denote the set of eigenvalue gaps as $G_{\gamma}$. Then the expectation can be split into two: $S_{\gamma}$ being the set of gaps that are close to $\gamma$ and $T_{\gamma}$ as those that are far apart. Let $N_{diff}$ denote the number of differences. Specifically, let $S_{\gamma} = \set{\lambda_S(k) - \lambda_S(l) = \Delta_S(k,l) : \sinc^2((\Delta_S(i,j) - \Delta_S(k,l))t/2) \geq \epsilon_{\sinc}}$, and $T_{\gamma} = \set{\Delta_S(k,l) : \sinc^2((\Delta_S(i,j) - \Delta_S(k,l))t/2) < \epsilon_{\sinc}}$. Then the expected value over $\gamma$ becomes
\begin{align}
    &\mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \nonumber \\
    &=\frac{1}{N_{diff}}\sum_{\gamma \in S_{\gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} + \frac{1}{N_{diff}} \sum_{\gamma \in T_{\gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \\
    &\leq \frac{1}{N_{diff}} \sum_{\gamma \in S_{\gamma}} \abs{1 - e^{\beta_E(\Delta_S(i,j) -\gamma)}} + \frac{1}{N_{diff}} \epsilon_{\sinc} \sum_{\gamma \in T_{\gamma}}\abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}}.
\end{align}
We see that for the right hand sum the factor of $\epsilon_{\sinc}$, which vanishes as $t \to \infty$, causes the total sum to vanish as there are no explicit $t$ dependent terms. The set $T_{\gamma}$ does change with $t$, but it is upper bounded by a finite value and so is each possible summand. Now the real kicker is what happens to the leftmost summation. We investigate when an eigenvalue gap $\Delta_S(k,l)$ can be included in $S_{\gamma}$ as $t \to \infty$. 
\begin{align}
    \lim_{t \to \infty} \sinc^2((\Delta_S(i,j) - \Delta_S(k,l)) t/2) = \begin{cases}
        0 & \Delta_S(i,j) \neq \Delta_S(k,l) \\
        1 & \Delta_S(i,j) = \Delta_S(k,l).
    \end{cases}
\end{align}
Because of this $S_{\gamma} = \set{\Delta_S(i,j)}$ becomes a multiset consisting solely of $\Delta_S(i,j)$ with the number of degeneracies of the eigenvalue $S_{\gamma}$. This means that any term in the summation then becomes $\abs{1 - e^{\beta_E(\Delta_S(i,j) - \Delta_S(i,j)}} = 0$. Then given that $\epsilon_{\sinc} = 1/(\Delta_(\min)^2 t^2) \to 0$, we have that $\lim_{t \to \infty} \mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} = 0$. Looking at Eq. \eqref{eq:detailed_balance_upper_bound}, along with $\epsilon_{\sinc} = 1/(\Delta_{\min}^2 t^2)$ and $\alpha = \epsilon_{\alpha} / t$, we see that 
\begin{align}
    &\lim_{t \to \infty} \mathbb{E}_{\gamma} \abs{\chi(i,j)} \leq 2 \lim_{t \to \infty} \norm{R_{\Phi}} + 4 \lim_{t \to \infty} \frac{\epsilon_{\alpha}^2}{t^2 \Delta_{\min}^2 (\dim + 1)} \nonumber \\
    &+ \frac{\epsilon_{\alpha}^2}{\dim + 1} \lim_{t \to \infty} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \\
    &= 2 \lim_{t \to \infty} \norm{R_{\Phi}}.
\end{align}
As 

\begin{align}
    \frac{\prob{i \to j}}{\prob{j \to i}} &\approx \frac{\frac{1}{1 + e^{-\beta_E \gamma}}\sinc^2 ((\Delta(i,j) - \gamma)t)+ 3 \epsilon_{\sinc}}{\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((-\Delta(i,j) + \gamma)t) + 3 \epsilon_{\sinc}}  \nonumber \\
    \lim_{t \to \infty} &\implies e^{\beta_E \gamma} = e^{-\beta_E \Delta(i,j)} \nonumber \\
    &= \frac{\prob{\text{System in } j}}{ \prob{\text{System in } i}} \nonumber
\end{align}

\section{Harmonic Oscillator}
In this section we show thermalization of a truncated Harmonic Oscillator with an environment tuned exactly to the gap of the oscillator. Specifically, the eigenvalues of the system are given by $\lambda_S(i) = i$ and $\gamma = 1$. In order to show thermalization we need to show that the thermal state is a fixed point $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_1 \leq \epsilon_{fix}$ and that there is a range of temperatures in which the channel cools the system $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1$. Showing that the thermal state at the environment temperature is a fixed point is easier and demonstrates the techniques we will use to show cooling, so we start with that.

\subsection{Fixed point}
\begin{lemma}
    For a truncated harmonic oscillator with $\dim_S \geq 4$, the thermalizing channel $\Phi$ has an approximate fixed point of temperature $\beta$, assuming an environment qubit at temperature $\beta$ and an environment gap $\gamma = 1$, that is controlled by the coupling constant $\alpha$, or specifically
    \begin{equation}
        \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 \le 12 \dim_S \alpha^2 + \bigo{\alpha^3 t^3 \dim_S \log^{3/2}(\dim_S)}
    \end{equation}
    \todo{Need to include units of $1/\omega$ next to $\alpha^2$ from $\Delta_{\min}$}
\end{lemma}
\begin{proof}
Using the Taylor series results from Section \ref{sec:taylor_series_phi}
\begin{align}
    \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 &= \norm{T^{(2)}(\rho_S(\beta)) + R_{\Phi}(\rho_S(\beta))}_1 \\
    &\leq \norm{T^{(2)}(\rho_S(\beta))}_1 + \norm{R_{\Phi}(\rho_S(\beta)}_1 \\
    &\le \norm{T^{(2)}(\rho_S(\beta))}_1 + \dim_S \norm{R_{\Phi}(\rho_S(\beta)}. \label{eq:fixed_pt_harmonic_osc_1}
\end{align}
Now recalling that since $\rho_S(\beta)$ is diagonal in the $H_S$ eigenbasis and that $T^{(2)}$ maps diagonal elements to diagonal elements only, we can compute the norm of $T^{(2)}$ by summing the absolute values of the diagonals
\begin{align}
    &\norm{T^{(2)}(\rho_S(\beta))}_1 = \sum_j \left| \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)} \sum_i \bra{j} T^{(2)}(\ketbra{i}{i}) \ket{j} \right| \\
    &= \left|\sum_{i}  p(i) \bra{1} T^{(2)}(\ketbra{i}{i}) \ket{1}\right| + \sum_{j = 2}^{\dim_S - 1} \left|  \sum_{i} p(i) \bra{j} T^{(2)}(\ketbra{i}{i}) \ket{j}\right| + \left|  \sum_{i} p(i)\bra{\dim_S} T^{(2)}(\ketbra{i}{i}) \ket{\dim_S}\right| \label{eq:fixed_pt_harmonic_osc_2}
\end{align}
We split the sum into these three parts because each of these terms will have different nontrivial contributions. We will start with the easiest, the leftmost term. The main tool we will use is Lemma \ref{lem:t_2_system_only}, and to do so we will add and subtract terms that will essentially allow us to substitute in expressions for $\bra{1}T^{(2)}(\ketbra{1}{1})\ket{1}$ that utilize sinc and probabilities. 
\begin{align}
    &\left| p(1) \bra{1} T^{(2)}(\ketbra{1}{1})\ket{1} + \sum_{i > 1} p(i) \bra{1} T^{(2)}(\ketbra{i}{i})\ket{1} \right| \\
    &= \bigg| p(1) \bra{1} T^{(2)}(\ketbra{1}{1})\ket{1} + p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) - p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) \nonumber \\ 
    &~ + \sum_{i > 1} p(i) \bra{1} T^{(2)}(\ketbra{i}{i})\ket{1} \bigg| \\
    &\le \left|- p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) + \sum_{i > 1} p(i) \bra{1} T^{(2)}(\ketbra{i}{i})\ket{1} \right| +  4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \left|- p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) + \frac{\alpha^2 t^2}{\dim + 1} \sum_{i > 1} p(i) q(0) \sinc^2((i - 2)t/2) \right| \nonumber \\
    &~+\sum_{i > 1} p(i) 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \left|- p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) + \frac{\alpha^2 t^2}{\dim + 1}  p(2) q(0) \right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \parens{ \sum_{i > 2} |-p(1) q(1) + p(i) q(0)| + (3 + 4 \dim_S)} \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \left|- p(1)  q(1) +  p(2) q(0) \right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \parens{ 2(\dim_S - 2) + (3 + 4 \dim_S)} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1} \parens{\left| -\frac{e^{-\beta}}{\partfun_S(\beta)} \frac{e^{-\beta}}{1 + e^{-\beta}} +\frac{e^{-\beta 2}}{\partfun_S(\beta)} \frac{1}{1 + e^{-\beta}}\right| + \epsilon_{\sinc} 6 \dim_S} \\
    &= 6 \dim_S \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \\
    &\le 3 \epsilon_{\sinc} \alpha^2 t^2 \\
    &= 12 \alpha^2.
\end{align}
In the second to last step we used the fact that $2 \dim_S = \dim$ and $\dim / (\dim + 1) \le 1$, and in the last step we used the fact that $\epsilon_{\sinc} = \frac{4}{\Delta_{\min}^2 t^2} = \frac{4}{t^2}$ where $\Delta_{\min} = 1$ for the harmonic oscillator. 

Next we can compute the last term where $j = \dim_S$, for this computation we will use $d = \dim_S$ to save space.
\begin{align}
    &\left| p(d) \bra{d} T^{(2)}(\ketbra{d}{d})\ket{d} + \sum_{i < d} p(i) \bra{d} T^{(2)}(\ketbra{i}{i})\ket{d} \right| \\
    &= \bigg| p(d) \bra{d} T^{(2)}(\ketbra{d}{d})\ket{d} + p(d) \frac{\alpha^2 t^2}{\dim + 1} q(0) \sum_{a < d} \sinc^2((d - a  + 1)t/2) - p(d) \frac{\alpha^2 t^2}{\dim + 1} q(0) \sum_{a < d} \sinc^2((d - a + 1)t/2) \nonumber \\ 
    &~ + \sum_{i < d} p(i) \bra{d} T^{(2)}(\ketbra{i}{i})\ket{d} \bigg| \\
    &\le \left|- p(d) \frac{\alpha^2 t^2}{\dim + 1} q(0) \sum_{a < d} \sinc^2((d - a + 1)t/2) + \sum_{i < d} p(i) \bra{d} T^{(2)}(\ketbra{i}{i})\ket{d} \right| +  4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1}\left|- p(d)  q(0) \sum_{a<d} \sinc^2((d - a + 1)t/2) + \sum_{i < d} p(i) q(1) \sinc^2((d - i + 1)t/2) \right| \nonumber \\
    &~+\sum_{i < d} p(i) 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1} \left|- p(d)q(0) +  p(d-1) q(1) \right| +  \epsilon_{\sinc}\frac{\alpha^2 t^2}{\dim + 1} \parens{ \sum_{i < d - 1} |-p(d) q(0) + p(i) q(1)| + (3 + 4 \dim_S)} \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \left| -\frac{e^{-\beta d}}{\partfun_S(\beta)} \frac{1}{1 + e^{-\beta}} +\frac{e^{-\beta (d - 1)}}{\partfun_S(\beta)} \frac{e^{-\beta}}{1 + e^{-\beta}}\right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \parens{ 2(\dim_S - 2) + (3 + 4 \dim_S)} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} 6 \dim_S \\
    &\le 12 \alpha^2.
\end{align}
Finally we can compute the middle terms for $2 \le j \le \dim_S - 1$. This time we will skip the explicit adding and subtracting of terms and triangle inequalities used to match the results of Lemma \ref{lem:t_2_system_only}
\begin{align}
&\left|p(j) \bra{j} T^{(2)}(\ketbra{j}{j})\ket{j} + \sum_{i < j} p(i) \bra{j} T^{(2)}(\ketbra{i}{i})\ket{j} + \sum_{i > j} p(i) \bra{j} T^{(2)}(\ketbra{i}{i})\ket{j} \right| \\
&\leq \left| p(j) \bra{j} T^{(2)}(\ketbra{j}{j})\ket{j} + \frac{\alpha^2 t^2}{\dim + 1} \left( \sum_{i < j} p(i) q(1) \sinc^2((i - j + 1)t/2) + \sum_{i > j} p(i)q(0) \sinc^2((i - j - 1)t/2) \right) \right| \nonumber \\
&\quad + 6 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\leq \left| p(j) \bra{j} T^{(2)}(\ketbra{j}{j})\ket{j} + \frac{\alpha^2 t^2}{\dim + 1} \left( p(j-1) q(1) + p(j + 1)q(0) \right) \right| + 7 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\le \frac{\alpha^2 t^2}{\dim + 1} \left| - p(j) \parens{q(0)\sum_{a < j} \sinc^2((a - j + 1)t/2) + q(1) \sum_{a > j} \sinc^2((a - j - 1)t/2)} +  \left( p(j-1) q(1) + p(j + 1)q(0) \right) \right| \nonumber \\
&~+ \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}\parens{7 + 4 \dim_S} \\
&\le \frac{\alpha^2 t^2}{\dim + 1} \left| -p(j) (q(0) + q(1)) + p(j-1)q(1) + p(j+1) q(0) \right| +\frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}\parens{7 + 4 \dim_S + (\dim_S - 3) } \\
&= \frac{\alpha^2 t^2}{\dim + 1} \left| - \frac{e^{-\beta j}}{\partfun_S(\beta)} + \frac{e^{-\beta (j - 1)}}{\partfun_S(\beta)} \frac{e^{-\beta}}{1 + e^{-\beta}} + \frac{e^{-\beta (j + 1)}}{\partfun_S(\beta)} \frac{1}{1+e^{-\beta}} \right| +\frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}\parens{7 + 4 \dim_S + (\dim_S - 3) } \\
&\le 6 \dim_S \frac{\alpha^2 t^2}{\dim +1} \epsilon_{\sinc} \\
&\le 12 \alpha ^2,
\end{align}
where we assumed that $\dim_S \geq 4$ for the second to last substitution.

Now that we have computed every term, we return to Eq. \eqref{eq:fixed_pt_harmonic_osc_2} and use the substitution that each term is less than $12 \alpha^2$ to write
\begin{equation}
    \norm{T^{(2)} (\rho_S(\beta))}_1 \leq 12 \dim_S \alpha^2.
\end{equation}
which, along with the remainder error bound from Lemma \ref{lem:remainder_bound}, when plugged into Eq. \eqref{eq:fixed_pt_harmonic_osc_1} yields the Lemma statement. 
\end{proof}

\subsection{Convergence}
We now want to analyze the converge of this channel for the Harmonic oscillator and environment tuned to the harmonic oscillator gap. We are trying to show
\begin{equation}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1
\end{equation}
We use $p_{\beta}(k) = \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}$ and $q(i) = \frac{e^{-\beta_E \lambda_E(i)}}{1 + e^{-\beta_E \gamma}}$, and due to the fact that the environment is a two level system with eigenvalues 0 and 1 we have $q(0) = \frac{1}{1 + e^{-\beta_E}}$ and $q(1) = \frac{e^{-\beta_E}}{1 + e^{-\beta_E}}$.
We continue by expanding
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 &\leq \sum_j \left| p_{\beta_E}(j) - p_{\beta}(j) - \sum_i p_{\beta}(i) \bra{j}T^{(2)}(\ketbra{i}{i})\ket{j} \right| + \norm{R_{\Phi}}_1.
\end{align}
What we are going to do is plug in terms to allow us to utilize Lemma \ref{lem:t_2_system_only} and we will do so term by term over $j$, starting with the $j=1$, or ground state, term. The next step is to split the summation on $i$ into two parts, one part where $i = j$ and we have to use the more complicated self-transition expression from Lemma \ref{lem:t_2_system_only} and the other part where $i > j$. These steps are essentially the same as the proof for the fixed point theorem shown above, so we will skip some of the more explicit steps and refer the reader to the above proof for reference.
\begin{align}
    &\left| p_{\beta_E}(1) - p_{\beta}(1) - p_{\beta}(1) \bra{1} T^{(2)}(\ketbra{1}{1})\ket{1} - \sum_{i > 1} p_{\beta}(i) \bra{1} T^{(2)}(\ketbra{i}{i})\ket{1} \right| \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) + p_{\beta}(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) - \sum_{i > 1} p_{\beta}(i) \bra{1} T^{(2)}(\ketbra{i}{i})\ket{1} \right| \nonumber \\
    & \quad +4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) + p_{\beta}(1) q(1) \frac{\alpha^2 t^2}{\dim + 1} - \sum_{i > 1} p_{\beta}(i) \bra{1} T^{(2)}(\ketbra{i}{i})\ket{1} \right| \nonumber \\
    &\quad + 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + p_{\beta}(1) q(1) \frac{\alpha^2 t^2}{\dim + 1} \sum_{a > 2} \epsilon_{\sinc} \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) + p_{\beta}(1) q(1) \frac{\alpha^2 t^2}{\dim + 1} - \frac{\alpha^2 t^2}{\dim + 1}\sum_{i > 1} p_{\beta}(i) q(0) \sinc^2((i - 2)t/2) \right| \nonumber \\
    &\quad + \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \parens{5 \dim_S + 3 \sum_{i > 1} p_{\beta}(i)} \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) + \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(1) q(1) - p_{\beta}(2) q(0))  \right| + \epsilon_{\sinc}\frac{\alpha^2 t^2}{\dim + 1} (5 \dim_S + 4) \\
    &= \left| p_{\beta_E}(1) - p_{\beta}(1) - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(2)q(0) (1 - e^{ - \delta})  \right| + \epsilon_{\sinc}\frac{\alpha^2 t^2}{\dim + 1} (5 \dim_S + 4) \\
\end{align}
Now assuming $\beta_E - \beta \coloneqq \delta > 0$, or we are cooling the system down, then for sufficiently small $\alpha^2 t^2$ this will reduce the ground state error. In other words, $\beta \leq \beta_E \implies p_{\beta}(1) \leq p_{\beta_E}(1)$, aka colder temperatures have a higher overlap with the ground state. This means that $\alpha t$ can be taken small enough such that it does not affect the sign of $p_{\beta_E}(1) - p_{\beta}(1) - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(2) q(0) (1 - e^{-\delta})$, and we can take this out of the absolute value, resulting in the final inequality
\begin{align}
    &\left| p_{\beta_E}(1) - p_{\beta}(1) - p_{\beta}(1) \bra{1} T^{(2)}(\ketbra{1}{1})\ket{1} - \sum_{i > 1} p_{\beta}(i) \bra{1} T^{(2)}(\ketbra{i}{i})\ket{1} \right| \\
    &\leq p_{\beta_E}(1) - p_{\beta}(1) - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(2) q(0)(1 - e^{-\delta}) + \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} (7 \dim_S) \\
    &= \left| p_{\beta_E}(1) - p_{\beta}(1)\right| - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(2) q(0)(1 - e^{-\delta}) + \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} (7 \dim_S)
\end{align}

We now compute a similar quantity but for $1 < j < \dim_S$. This time we will not include as many intermediate steps for isolating the dominant (non-$\epsilon_{\sinc}$) terms.
\begin{align}
    &\left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j) \bra{j} T^{(2)}(\ketbra{j}{j})\ket{j} - \sum_{i < j} p_{\beta}(i) \bra{j} T^{(2)}(\ketbra{i}{i})\ket{j} - \sum_{i > j} p_{\beta}(i) \bra{j} T^{(2)}(\ketbra{i}{i})\ket{j} \right| \\
&\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j) \bra{j} T^{(2)}(\ketbra{j}{j})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1}\sum_{i < j} p_{\beta}(i) q(1) \sinc^2((i - j + 1)t/2) - \sum_{i > j} p_{\beta}(i) \bra{j} T^{(2)}(\ketbra{i}{i})\ket{j} \right| \nonumber \\
&\quad + 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j) \bra{j} T^{(2)}(\ketbra{j}{j})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j-1) q(1) - \sum_{i > j} p_{\beta}(i) \bra{j} T^{(2)}(\ketbra{i}{i})\ket{j} \right| \nonumber \\
&\quad + 4 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j) \bra{j} T^{(2)}(\ketbra{j}{j})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j-1) q(1) - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j+1)q(0) \right| \nonumber \\
&\quad + 8 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\leq \bigg| p_{\beta_E}(j) - p_{\beta}(j) + p_{\beta}(j) \frac{\alpha^2 t^2}{\dim + 1} \parens{q(0) \sum_{a < j} \sinc^2 ((a - j + 1)t/2) + q(1) \sum_{a > j} \sinc^2((a - j - 1)t/2)} \nonumber \\
&\quad - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j-1) q(1) - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j+1)q(0) \bigg| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} (8 + 4\dim_S) \\
&\leq \left| p_{\beta_E}(j) - p_{\beta}(j) + \frac{\alpha^2 t^2}{\dim + 1} \parens{p_{\beta}(j) - p_{\beta}(j-1) q(1) - p_{\beta}(j+1) q(0)} \right| + 9 \dim_S \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} 
% &= \left| p_{\beta_E}(j) - p_{\beta}(j) + p_{\beta}(j) \frac{\alpha^2 t^2}{\dim + 1} \parens{1 - \frac{e^{\beta_E - \delta} + e^{\delta}}{e^{\beta_E} + 1}} \right| + 9 \dim_S \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1}.
\end{align}
Lets call the extra correction bits $s_j$. 

Now finally we study the last term, $j = \dim_S$
\begin{align}
    &\left| p_{\beta_E}(j) - p_{\beta}(j) - \sum_{i < j} p_{\beta}(i) \bra{j} T^{(2)}(\ketbra{i}{i}) \ket{j} - p_{\beta}(j) \bra{j} T^{(2)}(\ketbra{j}{j}) \ket{j} \right| \\
    &\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - \sum_{i < j} p_{\beta}(i) q(1) \frac{\alpha^2 t^2}{\dim + 1} \sinc^2((i - j + 1)t/2) - p_{\beta}(j) \bra{j} T^{(2)}(\ketbra{j}{j}) \ket{j} \right| \nonumber \\
    &\quad + 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j-1) q(1) \frac{\alpha^2 t^2}{\dim + 1} - p_{\beta}(j) \bra{j} T^{(2)}(\ketbra{j}{j}) \ket{j} \right| \nonumber \\
    &\quad + 4 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j-1) q(1) \frac{\alpha^2 t^2}{\dim + 1} + p_{\beta}(j) q(0) \frac{\alpha^2 t^2}{\dim + 1} \sum_{a < j} \sinc^2((a - j + 1)t/2) \right| \nonumber \\
    &\quad + 4 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j-1) q(1) \frac{\alpha^2 t^2}{\dim + 1} + p_{\beta}(j) q(0) \frac{\alpha^2 t^2}{\dim + 1} \right| \nonumber \\
    &\quad + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} (4 + 7 \dim_S) \\
    &\left| p_{\beta_E}(j) - p_{\beta}(j) + p_{\beta}(j) \frac{\alpha^2 t^2}{\dim + 1}  q(0)(1 - e^{-\delta}) \right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} (4 + 7 \dim_S)
\end{align}


Now we repeat a similar procedure to the first $j = 1$ term where we want to take the contribution of the channel outside of the absolute value. For $j = \dim_S$, we know that cooler temperatures will have smaller overlap with the highest energy state, or in symbols $\beta < \beta_E \implies p_{\beta}(\dim_S) > p_{\beta_E}(\dim_S)$. We can then make $\alpha t$ small enough such that it fits in the difference $p_{\beta}(\dim_S) - p_{\beta_E}(\dim_S)$, giving us
$$
p_{\beta}(\dim_S) - p_{\beta_E}(\dim_S) \geq p_{\beta} (\dim_S) q(0) \frac{\alpha^2 t^2}{\dim + 1} (1 - e^{-\delta}),
$$
as $\alpha t$ is user-controlled it is clear that a small enough value to satisfy the inequality exists. This inequality also implies the following
\begin{align}
    &\left| p_{\beta_E}(j) - p_{\beta}(j) + p_{\beta}(j) \frac{\alpha^2 t^2}{\dim + 1}  q(0)(1 - e^{-\delta}) \right| \\
    &= p_{\beta}(\dim_S) - p_{\beta_E}(\dim_S) - p_{\beta}(\dim_S) q(0) \frac{\alpha^2 t^2}{\dim + 1}(1 - e^{-\delta}) \\
    &= \abs{p_{\beta_E}(\dim_S) - p_{\beta}(\dim_S)} - p_{\beta}(\dim_S) q(0) \frac{\alpha^2 t^2}{\dim + 1}(1 - e^{-\delta}) \label{eq:harmonic_osc_s_dim}
\end{align}
and take it out of the absolute value

We need to find the terms where the intermediate contribution reduces the trace distance.
\begin{align}
    0 &\leq p_{\beta}(j) - p_{\beta}(j-1) q(1) - p_{\beta}(j+1) q(0) \\
    0 &\leq e^{-\beta j} - e^{-\beta(j - 1)} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} - e^{-\beta(j+1)} \frac{1}{1 + e^{-\beta_E}} \\
    0 &\leq 1 - e^{\beta} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} - e^{-\beta} \frac{1}{1 + e^{-\beta_E}} \\
    0&\leq 1 + e^{-\beta_E} -e^{-\delta} - e^{-\beta} \\
    0 &\leq 1 + e^{-\delta} e^{-\beta} - e^{-\delta } - e^{-\beta} \\
    0 &\leq 1 - e^{-\beta} - e^{-\delta} (1 - e^{-\beta}) \\
    0 &\leq (1-e^{-\beta})(1 - e^{-\delta}) \\
    \iff 0 &\leq \delta \text{ and } 0 \leq \beta.
\end{align}
Since we know that the extra term in the absolute value for the intermediate $1 < j < \dim_S$ contributions we can show that these reduce the trace distance if $p_{\beta_E}(j) - p_{\beta}(j) \leq 0$ for these terms. Our goal will be to show that this condition holds for $1 < j$ if $\delta$ is sufficiently small. 

For the truncated harmonic oscillator the partition function can be computed analytically 
\begin{equation}
    \partfun_S(\beta) = \frac{1 - e^{-\beta \dim_S}}{e^{\beta} - 1}. \label{eq:harmonic_oscillator_partfun}
\end{equation}
So can the average energy
 \begin{equation}
     \anglebrackets{H_S}_{\beta} = \frac{1}{e^{\beta} - 1} - \frac{\dim_S}{e^{\beta \dim_S} - 1},
 \end{equation}
 see the appendices for these computations. From the same lemma we see that if $k \leq \anglebrackets{H_S}_{\beta_E}$ then we know $\frac{e^{-\beta_E k}}{\partfun_S(\beta_E)} \geq \frac{e^{-\beta k}}{\partfun_S(\beta)}$.  Now we can use this to break our contributions up into useful and useless parts:
 \begin{align}
      &\sum_{j =1}^{\dim_S} \left| p_{\beta_E}(j) - p_{\beta}(j) - \sum_i p_{\beta}(i) \bra{j}T^{(2)}(\ketbra{i}{i})\ket{j} \right| \nonumber \\ 
      &\leq |p_{\beta_E}(1) - p_{\beta}(1) - s_1| + \sum_{j = 2}^{\dim_S - 1} |p_{\beta_E}(j) - p_{\beta}(j) + s_j| + |p_{\beta_E}(\dim_S) - p_{\beta}(\dim_S) - s_{\dim_S}| + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
      &= |p_{\beta_E}(1) - p_{\beta}(1)| - s_1 + \sum_{j = 2}^{\lfloor \anglebrackets{H_S}_{\beta_E} \rfloor} |p_{\beta_E}(j) - p_{\beta}(j) + s_j| + \sum_{j = \lceil \anglebrackets{H_S}_{\beta_E} \rceil}^{\dim_S - 1} |p_{\beta_E}(j) - p_{\beta}(j) + s_j| \nonumber \\
      &\quad +|p_{\beta_E}(\dim_S) - p_{\beta}(\dim_S)| - s_{\dim_S} + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
      &=\sum_{j=1}^{\dim_S} |p_{\beta_E}(j) - p_{\beta}(j)| - s_1 - s_{\dim_S} + \sum_{j = 2}^{\lfloor \anglebrackets{H_S}_{\beta_E} \rfloor} s_j - \sum_{j = \lceil \anglebrackets{H_S}_{\beta_E} \rceil}^{\dim_S - 1} s_j + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \label{eq:harmonic_oscillator_4}
\end{align}

Shifting our attention to the $s_j$ contributions, we will seek to linearize these terms with respect to $\delta$. Ignoring the $\alpha^2 t^2/(\dim + 1)$ prefactor, we write the $s_j$ terms for $1 < j < \dim_S$ as
\begin{align}
    p_{\beta}(j) - p_{\beta}(j - 1) q(1) - p_{\beta}(j + 1) q(0) &= p_{\beta}(j) \parens{1 - \frac{e^\beta e^{-\beta_E}}{1 + e^{-\beta_E}} - \frac{e^{-\beta}}{1 + e^{-\beta_E}}} \\
    &= p_{\beta}(j) \parens{1 - \frac{e^{-\delta} + e^{\delta} e^{-\beta_E}}{1 + e^{-\beta_E}}} \\
    &= p_{\beta}(j) \delta \frac{1 - e^{-\beta_E}}{1 + e^{-\beta_E}} + \bigo{\delta^2} \\
    &= p_{\beta}(j) \delta \tanh{\beta_E / 2} + \bigo{\delta^2} .
\end{align}
Similarly, the $s_1$ term can be expressed as
\begin{align}
    s_1 \parens{\frac{\alpha^2 t^2}{\dim + 1}}^{-1} &= p_{\beta}(2) q(0) (1-e^{-\delta}) \\
    &= p_{\beta}(2) \delta \frac{1}{1 + e^{-\beta_E}} + \bigo{\delta^2}. 
\end{align}
The $s_{\dim_S}$ term is given from Eq.\eqref{eq:harmonic_osc_s_dim} and is linearized with respect to $\delta$ as $p_{\beta}(\dim_S) \frac{\delta}{1 + e^{-\beta_E}}$. Plugging these in to Eq. \eqref{eq:harmonic_oscillator_4} yields 
\begin{align}
    &\sum_{j=1}^{\dim_S} |p_{\beta_E}(j) - p_{\beta}(j)| - s_1 - s_{\dim_S} + \sum_{j = 2}^{\lfloor \anglebrackets{H_S}_{\beta_E} \rfloor} s_j - \sum_{j = \lceil \anglebrackets{H_S}_{\beta_E} \rceil}^{\dim_S - 1} s_j + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &= \sum_{j=1}^{\dim_S} |p_{\beta_E}(j) - p_{\beta}(j)|  + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \nonumber \\
    &~+ \frac{\alpha^2 t^2}{\dim + 1}  \delta \left( - \frac{p_{\beta}(2) + p_{\beta}(\dim_S)}{1 + e^{-\beta_E}} + \tanh{\beta_E / 2} \sum_{j = 2}^{\lfloor \anglebrackets{H_S}_{\beta_E} \rfloor} p_{\beta}(j) - \tanh{\beta_E / 2} \sum_{j = \lceil \anglebrackets{H_S}_{\beta_E} \rceil}^{\dim_S - 1} p_{\beta}(j) \right)
    \end{align}

Now we need upper and lower bounds for the $s_j$ parts. For this we bound $s_j$ for $1 < j < \dim_S$ rather straightforwardly:
\begin{align}
    \frac{\alpha^2 t^2}{\dim + 1} \parens{p_{\beta}(j) - p_{\beta}(j-1) q(1) - p_{\beta}(j+1) q(0)} &\leq \frac{\alpha^2 t^2}{\dim + 1} p_{\beta} (j) \leq \frac{\alpha^2 t^2}{\dim + 1}
\end{align}
and
\begin{align}
    \frac{\alpha^2 t^2}{\dim + 1} \parens{p_{\beta}(j) - p_{\beta}(j-1) q(1) - p_{\beta}(j+1) q(0)} &\geq - \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(j-1) q(1) + p_{\beta}(j+1) q(0) ) \\
    &\geq - \frac{\alpha^2 t^2}{\dim + 1} (q(0) + q(1)) \\
    &= - \frac{\alpha^2 t^2}{\dim + 1}.
\end{align}
We leave $s_1$ and $s_{\dim_S}$ exact for now.
Plugging these simple bounds into Eq. \eqref{eq:harmonic_oscillator_4} yields
\begin{align}
    &\sum_{j=1}^{\dim_S} |p_{\beta_E}(j) - p_{\beta}(j)| - s_1 - s_{\dim_S} + \sum_{j = 2}^{\lfloor \anglebrackets{H_S}_{\beta_E} \rfloor} s_j - \sum_{j = \lceil \anglebrackets{H_S}_{\beta_E} \rceil}^{\dim_S - 1} s_j + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 - s_1 - s_{\dim_S} - \frac{\alpha^2 t^2}{\dim + 1} \parens{\dim_S - 1 - \ceil{\anglebrackets{H_S}_{\beta_E}} - \lfloor \anglebrackets{H_S}_{\beta_E} \rfloor + 1} + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}
\end{align}

One thing I'm curious about is if we can make $s_1 - s_{\dim_S}$ absorb the  $9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}$ error. So 
  
\subsubsection{Below is wrong but may have something useful.}
Now to tackle the above we want to find conditions on when the last two terms are at most zero, or in symbols
\begin{align}
    - \frac{\alpha^2 t^2}{\dim + 1} \parens{\dim_S - \ceil{\anglebrackets{H_S}_{\beta_E}} - \lfloor \anglebrackets{H_S}_{\beta_E} \rfloor} + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} &\leq 0 \\
    \epsilon_{\sinc} &\leq \frac{\dim_S - \ceil{\anglebrackets{H_S}_{\beta_E}} - \lfloor \anglebrackets{H_S}_{\beta_E} \rfloor}{9 \dim_S^2}. \label{eq:harmonic_osc_epsilon_sinc_bnd}
\end{align}
Now as a sanity check, the conditions for the RHS of the above expression to be positive are given by
\begin{align}
    \dim_S - \ceil{\anglebrackets{H_S}_{\beta_E}} - \lfloor \anglebrackets{H_S}_{\beta_E} \rfloor \geq 0, \label{eq:avg_energy_upper_bound}
\end{align}
which we can simplify using the fact that $2\anglebrackets{H_S}_{\beta_E} + 1 \geq \ceil{\anglebrackets{H_S}_{\beta_E}} + \lfloor \anglebrackets{H_S}_{\beta_E} \rfloor $, so Eq. \eqref{eq:avg_energy_upper_bound} is implied by
\begin{align}
    \anglebrackets{H_S}_{\beta_E} \leq \frac{\dim_S - 1}{2}.
\end{align}
Since the average energy is monotonically decreasing with respect to $\beta$, the sum $\anglebrackets{H_S}_{\beta = 0} = \frac{1}{\dim_S} \sum_{i = 1}^{\dim_S} i = \frac{\dim_S - 1}{2}$ shows that inequality \eqref{eq:avg_energy_upper_bound} is always satisfied. Since we know $1 \leq \anglebrackets{H_S}_{\beta_E} \leq \frac{\dim_S - 1}{2}$ and that $\epsilon_{\sinc} = \frac{4}{t^2 \Delta_{\min}^2} = \frac{4}{t^2}$, we can compute a worst-case lower bound on $t$, such that Eq. \eqref{eq:harmonic_osc_epsilon_sinc_bnd} is always satisfied, as
\begin{align}
    \frac{4}{t^2} &\leq \frac{\dim_S - 1 - 2\anglebrackets{H_S}_{\beta_E}}{9 \dim_S^2} \\
    t &\geq 6 \frac{\dim_S}{\sqrt{\dim_S - 1 - 2 \anglebrackets{H_S}_{\beta_E}}}.
\end{align}
% \todo{Need to compute a decent bound on the average energy to get a non-dumbo time bound.}

Now returning to our main task we have the inequality
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 &\le \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 - s_1 - s_{\dim_S} - \frac{\alpha^2 t^2}{\dim + 1} \parens{\dim_S - 1 - \ceil{\anglebrackets{H_S}_{\beta_E}} - \lfloor \anglebrackets{H_S}_{\beta_E} \rfloor + 1} + 9 \dim_S^2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
\end{align}

\subsubsection{Attempt \# 4}
So the above does not work because it appears that the leading order distance reduction comes from the middle term with the factor of $\dim_S$ and $\tilde{\alpha}^2$. So we have to analyze the contribution from this term much more carefully. But first, we want to linearize the contribution from $\delta$ to make the analysis a bit easier. Now we can write the sum of $s_1$ and $s_{\dim_S}$ as
$$
s_1 + s_{\dim_S} = \frac{\alpha^2 t^2}{\dim + 1}q(0)(1 - e^{-\delta}) \left( p_{\beta}(2) + p_{\beta}(\dim_S) \right).
$$
Our main goal now is to produce a useful lower bound on this quantity and we would like to do so exclusively in terms of $\beta_E$ and $\delta$. The first lower bound we take is that the $p_{\beta}(\dim_S)$ term should be vanishingly negligible in the large $\dim_S$ limit, precisely it is  $\bigo{e^{-\beta \dim_S}}$ so we ignore it. Then we utilize the partition function as computed in Eq. \eqref{eq:harmonic_oscillator_partfun} and convert it to $\beta_E$'s and $\delta$'s. Note that we fully expect $\frac{\delta}{\beta_E}$ to be equal to $1 / L$, where $L \in \bigo{\dim_S}$ in the worst case. 
\begin{align}
    q(0) p_{\beta}(2) (1 - e^{-\delta}) &= \frac{1}{1 + e^{-\beta_E}} \frac{e^{-2 \beta_E} e^{2 \delta}}{\parens{\frac{1 - e^{- \beta_E \dim_S} e^{\delta \dim_S}}{e^{\beta_E} e^{-\delta} - 1}}} (1 - e^{-\delta }) \\
    &= \frac{e^{-2 \beta_E} e^{2 \delta} (e^{\beta_E} e^{-\delta} - 1) ( 1 - e^{-\delta})}{(1 + e^{-\beta_E})(1 - e^{-\beta_E \dim_S} e^{\delta \dim_S})} \\
    &\geq \frac{e^{- \beta_E} e^{\delta}(1 - e^{-\beta_E} e^{\delta}) (1 - e^{-\delta})}{1 + e^{-\beta_E}}.
\end{align}
Now performing an expansion around $\delta = 0$ for the above upper bound shows
\begin{align}
    \frac{e^{- \beta_E} e^{\delta}(1 - e^{-\beta_E} e^{\delta}) (1 - e^{-\delta})}{1 + e^{-\beta_E}} &= \delta \frac{e^{-\beta_E}(1 - e^{-\beta_E})}{1 + e^{-\beta_E}} + \bigo{\delta^2} \\
    &= \delta \frac{1 - e^{-\beta_E}}{1 + e^{\beta_E}} + \bigo{\delta^2}
\end{align}
Need to determine what the upper bounds on $\delta$ are for this to be $\epsilon$ accurate.

Now we can 



\subsubsection{Rambling that I need to correct}
With an eye toward future needs, we will parametrize $\delta = \frac{\beta_E}{L}$. Now we want to lower bound each of the three terms by an arbitrary $\epsilon$ for now.
\begin{align}
    1 - e^{-\delta} &\geq \epsilon_1 \\
    \frac{\beta_E}{L} &\geq \log \left( \frac{1}{1 - \epsilon_1} \right)
\end{align}
and similarly
\begin{align}
    1 - e^{-\beta} e^{\delta} &\geq \epsilon_2 \\
    \beta_E \left( 1 - \frac{1}{L} \right) &\geq \log \parens{\frac{1}{1 - \epsilon_2}}.
\end{align}
The last inequality yields an upper bound on $\beta_E$
\begin{align}
    \frac{e^{-\beta_E} e^{\delta}}{1 + e^{-\beta_E}} &\geq \frac{1}{2} e^{-\beta_E} e^{\delta} \\
    \frac{1}{2} e^{-\beta_E} e^{\delta} &\geq \epsilon_3 \\
    \beta_E \parens{1 - \frac{1}{L}} &\leq \log \parens{\frac{1}{2 \epsilon_3}}.
\end{align}
How we divy up these bounds will be our next focus, but since we know that we want $\beta_E \in \Omega(1)$ we can think of $\epsilon_1$ and $\epsilon_2$ as constants.

Returning to our inequality, we would like to show that 
\begin{align}
    \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 - \frac{\alpha^2 t^2}{\dim + 1} \epsilon_1 \epsilon_2 \epsilon_3 &\leq 12 \dim_S \alpha^2.
\end{align}
Now using the bad upper bound from Lemma \ref{lem:thermal_state_diff_bound} we can reduce this to
\begin{align}
    2\dim_S^2 \delta - \frac{\alpha^2 t^2}{\dim + 1} \epsilon_1 \epsilon_2 \epsilon_3 &\leq 12 \dim_S \alpha^2.
\end{align}
Now one approach to salvaging something useful from this is to view it as a quadratic in terms of $\dim_S$. We can then use the solutions to quadratics to get the bound
\begin{align}
    \dim_S \le \frac{12 \alpha^2}{4 \delta} + \frac{1}{4 \delta}\sqrt{144 \alpha^4 + 8 \delta \frac{\alpha^2 t^2}{\dim+ 1}\epsilon_1 \epsilon_2 \epsilon_3}.
\end{align}
Or we can go right of the rip and use
\begin{align}
    \delta \le \frac{12 \dim_S \alpha^2 + \alpha^2 t^2 \epsilon_1 \epsilon_2 \epsilon_3 / (\dim+1)}{2 \dim_S^2}
\end{align}
Now can we linearize the RHS with respect to $\delta$? $\epsilon_1 = \delta +\bigo{\delta^2}$, $\epsilon_2 = 1 - e^{-\beta_E}( 1 + \delta) + \bigo{\delta^2} $ and lastly $\epsilon_3 = \frac{1}{2} e^{-\beta_E}(1 + \delta) + \bigo{\delta^2}$. Now this gives the product as $$ \epsilon_1 \epsilon_2 \epsilon_3 = \delta \frac{e^{-\beta_E}}{2}(1 - e^{-\beta_E}) + \bigo{\delta^2}. $$ Now substituting this in to the above we get
$$\delta \le \frac{6 \alpha^2}{\dim_S - \frac{\alpha^2 t^2}{4 (\dim + 1)} (1 - e^{-\beta_E})e^{-\beta_E}}.$$

So now given that this is true, lets see what ranges of temperatures can be prepared with this. 


\subsubsection{Improved norm upper bound}
The next computation we will do is an upper bound on the one norm of the difference in the two thermal states. For this we use Cauchy-Schwartz as shown below
\begin{align}
    \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 &= \sum_{i = 1}^{d} \abs{\frac{e^{-\beta_E i}}{\partfun_S(\beta_E)} - \frac{e^{-\beta i}}{\partfun_S(\beta)}} \\
    &= \sum_{i = 1}^{d} \abs{\frac{e^{-\beta_E i}}{\partfun_S(\beta_E)} \parens{1 - e^{\delta i } \frac{\partfun_S(\beta_E)}{\partfun_S(\beta)}}} \\
    &\le \sqrt{\sum_i \parens{\frac{e^{-\beta_E i}}{\partfun_S(\beta_E)}}^2} \sqrt{\sum_i \parens{1 - e^{\delta i } \frac{\partfun_S(\beta_E)}{\partfun_S(\beta)}}^2}.
\end{align}
As we would like to consider low-temperature states, the purity of which approaches 1 as $\beta \to \infty$, we can upper bound the left radical with 1. 


\subsection{Final convergence}
This section is going to summarize the results and runtime needed to reach a given thermal state $\beta$. Our approach for this is to show that by taking a cooling schedule with short enough steps we can guarantee that the output will be controllably close to the desired state. For this, we will imagine starting the system off in the maximally mixed state $\beta_0 = 0$ and an environment at temperature $\beta_1 = \frac{\beta}{L}$. With each interaction channel we cool the environment down by $\beta / L$, and we do this $L$ times until we reach an environment of $\beta_L = \beta$. Let $\Phi_i$ denote the channel with environment at inverse temperature $\beta_i$. Our goal is to show
\begin{align}
    \norm{\rho_S(\beta) - \Phi_L \circ \Phi_{L-1} \circ \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \leq \epsilon.
\end{align}
To do this, we add and subtract terms of the form $\Phi_{L}(\rho_S(\beta_{L-1}))$ and use the fact that the trace norm is non-increasing under the action of channels $\norm{\Psi(X)}_1 \leq \norm{X}_1$. 
\begin{align}
    \norm{\rho_S(\beta) - \Phi_L \circ  \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 &= \norm{\rho_S(\beta) - \Phi_L(\beta_{L-1}) + \Phi_{L}(\beta_{L - 1}) - \Phi_L \circ  \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \\
    &\leq \norm{\rho_S(\beta) - \Phi_L(\beta_{L-1})}_1 + \norm{\beta_{L-1} - \Phi_{L-1} \circ \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \\
    &\le \sum_{i = 1}^{L} \norm{\rho_S(\beta_i) - \Phi_{i}(\rho_S(\beta_{i - 1}))}_1
\end{align}
Now using the result we have that $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \le 12 \dim_S \alpha^2 $ we have that 
$$\norm{\rho_S(\beta) - \Phi_L \circ  \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \le 12 L \dim_S \alpha^2$$
Now this serves as an upper bound on $L$ as $L \le \frac{\epsilon}{12 \dim_S \alpha^2}$. Now to see if there exists an $L$ that satisfies these two bounds simultaneosly we require


\section{Fixed points and convergences}

In this first section we introduce the conditions required for more detailed analysis. The first is 
\begin{definition}[Well-separated Hamiltonian] \label{def:separated_hamiltonians}
    A Hamiltonian $H_S$ is $(\Delta_{\sinc}, \Delta_{\min}$)-separated if there are not only no degeneracies, but every eigenvalue difference is bounded from below and eigenvalue differences are sufficiently spaced.
    \begin{align}
        &\forall i \neq j : \Delta_{S}(i,j) \geq \Delta_{\min}, \\
        &\forall (i,j) \neq (k,l) : \abs{\Delta_S(i,j) - \Delta_S(k,l)} \geq \Delta_{\sinc} + \Delta_{\min}.
    \end{align}
    This rather strong condition on the spectrum is needed as we would like to make the guarantee that for a provided $\gamma$ there is at most 1 pair of indices $(i,j)$ such that $\sinc^2((\Delta_S(i,j) - \gamma)t/2) \geq 1 - \epsilon_{\sinc}$.
    \begin{equation}
        \abs{\set{(i,j): \abs{\Delta_S(i,j) - \gamma} \leq \Delta_{\sinc}}} \in \set{0, 1}.
    \end{equation}
    It will be useful when working from this to sample from these differences uniformly. Given a Hamiltonian with eigenvalues that are well separated there are $\frac{\dim_S (\dim_S - 1)}{2}$ positive differences. We create the following distribution which is a mixture of uniform distributions over the the eigenvalue differences, where each uniform distribution is of width $2 \Delta_{\sinc}$, so we can guarantee that any sampled value of $\gamma$ is close enough to an eigenvalue difference to satisfy the requirements of Corollary \ref{cor:gamma_difference_reqs}.

    \begin{equation}
        \prob{\gamma = x} = \begin{cases}
            \frac{2}{\dim_S (\dim_S - 1)}
            & x = \Delta_S(i,j) \text{ for some i,j} \\
            0 & \forall (i,j), x \neq \Delta_S(i,j).
        \end{cases}
    \end{equation}
    We will refer to this distribution as the \emph{perfect knowledge} distribution.
\end{definition}

The following lemma is purely a technical one to allow for easier computation of sums that appear from the second order correction of $\Phi$.
\begin{lemma} \label{lem:transition_idx_sub}
    Let $p, q$ denote probability distributions on the system and environment indices respectively and $\tau$ the transition amplitude from Theorem \ref{thm:second_order_transition_coeffs}. Then we have
    \begin{equation}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | k,l) = \sum_{i \neq k} \sum_{j,l} (p(i) q(j) - p(k) q(l)) \tau(i,j |k,l)
    \end{equation}
\end{lemma}
\begin{proof}
    \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | k,l) &= \sum_{i \neq k} \sum_{j,l} \parens{p(i) q(j) \tau(i,j|k,l)} + \sum_{j,l} p(k) q(j) \tau(k,j | k,l) \label{eq:n_qubit_fixed_pt_intermediate_1}.
    \end{align}
    We expand the simpler sum on the right:
    \begin{align}
        \sum_{j,l}p(k) q(j) \tau(k,j| k,l) &= p(k) q(0) (\tau(k,0|k,0) + \tau(k,0|k,1)) + p(k) b(1) (\tau(k,1|k,0) + \tau(k,1|k,1)) \\
        &= - p(k) q(0) \sum_{c \neq k}\sum_{l} \tau(k,0 | c, l) - p(k) b(1) \sum_{c \neq k} \sum_{l}\tau(k,1|c,l) \\
        &= - \sum_{c \neq k} \sum_{j,l} p(k) q(j) \tau(k,j |c,l).
    \end{align}
    Plugging this into Eq. \ref{eq:n_qubit_fixed_pt_intermediate_1} allows us to simplify as follows
    \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j|k,l) &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) \tau(i,j|k,l)) - \sum_{c \neq k} \sum_{j,l} p(k) q(j) \tau(k,j|c,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) \tau(i,j|k,l)) - \sum_{i \neq k} \sum_{j,l} p(k) q(l) \tau(i,j|k,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) - p(k) q(l)) \tau(i,j | k,l).
    \end{align}
\end{proof}

This lemma is also a purely technical one that allows us to substitute in simple values for the sums that appear in the transition terms. 
\begin{lemma} \label{lem:big_tau_sum_simplifier}
    Assume that $H_S$ is a $(\Delta_{\min}, \Delta_{\sinc}, \epsilon_{\sinc}$)-separated Hamiltonian satisfying Def. \ref{def:separated_hamiltonians}. Let $(\gamma_U,\gamma_L)$ denote the unique index pair such that $|\Delta_S(\gamma_U,\gamma_L) - \gamma| \leq \Delta_{\sinc}$ and for all $(i,j) \neq (\gamma_U, \gamma_L)$ we have $|\Delta_S(i,j) - \gamma| \geq \Delta_{\min}$. We let $\gamma_U$ denote the higher energy state, i.e $\lambda_S(\gamma_L) < \lambda_S(\gamma_U)$. Using $\delta \coloneqq \beta_E - \beta$, we show that the assumptions on $H$ and $\gamma$ along with the requirement $\delta \geq \beta_E \frac{\Delta_{\min}}{\Delta_S(u,l)} + \frac{1}{\Delta_S(u,l)} \ln \parens{\frac{1}{(2\dim + 1) \epsilon_{\sinc}}}$ imply the following bounds.
    Let $p(i) = \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)}$ and $q(j) = \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)}$. 
    \begin{align}
        \abs{\sum_{i, j, l} p(i) q(j) \tau(i,j| \gamma_L, l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} &\leq 4 \alpha^2 t^2 \epsilon_{\sinc}  \\
        \abs{\sum_{i, j, l} p(i) q(j) \tau(i,j| \gamma_U, l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_L) q(1)} &\leq 4  \alpha^2 t^2 \epsilon_{\sinc} \\
        k \neq \gamma_L, \gamma_U \implies \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j| k, l) } &\leq 4 \alpha^2 t^2 \epsilon_{\sinc}.
    \end{align}
\end{lemma}
\begin{proof}
    \todo{update this lemma to use $p,q$ instead of $a,b$. }
    For simplicity, we let $a(i') = \frac{e^{-\beta \lambda_S(i')}}{\partfun_S(\beta)}$ and $b(j') = \frac{e^{-\beta_E \lambda_E(j')}}{\partfun_E(\beta_E)}$. Using Lemma \ref{lem:transition_idx_sub} we write the summation as
    \begin{align}
        &\sum_{i,j,k} \sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) = \sum_{i \neq l} \sum_{j,k} (a(i) b(j) - a(l) b(k)) \tau(i,j|l,k) \\
        &= \sum_{j,k} (a(u) b(j) - a(l) b(k)) \tau(u,j | l,k) + \sum_{i \neq l,u} \sum_{j,k} (a(i) b(j) - a(l) b(k)) \tau(i,j| l,k).
    \end{align}
    Now we utilize the fact that $i \neq u,l$ implies that $\tau(i,j|l,k) \leq \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}$, and further that $\tau(u,j|l,k) \geq \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}$ if and only if $k = 1$ and $j = 0$. We also have that $-1 \leq a(i') b(j') - a(k') b(l') \leq 1$ for all $i', j', k', l'$ as $a,b$ are probabilities. These observations yield the upper bound
    \begin{align}
        &\sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) \nonumber \\
        &\leq (a(u) b(0) - a(l) b(1)) \tau(u,0|l,1) + 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 4 (\dim_S - 2) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
        &\leq (a(u) b(0) - a(l) b(1)) \tau(u,0|l,1) + 2 \dim \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \\
        &\leq (a(u) b(0) - a(l) b(1)) \tau(u,0|l,1) + 2 \epsilon_{\sinc} \alpha^2 t^2.
    \end{align}
    For the purposes of the upper bound, all we require is that $e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 0$, which implies $a(u) b(0) - a(l) b(1) \leq \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)}$. This, combined with the fact that $\tau(u,0|l,1) \leq \frac{\alpha^2 t^2}{\dim + 1}$, yields our final upper bound as
    \begin{equation}
        \sum_{i,j,k} \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_E(\beta_E)} \tau(i,j|l,k) \leq \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} + 2 \epsilon_{\sinc} \alpha^2 t^2. \label{eq:transition_prob_upper_bound_final}
    \end{equation}
    As constant factors are relatively unimportant, we will upper bound $2 \epsilon_{\sinc} \alpha^2 t^2 \leq 4 \epsilon_{\sinc} \alpha^2 t^2$ in the final lemma statement.

    We turn our attention now to the lower bound. This proceeds in a similar manner, of isolating the indices in the sum that are active in the transitions and those that aren't.
    \begin{align}
        &\sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) \nonumber \\
        &= \sum_{j,k} (a(u) b(j) - a(l) b(k)) \tau(u,j | l,k) + \sum_{i \neq l,u} \sum_{j,k} (a(i) b(j) - a(l) b(k)) \tau(i,j| l,k) \\
        &\geq (a(u) b(0) - a(l) b(1)) \tau(u,0 | l,1) -(\tau(u,0|l,0) + \tau(u,1|l,0) + \tau(u,1|l,1)) - \sum_{i \neq l,u} \sum_{j,k} \tau(i,j| l,k) \\
        &\geq (a(u) b(0) - a(l) b(1))\tau(u,0|l,1) - \frac{\alpha^2 t^2}{\dim + 1} \parens{ 3 \epsilon_{\sinc}  + 4 (\dim_S - 2) \epsilon_{\sinc} } \\
        &\geq (a(u) b(0) - a(l) b(1))\tau(u,0|l,1) - 2 \epsilon_{\sinc} \alpha^2 t^2. \label{eq:transition_prob_lower_bound}
    \end{align}
    We now return to the prefactor $a(u) b(0) - a(l) b(1)$ in a bit more detail. We write this as follows
    \begin{align}
        a(u) b(0) - a(l) b(1) &= \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)} - \frac{e^{-\beta \lambda_S(l)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} \\
        &= \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \parens{1 - e^{-\beta_E \gamma - \beta \lambda_S(l) + \beta \lambda_S(u)}}.
    \end{align}
    

    We need to understand the function $f(\beta) = 1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)}$ a bit better. We see that in the limit as $\gamma \to \Delta_S(u,l)$ and $\beta \to \beta_E$, $f(\beta) \to 0$. In the other regime we see as $\beta_E \to \infty$ then $f(\beta) \to 1$. This suggests that we need to look at the regime where $\delta \coloneqq \beta_E - \beta$ is sufficiently large. We first investigate when this function is positive
    \begin{align}
        1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 0 &\iff \beta_E \gamma \geq \beta \Delta_S(u,l) \\
        &\iff 0 \leq \beta_E (\gamma - \Delta_S(u.l)) + (\beta_E - \beta) \Delta_S(u,l).
    \end{align}
    Now using $\delta = \beta_E - \beta$ and $|\Delta_S(u,l) - \gamma| \leq \Delta_{\sinc} \leq \Delta_{\min}$, which implies $\gamma - \Delta_S(u,l) \geq - \Delta_{\min}$, we get that 
    \begin{equation}
        -\beta_E \Delta_{\min} + \delta \Delta_S(u,l) \leq \beta_E (\gamma - \Delta_S(u,l)) + (\beta_E - \beta) \Delta_S(u,l).
    \end{equation}
    We then conclude that $0 \leq -\beta_E \Delta_{\min} + \delta \Delta_S(u,l)$, or $\delta \geq \beta_E \frac{\Delta_{\min}}{\Delta_S(u,l)}$, implies that $1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 0$. Using this, we show that $\delta \geq \frac{\beta_E \Delta_{\min} + \ln(1/\epsilon_{exp})}{\Delta_S(u,l)}$ yields the following
    \begin{align}
        \delta \geq \frac{\beta_E \Delta_{\min} + \ln(1/\epsilon_{exp})}{\Delta_S(u,l)} &\implies \beta_E \gamma - \beta \Delta_S(u,l) \geq \ln 1/ \epsilon_{exp} \\
        &\implies e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \leq \epsilon_{exp} \\
        &\implies 1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 1 - \epsilon_{exp}.
    \end{align}

    We can profit off of the above upper bound, as well as the fact that $|\Delta_S(u,l) - \gamma| \leq \Delta_{\sinc}$ which implies $\tau(u,0|l,1) \geq \frac{\alpha^2 t^2}{\dim + 1} (1 - \epsilon_{\sinc})$, by simply plugging it into Eq. \eqref{eq:transition_prob_lower_bound} and simplifying
    \begin{equation}
        \sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) \geq \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)}\frac{\alpha^2 t^2}{\dim + 1} (1 - \epsilon_{exp})(1 - \epsilon_{\sinc}) - 2 \epsilon_{\sinc} \alpha^2 t^2.
    \end{equation}
    Using simple bounds allows us to rewrite this as
    \begin{equation}
        \sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) - \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)}\frac{\alpha^2 t^2}{\dim + 1} \geq - \alpha^2 t^2 \parens{\frac{\epsilon_{exp} + \epsilon_{\sinc}}{\dim + 1} + 2 \epsilon_{\sinc}}. \label{eq:transition_prob_lower_bound_final}
    \end{equation}
    We now use the fact that we only need to control $\epsilon_{exp}$ relatively loosely to simplify the bound. Setting $\epsilon_{exp} = (2 \dim + 1) \epsilon_{exp}$ for Eq. \eqref{eq:transition_prob_lower_bound_final}, along with the inequality from Eq. \eqref{eq:transition_prob_upper_bound_final}, yields the Lemma statement.

    The last inequality we need is the rather straightforward one where $k \neq \gamma_L, \gamma_U$
    \begin{align}
        \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|k,l)} &\leq \abs{\sum_{i\neq k} \sum_{j,l} (p(i)q(j) - p(k) q(l)) \tau(i,j|k,l)} \\
        &\leq \sum_{i \neq k} \sum_{j,l} \abs{p(i) q(j) - p(k) q(l)} \tau(i,j|k,l) \\
        &\leq \sum_{i \neq k} \sum_{j,l} \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
        &\leq 4 \dim \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
        &\leq 4 \alpha^2 t^2 \epsilon_{\sinc}
    \end{align}
\end{proof}

From the Detailed Balance arguments in Section \ref{sec:detailed_balance} we expect that in the infinite time limit, with perfect knowledge of the eigenvalue gaps, that the thermal state $\rho_S(\beta_E)$ is a fixed point of the channel $\Phi$. Here we give rigorous bounds showing that for all choices of the environment gap $\gamma$ we can make $\rho_S(\beta_E)$ as close to fixed as we want. As we know that $\gamma$ being far away from all eigenvalue differences yields a channel that is approximately the identity channel, aka it does nothing, we should aim to show that it is arbitrarily close to being a fixed point even when $\gamma$ is close to an eigenvalue gap $\Delta_S(i,j)$. 
\begin{theorem}[Approximate Fixed Points]
    Let $H_S$ be a well-separated Hamiltonian, per Def. \ref{def:separated_hamiltonians}. For all values of $\gamma \geq \Delta_{\min}$ we have that 
    \begin{align}
        2 \sqrt{2} \Delta_{\min} \dim \leq t& \text{ \emph{and} } \alpha \leq \min \set{\frac{\sqrt{\epsilon_{fix}}}{4 \sqrt{2} \Delta_{\min} \dim}, \parens{\frac{\epsilon_{fix}}{4 \dim^5}}^{1/3} \frac{1}{2 \sqrt{2} \Delta_{\min} }} \nonumber \\
        &\implies \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_1 \leq \epsilon_{fix}. \label{eq:beta_e_fixed_point_reqs}
    \end{align}
\end{theorem}
\begin{proof}
    We start out with the expansion of the channel
    \begin{align}
        \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_1 &= \norm{T^{(2)}(\rho_S(\beta_E), \alpha) + R_{\Phi}(\rho_S(\beta_E))}_1 \\
        &\leq \norm{T^{(2)}(\rho_S(\beta_E))}_1 + \norm{R_{\Phi}(\rho_S(\beta_E))}_1 \\
        &\leq \norm{T^{(2)}(\rho_S(\beta_E))}_1 + \dim \norm{R_{\Phi}} \\
        &\leq \norm{T^{(2)}(\rho_S(\beta_E))}_1 + \dim \norm{R_{\Phi}}.
    \end{align}
    Taking the straightforward approach of dividing our error budget equally among these two terms will be sufficient. Starting with the $\norm{R_{\Phi}}_1$ bound we have from Lemma \ref{lem:remainder_bound} that
    \begin{align}
        \alpha t \leq \parens{\frac{\epsilon_{fix}}{4 \dim^2}}^{1/3} \implies \dim \norm{R_{\Phi}} \leq \frac{\epsilon_{fix}}{2}. \label{tmp:first_alpha_t_req_fixed_pt}
    \end{align}

    We now bound the other half of our error budget, the term resulting from $\norm{T^{(2)}(\rho_S(\beta_E))}_1$. Plugging in the second order correction for the channel from Eq. \eqref{eq:second_order_channel_with_tau} gives us
    \begin{align}
        \norm{T^{(2)}(\rho_S(\beta_E))}_1 &= \sum_k \abs{\sum_{i,j,l} \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_E(\beta_E)} \tau(i,j|k,l)}.
    \end{align}
    For compactness, let $p(i) = \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}$ and $q(j) =\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_E(\beta_E)}$. Using Lemma \ref{lem:big_tau_sum_simplifier} we can substitute this 
    \begin{align}
        &\sum_{k \neq \gamma_L, \gamma_U} \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|k,l) } + \abs{\sum_{i,j,l} p(i)q(j) \tau(i,j | \gamma_L, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j| \gamma_U, l)} \\
        &\leq (\dim_S - 2) 4 \alpha^2 t^2 \epsilon_{\sinc} + \abs{\sum_{i,j,l} p(i)q(j) \tau(i,j | \gamma_L, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j| \gamma_U, l)}. \label{int:fixed_pt_1}
    \end{align}
    Now we use the following upper bound
    \begin{align}
        \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|\gamma_L ,l)} &= \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|\gamma_L ,l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0) + \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} \\
        &= \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|\gamma_L ,l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} + \abs{\frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} \\
        &\leq 4 \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0).
    \end{align}
    Plugging this into Eq. \eqref{int:fixed_pt_1} yields
    \begin{align}
        \norm{T^{(2)} (\rho_S(\beta_E))}_1 &\leq 2 \dim \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1}(p(\gamma_U) q(0) + p(\gamma_L) q(1)) \\
        &\leq 2 \dim \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} \\
        &= \alpha^2 t^2 \parens{2 \dim \epsilon_{\sinc} + \frac{1}{\dim + 1}}.
    \end{align}
    Now here we use the requirement that $\epsilon_{\sinc} \leq \frac{1}{\dim + 1} \frac{1}{2 \dim}$ to yield the inequality
    \begin{equation}
        \norm{T^{(2)}(\rho_S(\beta_E))}_1 \leq 2 \alpha^2 t^2. \label{eq:upper_bound_on_T_2}
    \end{equation}
    We note that $\epsilon_{\sinc} \leq \frac{1}{2 \dim (\dim + 1)}$ is equivalent to the lower bound on $t$ of $2 \sqrt{2} \Delta_{\min} \dim \leq t$. Scaling of $t$ with dimension is expected in the worst case. 

    It is then straightforward to see that $\alpha^2 t^2 \leq \frac{\epsilon_{fix}}{4}$ implies $\norm{T^{(2)}(\rho_S(\beta_E))} \leq \epsilon_{fix} / 2$, which along with Eq. \eqref{tmp:first_alpha_t_req_fixed_pt} implies Eq. \eqref{eq:beta_e_fixed_point_reqs}. 
\end{proof}

We now have shown that the thermal state at the same temperature as the environment, $\rho_S(\beta_E)$, remains approximately fixed even when we choose a $\gamma$ that causes transitions in the system. This argument relies on taking a very small value of $\alpha$. However, as $\alpha$ gets too small and approaches zero we have that the channel acts as time evolution by non-interacting Hamiltonians $H_S$ and $H_E$, meaning that every state that is diagonal in the $H_S$ basis could also be an approximate fixed point! We show that this is not the case and that there is a temperature $\beta$ that causes $\rho_S(\beta)$ to not be an approximate fixed point. This relies on $\gamma$ being close to an eigenvalue difference $\Delta_S(i,j)$, as otherwise our channel approximates the identity on matrices diagonal in the $H_S$ basis. 
\begin{theorem}[Not Fixed Points] \label{thm:which_beta_not_fixed}
    Let $H_S$ be a well-separated Hamiltonian, $\gamma$ be $\Delta_{\sinc}$ close to $\Delta_S(\gamma_U, \gamma_L)$, and $\delta = \beta_E - \beta$. We show that
    \begin{equation}
        \delta \geq \delta_{LB} \implies \norm{\rho_S(\beta) - \Phi^{(2)}(\rho_S(\beta))}_1 \geq \epsilon_{fix}.
    \end{equation}
\end{theorem}
\begin{proof}
\textbf{THIS SHOULD NOT BE A PROOF IT IS INCOMPLETE}


     We use the expansion of $\Phi^{(2)}(X) = X + T^{(2)} (X)$, along with defining $p(i) = \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)}$ and $q(j) = \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)}$ to write
     \begin{align}
        &\norm{\rho_S(\beta) - \Phi^{(2)} (\rho_S(\beta))}_1 = \norm{T^{(2)}(\rho_S(\beta))}_1 = \sum_k \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | k,l)} \\
        &= \sum_{k \neq \gamma_U, \gamma_L} \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|k,l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_U, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l)} \\
        &\geq \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_U, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l)} \\
        &\geq \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_U, l) + \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l)
     \end{align}
     
     It will prove simpler to lower bound one of the two terms and the other will follow almost the exact same manner.
     \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j| \gamma_U,l) &= \sum_{i \neq \gamma_U} \sum_{j,l} (p(i) q(j) - p(\gamma_U) q(l)) \tau(i,j|\gamma_U, l) \\
        &= (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) + \sum_{(j,l) \neq (1, 0)} (p(\gamma_L) q(j) - p(\gamma_U) q(l)) \tau(\gamma_L, j| \gamma_U , l) + \sum_{i \neq \gamma_U, \gamma_L} \sum_{j,l} (p(i) q(j) - p(\gamma_U) q(l)) \tau(i,j | \gamma_U, l) \\
        &\geq (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) + \sum_{(j,l) \neq (1, 0)} (-1) \tau(\gamma_L, j | \gamma_U, l) + \sum_{i \neq \gamma_U, \gamma_L} \sum_{j,l} (-1) \tau(i,j | \gamma_U,l) \\
        &\geq (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) - 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} - (\dim_S - 2) 4 \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} \\
        &\geq (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) - 2 \dim \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
     \end{align}
     Now we note that $\gamma$ is within $\Delta_{\sinc}$ of $\Delta_S(\gamma_U, \gamma_L)$ which allows us to say $1 - \epsilon_{\sinc} \leq \tau(\gamma_U, 0 | \gamma_L, 1) \frac{\dim + 1}{\alpha^2 t^2} \leq 1$ per Corollary \ref{cor:gamma_difference_reqs}. This simplifies the above bound to
     \begin{equation}
        (p(\gamma_L) q(1) - p(\gamma_U)q(0)) \frac{\alpha^2 t^2}{\dim + 1} - (2 \dim + 1) \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1}.
     \end{equation}
     This is rather unfortunate, because it is the wrong term we should analyze first. To see this, as $\beta_E \to \infty$ we have $q(0) \to 1, q(1) \to 0$. This means the above quantity is negative, implying it only contributes to the norm lower bound because of the absolute value. We instead look for the opposite term.

     Writing out the same for $\gamma_L$ yields
     \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l) &\geq \parens{(p(\gamma_U) q(0) - p(\gamma_L) q(1))  - (2 \dim + 1) \epsilon_{\sinc}} \frac{\alpha^2 t^2}{\dim + 1}.
     \end{align}
     Now we see that this should be positive. To do so, we analyze the probability differences in more detail.
     \begin{align}
        p(\gamma_U) q(0) - p(\gamma_L) q(1) &= \frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)} - \frac{e^{-\beta \lambda_S(\gamma_L)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} \\
        &= \frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \parens{1 - e^{-\beta_E \gamma + \beta \Delta_S(\gamma_U, \gamma_L)}}.
     \end{align}
     For now, let $\Delta_S = \Delta_S(\gamma_U, \gamma_L)$ to save space. We show that $\delta \geq \frac{\beta_E \Delta_{\min} + \ln (1 / \epsilon_{exp})}{\Delta_S} \geq \beta_E \frac{\Delta_{\min}}{\Delta_S}$ implies $1 - e^{-\beta_E \gamma + \beta \Delta_S} \geq 1 - \epsilon_{exp}$. Note the assumption $\ln (1/ \epsilon_{exp}) \geq 0$. 
     \begin{align}
        \ln (1 / \epsilon_{exp}) &\leq \delta \Delta_S - \beta_E \Delta_{\min} \\
        \iff \ln (1 / \epsilon_{exp}) &\leq \beta_E (\Delta_S - \Delta_{\min}) - \beta \Delta_S \\
        &\leq \beta_E \gamma - \beta \Delta_S \\
        \iff \epsilon_{exp} &\geq e^{-\beta_E \gamma + \beta \Delta_S} \\
        \iff 1 - e^{-\beta \gamma + \beta \Delta_S}  &\geq 1 - \epsilon_{exp}.
     \end{align}

     Plugging this in, along with a simple substitution that $\epsilon_{exp} = \epsilon_{\sinc}$ yields 
     \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l) &\geq \parens{\frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta) \partfun_E(\beta_E)} (1 - \epsilon_{exp}) - (2 \dim + 1) \epsilon_{\sinc}} \frac{\alpha^2 t^2}{\dim + 1} \\
        &\geq \parens{\frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta) \partfun_E(\beta_E)} - 2 (\dim + 1) \epsilon_{\sinc}} \frac{\alpha^2 t^2}{\dim + 1}
     \end{align}
\end{proof}

Now I want to explore a very simple example to show that analytically this protocol works.
\subsection{Small example}
Let $H_S = U_H \begin{bmatrix} 0 & 0 & 0 \\ 0 & \lambda_A & 0 \\ 0 & 0 & \lambda_B \end{bmatrix} U_H^\dagger$  be a simple three level system with known eigenvalues and unknown eigenvectors. Our goal is to show that this algorithm converges. First we show distance reduction 

\subsubsection{distance reduction}



\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}


\begin{claim}[Cooling Schedule]
    Suppose a nice cooling schedule existed and this algorithm worked perfectly.
\end{claim}
\begin{proof}
    Let $0 = \beta_0 \leq \beta_1 \leq \ldots \leq \beta_L = \beta$ denote a cooling schedule and $$\Phi_{i}(X) = \partrace{\hilb_E}{\int e^{-i (H + \alpha G)t} X \otimes \rho_E(\beta_i) e^{+i(H + \alpha G)t} dG},$$
    where we are simply changing the environment temperature at each step of the schedule with the end goal of preparing a state that approximates $\rho_S(\beta)$. We claim
    \begin{align}
        \norm{\rho_S(\beta) - \Phi_{L} \circ \Phi_{L - 1} \circ \ldots \circ \Phi_{1} (\identity / \dim)}_1 &= \norm{\rho_S(\beta_L) - \Phi_{L} (\rho_S(\beta_{L - 1})) + \Phi_L (\rho_S(\beta_{L-1})) - \Phi_L \circ \ldots \circ \Phi_1 (\rho_S(0))}_1 \\
        &\leq \norm{\rho_S(\beta_L) - \Phi_L (\rho_S(\beta_{L-1}))}_1 + \norm{\Phi_L \parens{\rho_S(\beta_{L-1}) - \Phi_{L-1} \circ \ldots \circ \Phi_1 (\rho_S(0))}}_1 \\
        &\leq \norm{\rho_S(\beta_L) - \Phi_L (\rho_S(\beta_{L-1}))}_1 + \norm{\rho_S(\beta_{L-1}) - \Phi_{L-1} \circ \ldots \circ \Phi_1 (\rho_S(0))}_1 \\
        & \ldots \nonumber \\
        &\leq \sum_{i = 1}^L \norm{\rho_S(\beta_i) - \Phi_i (\rho_S(\beta_{i - 1}))}_1
    \end{align}
\end{proof}

\bibliographystyle{unsrt}
\bibliography{bib}

\appendix



\section{Thermal State Bounds}


\begin{lemma} \label{lem:thermal_state_diff_bound}
    Let $0 \leq \delta = \beta_E - \beta$ and $H_S$ be a Hamiltonian such that $\lambda_S(k) - \lambda_S(i) \geq \Delta_{\min}$ for all $k > i$. The trace distance between the thermal states $\rho_S(\beta)$ and $\rho_S(\beta_E)$ can be bounded from above and below as
    \begin{align}
        \frac{\Delta_{\min} \delta^2 \epsilon_{\rho}}{2 \dim_S} \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 \leq 2 \dim_S \norm{H} \delta,
    \end{align}
    provided that $\beta_E$ satisfies the upper bound $\beta_E \leq \frac{1}{2 \norm{H}} \parens{\ln (\dim_S - 1) + \ln \left( \frac{1}{\epsilon_{\rho}} - 1\right) }$/. 
\end{lemma}
\begin{proof}
    We first decompose the trace distance using the fact that thermal states are diagonal in the $H_S$ basis
    \begin{equation}
        \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 = \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}}. \label{eq:thermal_diffs_1}
    \end{equation}
    This suggests a Taylor Series for the Boltzmann factors $\frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}$ about $\beta = \beta_E$ would be useful. We will only need the zeroth order approximation,
    \begin{equation}
        \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} = \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} + R_0(\beta, \beta_E, k),
    \end{equation}
    and will use the integral form of $R_0$ for the lower bound and the Lagrange or mean-value form for the upper bound. 
    
    As the upper bound is easier we will compute it first. The remainder in this case is given by
    \begin{equation}
        R_0(\beta, \beta_E, k) = (\beta - \beta_E) \frac{d}{d\beta} \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \bigg|_{\beta = \beta_{k}},
    \end{equation}
    where $\beta_{k} \in (\beta, \beta_E)$ is the arbitrary point guaranteed to exist by the Taylor's Theorem. To compute the derivative of the Boltzmann coefficient we first compute the derivative of the partition function
    \begin{align}
        \frac{d}{d\beta} \partfun_S(\beta) &= \frac{d}{d\beta} \sum_i e^{-\beta \lambda_S(i)} \\
        &= - \sum_i \lambda_S(i) e^{-\beta \lambda_S(i)} \\
        &= - \trace{H e^{-\beta H}}.
    \end{align}
    This gives the total derivative of the Boltzmann coefficient is computed as
    \begin{align}
        \frac{d}{d \beta}\frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} &= - \lambda_S(k) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)^2} \frac{d}{d\beta}\partfun_S(\beta) \\
        &= \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \parens{\trace{H \frac{e^{-\beta H}}{\partfun_S(\beta)}} - \lambda_S(k)}. \label{eq:boltzmann_derivative}
    \end{align}
    We can plug this in to Eq. \eqref{eq:thermal_diffs_1} to get
    \begin{align}
        \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 &= \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}} \\
        &= \sum_k \abs{R_0(\beta, \beta_E, k)} \\
        &= \sum_k \abs{\beta - \beta_E} \frac{e^{-\beta_{k} \lambda_S(k)}}{\partfun_S(\beta_{k})} \abs{\trace{\rho_S(\beta_{k})H} - \lambda_S(k)} \\
        &\leq \delta \sum_k \parens{\abs{\sum_{j} \frac{e^{-\beta_k \lambda_S(j)}}{\partfun_S(\beta_k)} \lambda_S(j)} + |\lambda_S(k)|} \\
        &\leq \delta \sum_k \parens{\norm{H} \sum_j \frac{e^{-\beta_k \lambda_S(j)}}{\partfun_S(\beta_k)} + \norm{H}} \\
        &\leq 2 \delta \dim_S \norm{H}.
    \end{align}
    This gives the upper bound required in the lemma statement.

    We now move on to the lower bound. We use the same breakdown in the eigenbasis of $H_S$, but this time we drop all terms that are not the ground state 
    \begin{align}
        \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 &= \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}} \\
        &\geq \abs{\frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)}} \\
        &= \abs{R_0(\beta, \beta_E, 0)} \\
        &\geq R_0(\beta, \beta_E, 0),
    \end{align}
    and we will stop writing the implicit argument of 0 at the end from now on. 
    
    We drop the higher order contributions for a few reasons. The first being that thermal states are typically used as close approximations to ground states, which approaches the exact ground state as $\beta_E \to \infty$, and we are imagining a scenario in which $\beta$ is somewhat close to $\beta_E$. In this situation, the dominant contribution to the trace distance comes from the difference in the ground state Boltzmann factors. The other rationale behind just studying the ground state contribution is the fact that $\beta_1 \leq \beta_2$ implies that $\frac{e^{-\beta_1 \lambda_S(0)}}{\partfun_S(\beta_1)} \leq \frac{e^{-\beta_2 \lambda_S(0)}}{\partfun_S(\beta_2)}$, or that the overlap that the thermal state has with the ground state, $\trace{\Pi_0 \rho_S(\beta)}$, is monotonically increasing with $\beta$. This is shown using a straightforward lower bound on the first derivative computed in Eq. \eqref{eq:boltzmann_derivative}:
    \begin{align}
        \frac{d}{d\beta} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} &= \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{ \sum_k \left(\lambda_S(k) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \right) - \lambda_S(0) } \\
        &= \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{ \sum_k \left(\lambda_S(k) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \right) - \lambda_S(0) \sum_k \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(k)} } \\
        &= \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \sum_k (\lambda_S(k) - \lambda_S(0)) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}  \\
        &\geq \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \Delta_{\min} \sum_{k > 0} \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(k)} \\
        &= \Delta_{\min} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{1 - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)}}, \label{eq:boltzmann_derivative_lower_bound}
    \end{align}
    where we see that this is always positive for finite $\beta$ and 0 in the limit $\beta \to \infty$.  
    
    To compute this lower bound we will use the integral version of the remainder $R_0$, which is given as 
    \begin{align}
        R_0(\beta, \beta_E) = \int_{\beta}^{\beta_E} (x - \beta) \frac{d}{d\beta} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)}\bigg|_{\beta = x} dx ~.
    \end{align}
    Plugging in the derivative computed in Eq. \eqref{eq:boltzmann_derivative} gives
    \begin{align}
        R_0(\beta, \beta_E) = \int_{\beta}^{\beta_E} (x - \beta) \frac{e^{-x \lambda_S(0)}}{\partfun_S(x)}\parens{\trace{H \frac{e^{- x H}}{\partfun_S(x)}} - \lambda_S(0)} dx,
    \end{align}
    where we note that this is positive as $x > \beta$ and the derivative is always positive, as noted above. Plugging in the lower bound computed in Eq. \eqref{eq:boltzmann_derivative_lower_bound} yields
    \begin{align}
        R_0(\beta, \beta_E) &\geq \Delta_{\min} \int_{\beta}^{\beta_E} (x - \beta) \frac{e^{-x \lambda_S(0)}}{\partfun_S(x)} \parens{1 - \frac{e^{-x \lambda_S(0)}}{\partfun_S(x)}} dx \\
        &\geq \Delta_{\min} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{1 - \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)}} \int_{\beta}^{\beta_E} (x - \beta) dx \\
        &= \frac{\Delta_{\min} (\beta_E - \beta)^2}{2} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{1 - \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)}}. \label{eq:boltzmann_lower_bound_penultimate}
    \end{align}

    The last step is to lower bound the the Boltzmann factors as seen above. We can use the simplistic bound $\frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \geq \frac{1}{\dim_S}$ to take care of the one factor, and the other we need to show a bound of $\parens{1 - \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E}} \geq \epsilon_{\rho}$
    This will be done through a bound on the partition function, seen below
    \begin{align}
        \partfun_S(\beta_E) &=  \sum_k e^{-\beta_E \lambda_S(k)} = e^{-\beta_E \lambda_S(0)} \parens{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))}}.
    \end{align}
    Rearranging yields
    \begin{align}
        \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} &= \frac{1}{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))}}.
    \end{align}
    To upper bound this quantity we use fact that $\lambda_S(k) - \lambda_S(0) \leq 2 \norm{H}$ holds for all $k$, which yields
    \begin{align}
        -\beta_E(\lambda_S(k) - \lambda_S(0)) &\geq -\beta_E 2 \norm{H} \\
        \implies e^{-\beta_E (\lambda_S(k) - \lambda_S(0))} &\geq e^{-\beta_E 2 \norm{H}} \\
        \implies \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))} &\geq (\dim_S - 1) e^{-\beta_E 2 \norm{H}} \\
        \implies \frac{1}{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))} } &\leq \frac{1} {1 + (\dim_S - 1) e^{-\beta_E 2 \norm{H}} } .
    \end{align}
    Now this serves as an upper bound on $\frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)}$. We would therefore like that 
    \begin{align}
        \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} = \frac{1}{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))}} &\leq \frac{1}{1 + (\dim_S - 1) e^{-\beta_E 2 \norm{H}}} \\
        &\leq 1 - \epsilon_{\rho} \label{eq:epsilon_rho_inequality}
    \end{align}
    Through straightforward algebra, it can be seen that requiring 
    \begin{equation}
        \beta_E \leq \frac{1}{2 \norm{H}} \parens{\ln (\dim_S - 1) + \ln \left( \frac{1}{\epsilon_{\rho}} - 1\right) } \label{eq:beta_e_upper_bound_thermal_diffs}
    \end{equation}
    is sufficient to satisfy the above inequality in Eq. \eqref{eq:epsilon_rho_inequality}. Plugging Eq. \eqref{eq:epsilon_rho_inequality}, along with the inequality $\frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \geq \frac{1}{\dim_S}$, into Eq. \eqref{eq:boltzmann_lower_bound_penultimate} yields
    \begin{align}
        R_0(\beta, \beta_E) \geq \frac{\epsilon_{\rho} \Delta_{\min} \delta^2}{2 \dim_S},
    \end{align}
    which gives the lemma statement. 

    \begin{lemma}
        Let $H_S$ be the truncated harmonic oscillator Hamiltonian. We will compute a Taylor's series for the Boltzmann distribution at $\beta$ about an environment temperature of $\beta_E$. The difference between these inverse temperatures is denoted $\delta = \beta_E - \beta$
        \begin{align}
            \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} &= \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \delta \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} \parens{\trace{H_S \frac{e^{-\beta_E H_S}}{\partfun_S(\beta_E)}} - \lambda_S(k)} + R_{\delta}
        \end{align}
        where we bound $|R_{\delta}| \in \bigo{\delta^2 \dim_S^2}$.
    \end{lemma}
    now what we need from this lemma is when is this remainder positive or negative. when is
    \begin{align}
        \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} &\geq 0 \\
        \delta \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} \parens{\trace{H_S \frac{e^{-\beta_E H_S}}{\partfun_S(\beta_E)}} - \lambda_S(k)} &\geq R_{\delta} \\
        \trace{H_S \frac{e^{-\beta_E H_S}}{\partfun_S(\beta_E)}} - \frac{R_{\delta}}{\delta p_{\beta_E}(k)} &\geq k.
    \end{align}
    Now we can plug in for the Harmonic oscillator to solve for the average energy at a given environment temperature $\beta_E$. To do so we need an expression for the partition function, which can be computed analytically for this system:
    \begin{equation}
        \partfun_S(\beta) = \trace{e^{-\beta H_S}} = \sum_{i = 1}^{\dim_S} e^{-\beta i} = \frac{e^{-\beta} (1 - e^{-\beta \dim_S})}{1 - e^{-\beta}}.
    \end{equation}
    Now we can use this to compute the average energy of a Gibbs state using the derivative of the partition function
    \begin{align}
        \trace{H_S e^{-\beta H_S}} &= - \frac{\partial}{\partial \beta} \partfun_S(\beta) \\
        &= - \frac{\partial}{\partial \beta} \frac{e^{-\beta} (1 - e^{-\beta \dim_S})}{1 - e^{-\beta}} \\
        &= \frac{1}{(e^{\beta} - 1)^2} - e^{-\beta \dim_S} \parens{\frac{\dim_S}{e^{\beta} - 1} + \frac{1}{(e^{\beta} - 1)^2}}.
    \end{align}
    Now dividing by the partition function yields the expected energy
    \begin{align}
        \trace{H_S \frac{e^{-\beta H_S}}{\partfun_S(\beta )}} &= \left( \frac{1 - e^{-\beta \dim_S}}{e^{\beta} - 1} \right)^{-1} \parens{\frac{1}{(e^{\beta} - 1)^2} - e^{-\beta \dim_S} \parens{\frac{\dim_S}{e^{\beta} - 1} + \frac{1}{(e^{\beta} - 1)^2}}} \\
        &= \frac{1}{(e^{\beta} - 1)(1 - e^{-\beta \dim_S} )} - \frac{e^{-\beta \dim_S}}{1 - e^{-\beta \dim_S}} \parens{\dim_S + \frac{1}{e^{\beta} - 1}} \\
        &= \frac{1}{e^{\beta} - 1} - \frac{\dim_S}{e^{\beta \dim_S} - 1}.
    \end{align}
    Now this expression clearly fails in the large $\beta$ limit, as it approaches $e^{-\beta}$ instead of 1, but we might be able to get away with it.
\end{proof}



\section{Some Scratch}
\begin{lemma} \label{lem:expected_second_order_term}
    Let $T_{\gamma}^{(2)}(\rho_S(\beta))$ denote the second order term for a thermalizing channel, with a fixed value of $\gamma$, as defined in Eq. \eqref{def:taylor_series_terms}. We compute the expectation value of the $k$-th component of this matrix with respect to $\gamma$ as chosen from the perfect knowledge distribution of a well-separated Hamiltonian $H_S$, per Definition \ref{def:separated_hamiltonians}, as 
    \begin{align}
        \abs{\mathbb{E}_{\gamma} \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} - \frac{\alpha^2 t^2}{\dim + 1} \frac{2}{\dim_S (\dim_S - 1)} \sum_{i \neq k} \parens{ p(i) \frac{1}{1 + e^{-\beta_E |\Delta_S(i,k)|}}  - p(k) \frac{e^{-\beta_E |\Delta_S(i,k)|}}{1 + e^{-\beta_E |\Delta_S(i,k)|}} }} \leq  \parens{\frac{4 \alpha}{\Delta_{\min}}}^2
    \end{align}
\end{lemma}
\begin{proof}
    We start by decomposing $T_{\gamma}^{(2)}$ for a \emph{fixed} value of $\gamma$, using the results of Lemma \ref{lem:transition_idx_sub}
    \begin{align}
        \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} &= \sum_{i,j,l} p(i) q(j) \tau(i,j|k,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) - p(k) q(l)) \tau(i,j| k,l) \\
        &= \frac{\alpha^2 t^2}{\dim + 1} \sum_{i \neq k} \sum_{j,l} \parens{ p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2).
    \end{align}
    To tackle the sum over $i$, we note that $\Delta_S(i,k)$ is positive if $i > k$ and negative if $i < k$, due to the sorting of eigenvalues. We will first show the $i > k$ condition and the other case is the same argument.

    We expand the $j,l$ summation and note that 3 of the resulting terms are in the tail end of the sinc function and can therefore be upper bounded by $\epsilon_{\sinc}$, as $\Delta_S(i,k) \geq \Delta_{\min}$
    \begin{align}
        & \sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &= \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)}} \sinc^2(\Delta_S(i,k)t/2) \nonumber \\
        &\quad + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2) \nonumber \\
        &\quad + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \gamma) t/2) \nonumber \\
        &\quad + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k)) t/2) \label{tmp:expected_second_order_coeff_1} \\
        &\leq 1 \cdot \sinc^2(\Delta_S(i,k) t/2) + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2) \nonumber \\
        &\quad + 1 \cdot \sinc^2((\Delta_S(i,k) + \gamma)t/2) + 1 \cdot \sinc^2(\Delta_S(i,k) t/2) \\
        &\leq 3 \epsilon_{\sinc} + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2).
    \end{align}
    Similar arguments show that Eq. \eqref{tmp:expected_second_order_coeff_1} is lower bounded by $-3 \epsilon_{\sinc} + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2)$. 

    Now we look at the case in which $i < k$. We have the same setup in Eq. \eqref{tmp:expected_second_order_coeff_1}, but $\Delta_S(i,k) \leq -\Delta_{\min}$. This gives virtually the same result as above, but the term that is not small is the term with $+ \gamma$ in the argument of the sinc function as opposed to the $-\gamma$ term. So for $i < k$ we have
    \begin{align}
        & \sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &\leq 3 \epsilon_{\sinc} + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)} } \sinc^2((\Delta_S(i,k) + \gamma) t/2) \\
        &= 3 \epsilon_{\sinc} + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)} } \sinc^2((|\Delta_S(i,k)| - \gamma) t/2).
    \end{align}
    Notice how the factor of $e^{-\beta_E \gamma}$ switched in the $i < k$ case from the $i > k$ one. Similar arguments give the same lower bound as before. 

    Now we introduce the expectation over $\gamma$, as expectations are linear we can do this term by term over the sum on $i \neq k$ in Eq. idk. As $H_S$ is well-separated, and $\gamma$ is distributed assuming perfect knowledge, then for each $i \neq k$ we will sample $\gamma = \Delta_S(i,k)$ exactly and every other term will be $\gamma \sim \Delta_S(a,b)$ for $(a,b) \neq (i,k)$. These other terms leave the resulting sinc function to be $\epsilon_{\sinc}$ small. We again will analyze this expectation first for terms in which $i > k$ and then show the $i < k$ case. \matt{This is where we need to tackle the $H_S$ well-separated condition.}

    \begin{align}
        & \mathbb{E}_{\gamma}\sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &\leq \mathbb{E}_{\gamma} 3 \epsilon_{\sinc} + \mathbb{E}_{\gamma} \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2) \\
        &= 3 \epsilon_{\sinc} + \sum_{(a,b) : a > b} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(a,b)}} - p(k) \frac{e^{-\beta_E \Delta_S(a,b)}}{1 + e^{-\beta_E \Delta_S(a,b)}}} \sinc^2((\Delta_S(i,k) - \Delta_S(a,b)) t/2) \\
        &= 3 \epsilon_{\sinc} \nonumber \\
        &+ \sum_{(a,b): a > b, (a,b) \neq (i,k)} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(a,b)}} - p(k) \frac{e^{-\beta_E \Delta_S(a,b)}}{1 + e^{-\beta_E \Delta_S(a,b)}}} \sinc^2((\Delta_S(i,k) - \Delta_S(a,b)) t/2) \nonumber \\
        &+ \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}}} \sinc^2((\Delta_S(i,k) - \Delta_S(i,k)) t/2) \\
        &\leq 3 \epsilon_{\sinc} + \epsilon_{\sinc} \frac{2}{\dim_S (\dim_S - 1)} \sum_{(a,b): a > b, (a,b) \neq (i,k)} 1 \nonumber \\
        &\quad + \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}}} \\
        &\leq 4 \epsilon_{\sinc} + \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}}}.
    \end{align}
    Similar lower bounds simply swap the sign of $4 \epsilon_{\sinc}$ to $- 4 \epsilon_{\sinc}$. As with the above, in the situation with $i < k$ we simply move the $e^{-\beta_E \gamma}$ factor, as the sinc function evaluates to 1 in term in the expectation that actually contributes
    \begin{align}
        & \mathbb{E}_{\gamma}\sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &\leq 4 \epsilon_{\sinc} + \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}}}.
    \end{align}
    This then leads to the bounds for the expectation value of $T_{\gamma}^{(2)}$ as 
    \begin{align}
        \mathbb{E}_{\gamma} \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} &\leq \frac{\alpha^2 t^2}{\dim + 1} \sum_{i \neq k} 4 \epsilon_{\sinc} \nonumber \\
        &\quad + \frac{\alpha^2 t^2}{\dim + 1} \sum_{i < k} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}}} \nonumber \\
        &\quad + \frac{\alpha^2 t^2}{\dim + 1} \sum_{i > k} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}} } \\
        &\leq 4 \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} \frac{2}{\dim_S (\dim_S - 1)} \sum_{i \neq k} \parens{ p(i) \frac{1}{1 + e^{-\beta_E |\Delta_S(i,k)|}}  - p(k) \frac{e^{-\beta_E |\Delta_S(i,k)|}}{1 + e^{-\beta_E |\Delta_S(i,k)|}} }.
    \end{align}
    The exact same lower bound but with $4 \alpha^2 t^2 \epsilon_{\sinc}$ goes to $-4\alpha^2 t^2 \epsilon_{\sinc}$ holds. This, along with $\epsilon_{\sinc} = \frac{4}{t^2 \Delta_{\min}^2}$, gives the final result
    \begin{align}
        \abs{\mathbb{E}_{\gamma} \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} - \frac{\alpha^2 t^2}{\dim + 1} \frac{2}{\dim_S (\dim_S - 1)} \sum_{i \neq k} \parens{ p(i) \frac{1}{1 + e^{-\beta_E |\Delta_S(i,k)|}}  - p(k) \frac{e^{-\beta_E |\Delta_S(i,k)|}}{1 + e^{-\beta_E |\Delta_S(i,k)|}} }} \leq  \parens{\frac{4 \alpha}{\Delta_{\min}}}^2
    \end{align}
\end{proof}

\section{Haar Integral Proofs} \label{sec:haar_integral_appendix}

In this section we present the more technical work needed to state our results in Section \ref{sec:taylor_series_phi}. The first two results are Lemmas that compute the effects of the randomized interaction in a form that are usable in the main result of Theorem \ref{lem:big_one}.

\begin{lemma} \label{lem:two_heisenberg_interactions}
    Let $G(t)$ denote the Heisenberg evolved random interaction $G(t) = e^{iHt} G e^{-iHt}$ for a total Hamiltonian $H$. After averaging over the interaction measure the product $G(x) G(y)$ can be computed as
    \begin{equation}
        \int G(x) G(y) dG = \frac{1}{\dim + 1} \parens{\sum_{(i,j),(k,l)} e^{i \Delta(i,j|k,l) (x-y)} \ketbra{i,j}{i,j} + \identity}.
    \end{equation}
\end{lemma}
\begin{proof}
The overall structure of this proof is to evaluate the product in the Hamiltonian eigenbasis and split the product into three factors: a phase contribution from the time evolution, a Haar integral from the eigenvalues of the random interaction, and the eigenvalue contribution of the random interaction. Since this involves the use of multiple indices, it will greatly simplify the proof to use a single index over the total Hilbert space $\hilb$ as opposed to two indices over $\hilb_S \otimes \hilb_E$. For example, the index $a$ should be thought of as a pair $(a_s, a_e)$, and functions $\lambda(a)$ should be thought of as $\lambda(a_s, a_e)$. Once the final form of the expression is reached we will substitute in pairs of indices for easier use of the lemma in other places.
    \begin{align}
        \int G(x) G(y) dG &= \int e^{+i H x} U_G D U_G^\dagger e^{-i H x} e^{+i H y} U_G D U_G^\dagger e^{-i H y} dU_G ~dD \\
        &= \int \bigg[\sum_a e^{+i \lambda(a)x}\ketbra{a}{a}  U_G \sum_b D(b)\ketbra{b}{b} U_G^\dagger \nonumber \\
        &\quad \sum_c e^{-i \lambda(c) (x - y)} \ketbra{c}{c} U_G \sum_d D(d)\ketbra{d}{d} U_G^\dagger \sum_e e^{-i \lambda(e) y} \ketbra{e}{e} \bigg] dU_G ~dD\\
        &=\sum_{a,b,c,d,e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \nonumber \\
        &\quad \times \int \bra{a} U_G \ket{b} \bra{c} U_G \ket{d} \bra{b} U_G^{\dagger} \ket{c} \bra{d} U_G^\dagger \ket{e} dU_G \int D(b) D(d) dD \\
        &=  \sum_{a, b, c, d, e} \delta_{bd} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \nonumber \\
        &\quad \times \int \bra{a} U_G \ket{b} \bra{c} U_G \ket{d} \bra{b} U_G^{\dagger} \ket{c} \bra{d} U_G^\dagger \ket{e} dU_G. \\
    \end{align}
    Now the summation over $d$ fixes $d=b$ and we use Lemma \ref{lem:haar_two_moment} to compute the Haar integral, which simplifies greatly due to the repeated $b$ index. Plugging the result into the above yields the following
    \begin{align}
        &= \frac{1}{\dim^2 - 1} \sum_{a, b, c, e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \parens{\delta_{ac} \delta_{ce} + \delta_{ae} - \frac{1}{\dim} \parens{\delta_{ac} \delta_{ce} + \delta_{ae}}}  \\
        &= \frac{1}{\dim^2 - 1} \parens{1 - \frac{1}{\dim}} \sum_{a, b, c, e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \delta_{ae} (1 + \delta_{ac}) \\
        &= \frac{1}{\dim^2 - 1} \parens{1 - \frac{1}{\dim}} \sum_{a, b, c} \ketbra{a}{a} e^{i (\lambda(a) - \lambda(c))(x-y)} (1 + \delta_{ac}) \\
        &= \frac{1 \parens{\dim - 1}}{\dim^2 - 1} \sum_{a,c} \ketbra{a}{a} e^{i (\lambda(a) - \lambda(c))(x - y)} (1 + \delta_{ac}) \\
        &= \frac{1}{\dim + 1} \parens{\sum_{a,c} e^{i (\lambda(a) - \lambda(c))(x-y)} \ketbra{a}{a} + \identity}.
    \end{align}
    Reindexing by $a \mapsto i,j$, $c \mapsto k,l$, and plugging in the definition of $\Delta$ yields the statement of the lemma.
\end{proof}


\begin{lemma} \label{lem:sandwiched_interaction}
    Given two Heisenberg evolved random interactions, $G(x)$ and $G(y)$, we compute their action on an outer-product $\ketbra{i,j}{k,l}$ as
    \begin{equation}
        \int G(x) \ketbra{i,j}{k,l} G(y) ~dG = \frac{1}{\dim + 1} \parens{\ketbra{i,j}{k,l} + \delta(i,j|k,l) \sum_{m,n} e^{i \Delta(m,n | i,j) (x-y)} \ketbra{m,n}{m,n}}
    \end{equation}
\end{lemma}
\begin{proof}
This proof is structured the same as Lemma \ref{lem:two_heisenberg_interactions} and similarly we will use a single index of the total Hilbert space $\hilb$ and switch to two indices to match the rest of the exposition.
    \begin{align}
        \int G(x) \ketbra{a}{b} G(y) dG &=  \int e^{i H x} U_G D U_G^{\dagger} e^{-i H x} \ketbra{a}{b} e^{i H y} U_G D U_G^\dagger e^{-i H y} ~dG \\
        &= \sum_{c, d, e, f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \nonumber \\
        &\quad \times \int \ketbra{c}{c} U_G D(d) \ketbra{d}{d} U_G^\dagger \ketbra{a}{b} U_G D(e) \ketbra{e}{e} U_G^\dagger \ketbra{f}{f} dG \\
        &= \sum_{c, d, e, f}  e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} \nonumber \\
        &\quad \times \int D(d) D(e) dD \int \bra{c} U_G \ket{d} \bra{b} U_G \ket{e} \bra{d} U_G^\dagger \ket{a} \bra{e} U_G^\dagger \ket{f} dU_G \\
        &=  \sum_{c,d,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} \nonumber \\ 
        &\quad \times \int \bra{c} U_G \ket{d} \bra{b} U_G \ket{d} \bra{a} \overline{U_G} \ket{d} \bra{f} \overline{U_G} \ket{d} dU_G \\
        &= \frac{1}{\dim^2 - 1} \sum_{c,d,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} (\delta_{ca} \delta_{bf} + \delta_{cf}\delta_{ab})\parens{1 - \frac{1}{\dim}} \\
        &= \frac{1}{\dim + 1} \sum_{c,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} (\delta_{ca} \delta_{bf} + \delta_{cf}\delta_{ab}) \\
        &= \frac{1}{\dim + 1} \parens{\ketbra{a}{b} + \delta_{ab} \sum_{c} e^{i(\lambda(c) - \lambda(a))(x-y)} \ketbra{c}{c} }.
    \end{align}
    Now re-indexing by $a \mapsto (i,j)$, $b \mapsto (k,l)$ and $c \mapsto (m,n)$ results in the expression given in the statement of the lemma.
\end{proof}


\secondOrderChannelHaar*
\begin{proof}
To start we would like to note that we will use a single index notation to refer to the joint system-environment eigenbasis during this proof to help shorten the already lengthy expressions. We will convert back to a double index notation to match the statement of the theorem. We start from the expression for the first derivative of the channel $\frac{\partial}{\partial \alpha} \Phi_G(\rho_S)$ given by Eq. \eqref{eq:first_order_alpha_derivative}. To take the second derivative there are six factors involving $\alpha$, so we will end up with six terms. We repeat Eq. \eqref{eq:first_order_alpha_derivative} below, add a derivative, and label each factor containing an $\alpha$ for easier computation
\begin{align}
    \frac{\partial^2}{\partial \alpha^2} \Phi_G(\rho_S) =& \frac{\partial}{\partial \alpha} \parens{\int_{0}^{1} \underset{\substack{\downarrow \\ (A)}}{e^{i s (H+\alpha G)t}} (i t G) \underset{\substack{\downarrow \\ (B)}}{e^{i (1-s) (H+\alpha G)t}} ds ~ \rho \underset{\substack{\downarrow \\ (C)}}{e^{-i(H+\alpha G)t}} } \nonumber \\
    &~ ~+\frac{\partial}{\partial \alpha} \parens{ \underset{\substack{\downarrow \\ (D)} }{e^{i(H+\alpha G)t}} \rho \int_{0}^1 \underset{\substack{\downarrow \\ (E)} }{e^{-i s (H+\alpha G) t} } (- i t G) \underset{\substack{\downarrow \\ (F)}}{e^{-i (1-s) (H+\alpha G)t}} ds }.
\end{align}
Our goal is to get each of these terms in a form in which we can use either Lemma \ref{lem:two_heisenberg_interactions} or \ref{lem:sandwiched_interaction}. 
\begin{align}
    (A) &=i t\int_0^1 \parens{\frac{\partial}{\partial \alpha} e^{i s_1 (H+ \alpha G)t}} G e^{i(1-s_1)(H+\alpha G)t} ds_1 \rho e^{-i (H+\alpha G)t} \bigg|_{\alpha=0} \\
    &= (it)^2 \int_0^1 \parens{\int_0^1 e^{i s_1 s_2 (H+\alpha G)t} s_1 G e^{i s_1 (1-s_2) (H+\alpha G)t} ds_2} G e^{i(1-s_1) (H+\alpha G)t} ds_1 \rho e^{-i(H+\alpha G) t} \bigg|_{\alpha=0} \label{eq:second_order_deriv_intermediate_a}\\
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 s_2 H t} G e^{-i s_1 s_2 H t} e^{i s_1 H t} G e^{-i s_1 H t} s_1 ds_1 ds_2 e^{i H t} \rho e^{-i H t} \\
    &= -t^2 \int_0^1 \int_0^1 G(s_1 s_2 t) G(s_1 t) s_1 ds_1 ds_2 \rho(t). \label{eq:second_deriv_alpha_first_term}
\end{align}

\begin{align}
    (B) &= it \int_0^1 e^{i s_1 (H + \alpha G)t} G \frac{\partial}{\partial \alpha}\parens{e^{i(1-s_1)(H + \alpha G)t}} ds_1 \rho e^{-i(H + \alpha G) t} \bigg|_{\alpha = 0} \\
    &= (it)^2 \int_0^1 e^{i s_1 (H + \alpha G)t} G \parens{\int_0^1 e^{i(1-s_1)s_2 (H + \alpha G)t} (1-s_1) G e^{i(1 - s_1)(1 - s_2)(H + \alpha G)t} ds_2} ds_1 ~ \rho e^{-i ( H + \alpha G)t} \bigg|_{\alpha = 0} \\
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 H t} G e^{i(1-s_1)s_2 H t} G e^{i(1-s_1)(1-s_2) H t} (1-s_1) ds_1 ds_2 ~ \rho e^{-i H t}\\ 
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 H t} G e^{-i s_1 H t} e^{i(s_1 + s_2 - s_1 s_2) H t} G e^{-i (s_1 + s_2 - s_1 s_2) H t} (1-s_1) ds_1 ds_2 ~ \rho(t) \\
    &= -t^2 \int_0^1 \int_0^1 G(s_1 t) G((s_1 + s_2 - s_1 s_2)t) (1-s_1) ds_1 ds_2 ~ \rho(t)
\end{align}

\begin{align}
    (C) &= it \int_0^1 e^{i s (H + \alpha G)t} G e^{i(1-s) (H + \alpha G) t} ds ~\rho ~ \frac{\partial}{\partial \alpha} \parens{ e^{-i (H + \alpha G) t} } \bigg|_{\alpha = 0} \\
    &= (i t) (-it) \int_0^1 e^{i s (H + \alpha G)t} G e^{i (1 - s) (H + \alpha G)t} ds ~ \rho ~ \parens{ \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1- s) ( H + \alpha G)t } ds}\bigg|_{\alpha = 0} \\
    &= + t^2 \parens{\int_0^1 e^{i s H t} G e^{-i s H t} ds} e^{i H t} \rho e^{-i H t} \parens{\int_0^1 e^{i (1-s) H t} G e^{-i (1-s) H t} ds} \\
    &= + t^2 \int_0^1 G(st) ds ~ \rho(t) \int_0^1 G((1-s)t) ds
\end{align}

\begin{align}
    (D) &= (-it) \frac{\partial}{\partial \alpha} \parens{e^{i(H + \alpha G)t}} \rho \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1-s)(H + \alpha G)t} ds \bigg|_{\alpha = 0} \\
    &= t^2 \parens{\int_0^1 e^{i s (H+ \alpha G)t} G e^{i (1-s) (H + \alpha G)t}ds} \rho \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1-s)(H + \alpha G)t} ds \bigg|_{\alpha = 0} \\
    &=  t^2 \int_0^1 e^{i s H t} G e^{-i s H t} ds ~\rho(t) \int_0^1 e^{i (1-s) H t} G e^{-i (1-s) H t} ds \\
    &= t^2 \int_0^1 G(st) ds ~ \rho(t) ~ \int_0^1 G((1-s)t) ds
\end{align}

\begin{align}
    (E) &= (-it) e^{i (H+ \alpha G) t} ~ \rho ~\int_0^1 \frac{\partial}{\partial \alpha} \parens{e^{-i s_1 (H + \alpha G)t}} G e^{-i (1-s_1)(H + \alpha G)t} ds_1 \bigg|_{\alpha = 0} \\
    &= - t^2 e^{i(H + \alpha G)t} ~ \rho ~\int_0^1 \parens{\int_0^1 e^{-i s_1 s_2 (H + \alpha G) t} (s_1 G) e^{-i s_1 (1-s_2) (H + \alpha G)t} ds_2} G e^{-i(1-s_1)(H + \alpha G)t} ds_1 \bigg|_{\alpha = 0} \\
    &= -t^2 e^{i H t} \rho e^{-i H t} \int_0^1 \int_0^1 e^{i (1 - s_1 s_2) H t} G e^{-i (s_1 - s_1 s_2)H t} G e^{-i (1-s_1)H t} s_1 ds_1 ds_2 \\
    &= -t^2 \rho(t) \int_0^1 \int_0^1 G((1- s_1 s_2) t) G((1-s_1)t) s_1 ds_1 ds_2
\end{align}

\begin{align}
    (F) &= (-it) e^{i(H + \alpha G) t} \rho \int_0^1 e^{-i s_1 ( H + \alpha G) t} G \frac{\partial}{\partial \alpha} \parens{ e^{-i (1-s_1) ( H +\alpha G)t}} ds_1 \bigg|_{\alpha = 0} \\
    &= (-it)^2 e^{i (H + \alpha G)t} \rho \int_0^1 e^{-i s_1 (H + \alpha G)t} G \parens{\int_0^1 e^{-i(1-s_1) s_2 (H + \alpha G)t} (1-s_1) G e^{-i(1-s_1) (1-s_2) (H + \alpha G) t} ds_2} ds_1 \bigg|_{\alpha = 0} \\
    &= -t^2 e^{-i H t} \rho e^{-i H t} \int_0^1 \int_0^1 e^{i (1- s_1) H t} G e^{-i (1-s_1) H t} e^{i (1-s_1)(1-s_2) H t} G e^{-i(1-s_1)(1-s_2) H t} (1-s_1) ds_1 ds_2 \\
    &= -t^2 \rho(t) \int_0^1 \int_0^1 G((1-s_1)t) G((1-s_1)(1 - s_2) t) (1-s_1)ds_1 ds_2
\end{align}

Now our goal is to compute the effects of averaging over the interaction $G$ on the above terms, starting with $(A)$. As this involves a lot of index manipulations, similarly to the proofs of Lemmas \ref{lem:two_heisenberg_interactions} and \ref{lem:sandwiched_interaction} we will use a single index for the total system-environment Hilbert space and switch back to a double index to state the results. We will make heavy use of Lemma \ref{lem:two_heisenberg_interactions}.
\begin{align}
    \int (A) dG &= -t^2 \int_0^1 \int_0^1 \int G(s_1 s_2 t) G(s_1 t) dG s_1 ds_1 ds_2 \rho(t) \\
    &= \frac{-t^2 }{\dim + 1} \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i (\lambda(i) - \lambda(j)) (s_1 s_2 t - s_1 t)} \ketbra{i}{i} + \identity} s_1 ds_1 ds_2 \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \int_0^1 \int_0^1 e^{i(\lambda(i) - \lambda(j))t (s_1 s_2 - s_1)} s_1 ds_1 ds_2 \ketbra{i}{i} + \sum_{i} \sum_{j : \lambda(i) = \lambda(j)}\frac{1}{2} \ketbra{i}{i} + \frac{1}{2} \identity} \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 - i (\lambda(i) - \lambda(j))t - e^{-i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i} } \rho(t) \\
    &= \frac{- 1}{\dim + 1}\parens{\sum_{i} \sum_{j: \Delta_{ij} \neq 0} \frac{1 - i \Delta_{ij}t - e^{-i \Delta_{ij} t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2} \sum_{i} (\eta(i) + 1)\ketbra{i}{i} } \rho(t)
\end{align}

We can similarly compute the averaged $(B)$ term:
\begin{align}
    \int (B) dG &= -t^2 \int_0^1 \int_0^1 \int G(s_1 t) G((s_1 + s_2 - s_1 s_2) t) dG (1-s_1) ds_1 ds_2 ~ \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i (\lambda(i) - \lambda(j))(s_1 s_2 - s_2) t} \ketbra{i}{i} + \identity} (1 -s_1) ds_1 ds_2 \rho \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \int_0^1 \int_0^1 e^{i(\lambda(i) - \lambda(j))t (s_1 s_2 - s_2)} (1 - s_1) ds_1 ds_2 \ketbra{i}{i} + \sum_{i} \sum_{j : \lambda(i) = \lambda(j)}\frac{1}{2} \ketbra{i}{i} + \frac{1}{2} \identity} \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 - i (\lambda(i) - \lambda(j))t - e^{-i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i} } \rho(t) \\
    &= \frac{-1}{\dim + 1}\parens{\sum_{i} \sum_{j: \Delta_{ij} \neq 0} \frac{1 - i \Delta_{ij}t - e^{-i \Delta_{ij} t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2} \sum_{i} (\eta(i) + 1)\ketbra{i}{i} } \rho(t),
\end{align}
which we note is identical to $\int (A) dG$. As terms $(C)$ and $(D)$ involve a different method of computation we skip them for now and compute $(E)$ and $(F)$. 
\begin{align}
    \int (E) dG &= -t^2 \rho(t) \int_0^1 \int_0^1 \int G((1- s_1 s_2) t) G((1-s_1)t) dG s_1 ds_1 ds_2 \\
    &= \frac{- t^2}{\dim + 1} \rho(t) \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i(\lambda(i) - \lambda(j)) t (s_1 - s_1 s_2)} \ketbra{i}{i} + \identity } s_1 ds_1 ds_2 \\
    &= \frac{- t^2}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 + i (\lambda(i) - \lambda(j))t - e^{i(\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2}\ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1 )\ketbra{i}{i}} \\
    &= \frac{- 1}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j: (\Delta_{ij} \neq 0)} \frac{1 + i \Delta_{ij}t - e^{i\Delta_{ij}t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2}\sum_i (\eta(i) + 1) \ketbra{i}{i}}.
\end{align}
Computing $(F)$ yields
\begin{align}
    \int (F) dG &= -t^2 \rho(t) \int_0^1 \int_0^1 \int G((1-s_1)t) G((1-s_1)(1 - s_2) t) dG (1-s_1)ds_1 ds_2 \\
    &= \frac{- t^2 \sigma ^2}{\dim + 1} \rho(t) \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i(\lambda(i) - \lambda(j))t (s_2 - s_1 s_2)}\ketbra{i}{i} + \identity} (1-s_1) ds_1 ds_2 \\
    &= \frac{- t^2 }{\dim + 1} \rho(t) \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 + i (\lambda(i) - \lambda(j))t - e^{i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} +\frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i}} \\
    &= \frac{- 1}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j: (\Delta_{ij} \neq 0)} \frac{1 + i \Delta_{ij}t - e^{i\Delta_{ij}t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2}\sum_i (\eta(i) + 1) \ketbra{i}{i}}
\end{align}
 which is identical to $\int (E) dG$.

 The last two terms $(C) = (D)$ are computed as follows:
 \begin{align}
     \int (C) dG &= t^2 \int_0^1 \int_0^1 \int G(s_1 t) \rho(t) G((1-s_2)t) ~dG ~ ds_1 ds_2 \\
     &= t^2 \sum_{i,j} \rho_{ij} e^{i(\lambda(i) - \lambda(j))t} \int_0^1 \int_0^1 \int G(s_1 t) \ketbra{i}{j} G((1-s_2)t) ~ dG ~ ds_1 ds_2 \\
     &= \frac{ t^2}{\dim + 1} \sum_{i,j} \rho_{ij} e^{i(\lambda(i) - \lambda(j))t} \parens{ \ketbra{i}{j} + \delta_{ij} \sum_{a} \int_0^1 \int_0^1 e^{i(\lambda(a) - \lambda(i))(s_1 + s_2 - 1)t} ds_1 ds_2 \ketbra{a}{a}} \\
     &= \frac{ t^2}{\dim + 1} \sum_{i,j} \rho_{ij} e^{i \Delta_{ij} t} \parens{\ketbra{i}{j} + \delta_{ij} \sum_{a : \Delta_{ai} \neq 0} \frac{2( 1- \cos (\Delta_{ai} t))}{\Delta_{ai}^2 t^2} \ketbra{a}{a} + \delta_{ij} \sum_{a : \Delta_{ai} = 0} \ketbra{a}{a}}
 \end{align}

 We can now combine each of these terms to offer the full picture of the output of the channel to second order. We make two modifications to the results from each sum: first, we will switch to double index notation to make for easier use in other areas, and secondly we let $\rho = \ketbra{i,j}{k,l}$. We note that the first term in the following equation is provided by $(A) + (B)$, the second through $(E) + (F)$, and the last two through $(C) + (D)$. 
 \begin{align}
     &\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{k,l})\bigg|_{\alpha = 0} dG \\
     &= -\frac{2  e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2} \nonumber \\
     &~+ \sum_{(a,b): \Delta(k,l|a,b) \neq 0} \frac{1 + i \Delta(k,l|a,b) t - e^{i \Delta(k,l|a,b) t}}{\Delta(k,l|a,b)^2} + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) \ketbra{i,j}{k,l} \nonumber \\
    &~ +\delta_{i,k} \delta_{j,l} \frac{2 e^{i \Delta(i,j|k,l)t}}{\dim+1} \parens{ \sum_{(a,b): \Delta(i,j|a,b) \neq 0 } \frac{2(1- \cos (\Delta(i,j|a,b)t))}{\Delta(i,j|a,b)^2} \ketbra{a,b}{a,b} + t^2 \sum_{(a,b) : \Delta(i,j|a,b) = 0} \ketbra{a,b}{a,b}} \label{eq:second_order_output}
 \end{align}
\end{proof}

\end{document}