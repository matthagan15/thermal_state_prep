\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{biblatex}
\usepackage{amsmath,amsthm, amssymb}
\usepackage[margin=3cm]{geometry}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}
\usepackage{todonotes}
\usepackage{nicefrac}
\usepackage{mathrsfs}
\usepackage{tikz}
\usepackage{thm-restate}
\usepackage{hyperref}

\usepackage{etoc}

%%%%%%%%    THEOREM DEFINITIONS AND RESTATABLE
% \newcounter{claim}
% \setcounter{claim}{0}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{dependency}{Dependency}
\newtheorem{definition}{Definition}

\newcommand{\matt}[1]{\todo[color=red!50, prepend, caption={Matt}, tickmarkheight=0.25cm]{#1}}
\newcommand{\inlinetodo}[1]{\textcolor{red}{{\Large TODO:} #1}}

\newcommand{\on}{\text{on}}
\newcommand{\off}{\text{off}}

%%%%%%%%    NOTATION DEFINITIONS FOR EASIER WRITING
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\ketbra}[2]{| #1\rangle\! \langle #2|}
\newcommand{\parens}[1]{\left( #1 \right)}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left| \left| #1 \right| \right|}
\newcommand{\diamondnorm}[1]{\left| \left| #1 \right| \right|_\diamond}
\newcommand{\anglebrackets}[1]{\left< #1 \right>}
\newcommand{\overlap}[2]{\anglebrackets{#1 , #2 }}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\openone}{\mathds{1}}
\newcommand{\expect}[1]{\mathbb{E}\brackets{#1}}
\newcommand{\variance}[1]{\textit{Var} \brackets{ #1 }}
\newcommand{\prob}[1]{\text{Pr}\left[ #1 \right]}
\newcommand{\bigo}[1]{O\left( #1 \right)}
\newcommand{\bigotilde}[1]{\widetilde{O} \left( #1 \right)}
\newcommand{\ts}{\textsuperscript}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\trace}[1]{\Tr \brackets{ #1 }}
\newcommand{\partrace}[2]{\Tr_{#1} \brackets{ #2 }}
\newcommand{\complex}{\mathbb{C}}

%%%%% COMMONLY USED OBJECTS
\newcommand{\hilb}{\mathcal{H}}
\newcommand{\partfun}{\mathcal{Z}}
\newcommand{\identity}{\mathds{1}}
\newcommand{\gue}{\rm GUE}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\hermMathOp}{Herm}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\diag}{diag}
\newcommand{\herm}[1]{\hermMathOp\parens{#1}}


\title{Thermal State Prep}
\author{Matthew Hagan, Nathan Wiebe}
\date{May 2022}

\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Going to leave this blank for now. \cite{shiraishi_undecidability_2021}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
We denote the Hilbert space of the system as $\hilb_{S}$ and the environment as $\hilb_{E}$, with the Hamiltonians governing each as $H_{S}$ and $H_{E}$. We will assume without loss of generality that the system's Hilbert space can be encoded with $n$ qubits, giving $\dim_S = 2^{n}$, and the environment's Hilbert space can be encoded with $m$ qubits giving $\dim_E = 2^{m}$. The Hamiltonian for the joint system on $\hilb_{S} \otimes \hilb_{E}$ is then $H = H_{S} \otimes \identity + \identity \otimes H_{E}$. The Hilbert space of the combined system and environment is of dimension $\dim = \dim_E \cdot \dim_S = 2^{n + m}$. 

We will primarily work in the eigenbasis for each Hamiltonian:
\begin{equation}
    H_{S} = \sum_{i = 0}^{2^n - 1} \lambda_S(i) \ketbra{s_i}{s_i} ~,~ H_{E} = \sum_{j=0}^{2^m - 1} \lambda_E(j) \ketbra{e_j}{e_j} ~,~ H = \sum_{i=0}^{2^n - 1} \sum_{j=0}^{2^m - 1} \lambda(i,j) (\ket{s_i} \otimes \ket{e_j})(\bra{s_i} \otimes \bra{e_j}),
\end{equation}
for convenience we will denote the tensor product of eigenvectors simply by their indices $\ket{i,j} \coloneqq \ket{s_i} \otimes \ket{e_j}$. For convenience we define $\lambda(i,j) \coloneqq \lambda_S(i) + \lambda_E(j)$. We also make use of the following notation for the energy differences of the system-environment Hamiltonian
$$\Delta(i,j|k,l) \coloneqq \lambda(i,j) - \lambda(k,l),$$
and will use $\Delta_S(i,j) \coloneqq \lambda_S(i) - \lambda_S(j)$ for just the system differences. We use the notation $\delta(i,j|k,l)$ to denote the product of Kronecker delta functions $\delta(i,j|k,l) = \delta_{i,k} \delta_{j,l}$.

For input states we will typically assume thermal states of the form $\rho_S(\beta) = \frac{e^{-\beta H_S}}{\partfun_S}$, where $\partfun_S = \trace{e^{-\beta H_S}}$, where the inverse temperature $\beta$ of the partition function will typically be assumed but written explicitly if need be. We will assume environment states of the form $\rho_E(\beta) = \frac{e^{-\beta H_E}}{\partfun_E}$.


Overall one application of our channel is represented as
\begin{equation}
    \Phi(\rho ; \alpha, \beta_E, t) :=  \partrace{\hilb_E}{\int e^{+i(H + \alpha G)t} \rho \otimes \rho_E(\beta_E) e^{-i(H + \alpha G) t} dG},
\end{equation}
and we will typically let the parameters $\alpha, \beta_E,$ and $t$ for the channel be implicit. It will prove convenient to study the time evolution of a fixed interaction as a map from the total system-environment Hilbert space to itself. We denote this channel for a fixed interaction $G$ as
\begin{equation}
    \Phi_G(\rho_S \otimes \rho_E(\beta_E) := e^{+i (H+ \alpha G) t} \rho_S \otimes \rho_E(\beta_E) e^{-i (H + \alpha G) t}. \label{eq:phi_g_definition}
\end{equation}
Clearly then $\Phi(\rho_S) = \partrace{\hilb_E}{\int \Phi_G (\rho_S \otimes \rho_E) dG}$. We use $G$ to denote the randomized interaction term, where $G = U_G D U_G^\dagger$. The measure we choose for the eigenbasis of $G$ is $U_G \sim Haar$ and the eigenvalues are i.i.d with mean 0 and variance $1$.  This gives the overall interaction measure as the decomposition $\int dG = \int \int dD ~ dU_G$. The interaction strength of $G$ will be controlled through the coupling coefficient $\alpha$.

\begin{restatable}{lemma}{haar_two_moment} \label{lem:haar_two_moment}
    Let $\int (\cdot) dU$ denote the average distributed according to the Haar measure over $\dim$-dimensional unitary matrices $U$. Then for $\ket{i_1},\ket{i_2},\ldots,\ket{k_2}$ drawn from an orthonormal basis
    \begin{align}
        &\int \bra{i_1} U \ket{j_1} \bra{i_2} U \ket{j_2} \bra{k_1} U^\dagger \ket{l_1} ~ \bra{k_2} U^\dagger \ket{l_2} dU \nonumber \\
        &= ~\frac{1}{\dim^2 - 1} \parens{\delta_{i_1, l_1} \delta_{j_1, k_1} \delta_{i_2, l_2} \delta_{j_2, k_2} + \delta_{i_1, l_2} \delta_{j_1, k_2} \delta_{i_2, l_1} \delta_{j_2, k_1}} \nonumber \\
        &\quad - \frac{1}{\dim(\dim^2 - 1)} \parens{\delta_{i_1, l_2} \delta_{j_1, k_1} \delta_{i_2, l_1} \delta_{j_2, k_2} + \delta_{i_1, l_1} \delta_{j_1, k_2} \delta_{i_2, l_2} \delta_{j_2, k_1}}. \label{eq:haar_two_moment_integral}
    \end{align}
\end{restatable}

\begin{lemma}[Sinc Function Bounds] \label{lem:sinc_poly_approx}
    The following implications hold 
    \begin{align}
        |x| \leq \sqrt{10 \epsilon_{\sinc}/7} &\implies \sinc^2(x) \geq 1 - \epsilon_{\sinc} \label{eq:sinc_lower_bound}\\
        |x| \geq 1 / \sqrt{\epsilon_{\sinc}} &\implies \sinc^2(x) \leq \epsilon_{\sinc}. \label{eq:sinc_upper_bound}
    \end{align}
    % The constant approximation $f(x) = 1$ has error $|f(x) - 1| \leq \widetilde{\epsilon}_{\sinc}$ if $|x| \leq \sqrt{2 \widetilde{\epsilon}_{\sinc}}$. This leads to the observation that $\widetilde{\epsilon}_{\sinc}$ acts as a lower bound for $f(x)$, as $|x| \leq \sqrt{2 \widetilde{\epsilon}_{\sinc}}$ implies $f(x) \geq 1 - \widetilde{\epsilon}_{\sinc}$. We denote this upper bound with $\Delta_{\sinc} \coloneqq \sqrt{2 \widetilde{\epsilon}_{\sinc}}$. 
    % We also require a lower bound $\epsilon_{\sinc}$, such that $|x| \geq \Delta_{\min} \implies f(x) \leq \epsilon_{\sinc}$. Setting $\Delta_{\min} = 1 / \sqrt{\epsilon_{\sinc}}$ guarantees this to hold. 
\end{lemma}
\begin{proof}
    We start with a Taylor Series for $\sinc^2$, which we compute using the expression of $\sinc$ as $\sinc(x) = \frac{\sin x}{x} = \int_0^1 \cos(sx) ds$. We compute the first two derivatives as
    \begin{align}
        \frac{d \sinc^2(x)}{dx} &= -2 \int_0^1 \sin(sx) s ds \int_0^1 \cos(sx) ds \\
        \frac{d^2 \sinc^2(x)}{dx^2} &= -2 \int_0^1 \cos(sx)s^2 ds \int_0^1 \cos(sx) ds + 2\int_0^1 \sin(sx) s ~ds \int_0^1 \sin(sx) s ~ds.
    \end{align}
    We can evaluate each of these derivatives about the origin using continuity of the derivatives along with the limits $\lim_{x \to 0} \cos(sx) = 1$ and $\lim_{x \to 0} \sin(sx) = 0$. We can now compute the Maclaurin Series for some $x_{\star} \in [0,1]$ as
    \begin{align}
        f(x) &= f(0) + x \frac{df}{dx}\bigg|_{x = 0} + \frac{x^2}{2!} \frac{d^2f}{dx^2}\bigg|_{x = x_{\star}}.
    \end{align}
    Plugging in $\sinc^2(0) = 1$ and $\frac{d\sinc^2(x)}{dx}\big|_{x = 0} = 0$ then yields $|\sinc^2(x) - 1| = \frac{|x|^2}{2} \abs{\frac{d^2\sinc^2(x)}{dx^2}(x_{\star})}$. We make use of the rather simplistic bound
    \begin{align}
        \abs{\frac{d^2\sinc^2(x)}{dx^2}(x_{\star})} &\leq 2 \abs{\int_0^1 \cos(sx) s^2 ds \int_0^1 \cos(sx) ds} + 2\abs{\int_0^1 \sin(sx) s ds \int_0^1 \sin(sx) s ds} \\
        &\leq 2 \int_0^1 \abs{\cos(sx)} s^2 ds \int_0^1 \abs{\cos(sx)} ds + 2\parens{\int_0^1 \abs{\sin(sx)} |s| ds}^2 \\
        &\leq 2 \int_0^1 s^2 ds + 2\parens{\int_0^1 s ds}^2 \\
        &\leq 2/3 + 1/2 = 7/6.
    \end{align}
    This yields the final inequality $|\sinc^2(x) - 1| \leq \frac{7|x|^2}{10}$. We then see that $|x| \leq \sqrt{10 \widetilde{\epsilon}_{\sinc}/7}$ implies $|f(x) - 1| \leq \widetilde{\epsilon}_{\sinc}$. 

    The upper bound of $\epsilon_{\sinc}$ for large $|x|$ is relatively straightforward:
    \begin{align}
        f(x) &= \frac{\sin^2(x)}{x^2} \\
            &\leq \frac{1}{|x|^2},
    \end{align}
    where we see that $|x| \geq 1 / \sqrt{\epsilon_{\sinc}}$ implies $\sinc^2(x) \leq \epsilon_{\sinc}$.
\end{proof}

We will often rely on a particularly parametrized form of $f(x)$ which is worth investigating on it's own right. Note we can get a square root improvement of the dependence of $|\Delta_S(i,j) - \gamma|$ on $\epsilon_{\sinc}$ in the below Corallary if we only require $f(x) \geq 1 - \widetilde{\epsilon}_{\sinc}$. This then requires $|\Delta_S(i,j) - \gamma| \in \bigo{\sqrt{\epsilon_{\sinc} \widetilde{\epsilon}_{\sinc}}}$, however this will not prove significantly useful for us so we use the looser bound.
\begin{corollary} \label{cor:gamma_difference_reqs}
    The statements 
    $$|x| \geq \Delta_{\min} \implies \sinc^2\parens{xt / 2} \leq \epsilon_{\sinc}$$
    and
    $$|x| = |\Delta_S(i,j) - \gamma| \leq \sqrt{2} \Delta_{\min} \epsilon_{\sinc} \implies f(xt/2) \geq 1 - \epsilon_{\sinc}$$
    hold for $t = \frac{2}{\Delta_{\min} \sqrt{\epsilon_{\sinc}}}$. This gives $\epsilon_{\sinc} = \frac{4}{\Delta_{\min}^2 t^2}$. We denote the barrier $\Delta_{\sinc} = \sqrt{2} \Delta_{\min} \epsilon_{\sinc}$. 
\end{corollary}
\begin{proof}
    Throughout this proof we can think of $0 \leq \Delta_{\min} \leq \Delta_S(i,j)$ as a constant, so we avoid writing it as function arguments.
    We first want to provide a bound on $t$ such that $|x| \geq \Delta_{\min}$ implies $f(xt/2) \leq \epsilon_{\sinc}$. This is provided through Eq. \eqref{eq:sinc_upper_bound}
    \begin{align}
        \left| \frac{x t }{ 2} \right| = \frac{|x| t}{2} \geq \frac{\Delta_{\min}t}{2}.
    \end{align}
    We see that setting $t$ such that $\Delta_{\min} t / 2 = 1 /\sqrt{\epsilon_{\sinc}}$, which can be rewritten as $t = \frac{2}{\Delta_{min} \sqrt{\epsilon_{\sinc}}}$, yields the implication $|x| \geq \Delta_{\min} \implies f(xt/2) \leq \epsilon_{\sinc}$. 

    We now want to investigate what values of $x = \Delta_S(i,j) - \gamma$, for the given $t$ as above, yields $f(xt/2) \geq 1 - \epsilon_{\sinc}$. We see that the inequality required for this is
    \begin{align}
        \frac{|x| t}{2} &\leq \sqrt{2 \epsilon_{\sinc}} \\
        \iff  |\Delta_S(i,j) - \gamma| \frac{2}{2 \Delta_{\min} \sqrt{\epsilon_{\sinc}}} &\leq \sqrt{2 \epsilon_{\sinc}} \\
        \iff \abs{\Delta_S(i,j) - \gamma} &\leq \sqrt{2} \Delta_{\min} \epsilon_{\sinc}
    \end{align}
\end{proof}


We now see that if we want there to be unique $(i,j)$ such that $|\Delta_S(i,j) - \gamma| \leq \Delta_{\sinc}$ and for $(i',j') \neq (i,j) \implies |\Delta_S(i',j') - \gamma| \geq \Delta_{\min}$, then we require $|\Delta_S(i,j) - \Delta_S(k,l)| \geq \Delta_{\min} + \Delta_{\sinc}$. 

Suppose $|\Delta_S(i,j) - \gamma| \leq \Delta_{\sinc}$ and $|\Delta_S(i,j) - \Delta_S(k,l)| \geq \Delta_{\sinc} + \Delta_{\min}$ for $(k,l) \neq (i,j)$. We would like to show then that $|\Delta_S(k,l) - \gamma| \geq \Delta_{\min}$. We see that given three real numbers $\gamma, \Delta_S(i,j), \Delta_S(k,l)$ we have four relevant orderings:
\begin{align}
    \gamma \leq \Delta_S(i,j) \leq \Delta_S(k,l) \\
    \Delta_S(k,l) \leq \Delta_S(i,j) \leq \gamma \\
    \Delta_S(i,j) \leq \gamma \leq \Delta_S(k,l) \\
    \Delta_S(k,l) \leq \gamma \leq \Delta_S(i,j).
\end{align}
The scenario $\gamma \leq \Delta_S(i,j) \leq \Delta_S(k,l)$ yields
\begin{align}
    |\Delta_S(k,l) - \gamma| &= \Delta_S(k,l) - \gamma \\
    &= \Delta_S(k,l) - \Delta_S(i,j) + \Delta_S(i,j) - \gamma \\
    &\geq \Delta_S(k,l) - \Delta_S(i,j) \\
    &= |\Delta_S(k,l) - \Delta_S(i,j)| \\
    &\geq \Delta_{\min} + \Delta_{\sinc} \\
    &\geq \Delta_{\min}.
\end{align}
The other direction ($\Delta_S(k,l) \leq \Delta_S(i,j) \leq \gamma$) holds similarly. 

The scenario $\Delta_S(k,l) \leq \gamma \leq \Delta_S(i,j)$ holds through the following computation
\begin{align}
    |\Delta_S(k,l) - \gamma| &= \gamma - \Delta_S(k,l) \\
    &= \gamma + \Delta_S(i,j) - \Delta_S(i,j) - \Delta_S(k,l) \\
    &= \gamma - \Delta_S(i,j) + |\Delta_S(i,j) - \Delta_S(k,l)| \\
    &= -|\gamma - \Delta_S(i,j)| + |\Delta_S(i,j) - \Delta_S(k,l)| \\
    &\geq -\Delta_{\sinc} + \Delta_{\min} + \Delta_{\sinc} \\
    &= \Delta_{\min}.
\end{align}
The other direction ($\Delta_S(i,j) \leq \gamma \leq \Delta_S(k,l)$ ) holds similarly.


\section{Taylor's Series for $\Phi$} \label{sec:taylor_series_phi}

The easiest way for us to understand the thermalizing channel $\Phi$ is through a Taylor's series with respect to the coupling constant $\alpha$, which will turn out to give us a series in terms of $\alpha t$ instead. This can be thought of as the weak interaction regime, which is extensively studied in open quantum systems. This section provides results about the first and second order terms in the Taylor's Series for $\Phi$ and we give an upper bound on the norm of the remainder term $R_{\Phi}$. As many of the proofs for the statements in this section tend to be rather technical and do not lend themselves to much insight they can be found in the appendix. 

$\Phi$ can be written with the mean-value version of Taylor's theorem as:
\begin{equation}
    \Phi(\rho_S; \alpha) = \Phi(\rho_S; \alpha = 0) + \alpha \frac{\partial}{\partial \alpha} \Phi(\rho_S; \alpha) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho_S; \alpha) \bigg|_{\alpha = 0} + R_{\Phi}(\rho_S, \alpha_{\star}).
\end{equation}
We will denote the second order approximation as
\begin{equation}
    \Phi^{(2)}(\rho_S) \coloneqq \Phi(\rho_S) - R_{\Phi}(\rho_S, \alpha_\star) = \Phi(\rho_S;\alpha= 0) + \frac{\partial}{\partial \alpha} \Phi(\rho_S, \alpha) \bigg|_{\alpha = 0} + \frac{1}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho_S, \alpha) \bigg|_{\alpha = 0}. \label{def:second_order_approx}
\end{equation}
We will also find it helpful to denote each of the terms as
\begin{equation}
    T^{(k)}(X, \alpha) \coloneqq \frac{\alpha^k}{k!} \frac{\partial^k}{\partial \alpha^k} \Phi(X, \alpha)\bigg|_{\alpha = 0}.\label{def:taylor_series_terms}
\end{equation}
If we were to write out the channel $\Phi$ as a Taylor's Series it would be $\Phi(\rho_S) = \sum_{k = 0}^{\infty} T^{(k)}(\rho_S)$.

We now go through and compute the correction terms $T^{(0)}$, $T^{(1)}$, and $\mathcal{T}^{(2)}$. The first one is nearly trivial
\begin{lemma}
    The zeroth order correction $T^{(0)}$ to the thermalizing channel $\Phi$ is the Heisenberg evolved input 
    \begin{equation}
        T^{(0)}(\rho_S) = e^{i H_S t} \rho_S e^{-i H_S t},
    \end{equation}
    where this expression holds for all matrix inputs and not just density operators.
\end{lemma}
\begin{proof}
This is a straightforward computation after plugging in the definitions
    \begin{align}
        T^{(0)}(\rho_S) &= \partrace{\hilb_E}{\int e^{i(H + 0 G)t} \rho_S \otimes \rho_E(\beta_E) e^{-i(H + 0 G)t} dG } \\
        &= \partrace{\hilb_E}{e^{i (H_S \otimes \identity + \identity \otimes H_E)t} \rho_S \otimes \rho_E(\beta_E) e^{-i (H_S \otimes \identity + \identity \otimes H_E)t}} \\
        &= e^{i H_S t} \rho_S e^{-i H_S t}.
    \end{align}
\end{proof}
It will prove useful that this result holds even if we extend the $\rho_S$ input to an arbitrary matrix input, as nowhere in the proof did we rely on the properties of density matrices. This will be necessary in arguing that coherences, or off-diagonal matrix elements in a density matrix, do not accumulate with repeated uses of our channel.

The next order correction shows that to $O(\alpha)$ the effects of the environment on the system are zero. This shows that higher order corrections are necessary to compute nontrivial environmental effects. We leave this proof in this section to give the reader a taste for how these arguments work in the higher order calculations. This proof in particular solely relies on the randomly chosen eigenvalues of the interaction to be mean 0.
\begin{lemma}
   The first order correction $T^{(1)}$ to the thermalizing channel $\Phi$, with randomized interactions $G = U_G D U_G^\dagger $ such that the average of each eigenvalue satisfies $\mathbb{E}[d_{i,i}] = 0$, is zero:
   \begin{equation}
        T^{(1)}(\rho_S) = 0.
   \end{equation}
\end{lemma}
\begin{proof}
    We start by using linearity of derivatives, integration, and partial trace to compute the action of the $\alpha$ derivative on $\Phi_G$ as
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi(\rho_S) \bigg|_{\alpha = 0} &= \frac{\partial}{\partial \alpha} \partrace{\mathcal{H}_E}{\int \Phi_G(\rho_S) dG} \bigg|_{\alpha = 0} \\
         &= \partrace{\mathcal{H}_E}{\int \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) dG \bigg|_{\alpha = 0} } .
    \end{align}
    Now we use the expression for $\Phi_G$ in Eq. \eqref{eq:phi_g_definition} to compute the derivatives, and we use the compact notation $\rho = \rho_S \otimes \rho_E(\beta_E)$ to represent the entire system-environment input,
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi_G (\rho_S) &= \parens{\frac{\partial}{\partial \alpha} e^{+ i (H + \alpha G)t}} \rho e^{-i (H + \alpha G) t} + e^{+i (H + \alpha G)t} \rho \parens{\frac{\partial}{\partial \alpha} e^{- i (H + \alpha G)t}} \\
        &= \parens{\int_{0}^{1} e^{i s (H+\alpha G)t} (i t G) e^{i (1-s) (H+\alpha G)t} ds} \rho e^{-i(H+\alpha G)t} \nonumber \\
    &~ ~+ e^{i(H+\alpha G)t} \rho \parens{\int_{0}^1 e^{-i s (H+\alpha G) t} (- i t G) e^{-i (1-s) (H+\alpha G)t} ds}. \label{eq:first_order_alpha_derivative}
    \end{align}
    We can further simplify this by bringing in the evaluation of $\alpha = 0$ through the partial trace and integration, as they are uniformly convergent over $\alpha$ (is that the correct notion that allows us to switch orders?)
    \begin{align}
        \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) \bigg|_{\alpha = 0} &= i t \int_0^1 e^{i s H t} G e^{-i s H t} ds e^{i H t} \rho e^{-i H t} - i t e^{+i H t} \rho \int_0^1 e^{-is H t} G e^{-i(1-s) Ht} ds \\
        &= i t \parens{\int_0^1 G(s t) ds} \rho(t) - it \rho(t) \parens{\int_0^1 G(s t) ds} \\
        &= i t \int_0^1 [G(s t), \rho(t)] ds,
    \end{align}
    where we have used the Heisenberg picture $\rho(t) = e^{i H t} \rho e^{-i H t}$ to simplify the notation.

    This expression is now amenable to computing the correction to the total channel. We do so by performing the integration over the randomized interactions. We take advantage of the structure of our interaction measure, that is $G = U_G D U_G^\dagger$ and $dG = dU_G dD$, which allows us to write
    \begin{align}
        \int \frac{\partial}{\partial \alpha} \Phi_G(\rho_S) \bigg|_{\alpha = 0} dG &= it \int \int_0^1 \left[ e^{i H s t} G e^{-i H s t}, \rho(t) \right] ds ~dG \\
        &= it \int_0^1 \left[ e^{i H s t} \parens{\int \int U_G D U_G^\dagger ~dU_G ~ dD} e^{-i H s t}, \rho(t)  \right] ds \\
        &= i t \int_0^1 \left[ e^{i H s t} \parens{\int U_G \parens{\int D ~ dD} ~ U_G^\dagger dU_G } e^{-i H s t}, \rho(t) \right] ds \\
        &= 0.
    \end{align}
    This last step relies on the use of random eigenvalues with mean 0, implying $\int D ~dD = 0$ which shows that $\frac{\partial}{\partial \alpha} \Phi(\rho_S) \big|_{\alpha = 0 } = 0$.
\end{proof}

Now we move on to calculating the second order correction $\mathcal{T}^{(2)}$. This is a significantly more tedious computation, so we move the proof of this result to the appendices. First we compute the "pre-trace" matrix elements of the second order correction $\mathcal{T}^{(2)}$, but for off-diagonal elements first and then diagonal elements.

These can be seen from Lemma \ref{lem:big_one}.

\begin{lemma} \label{lem:t_2_both}
    The diagonal elements in the pre-trace second order correction are given as
    \begin{align}
        &\int \bra{i', j'} \mathcal{T}_G \left( \ketbra{i, j}{i, j} \right) \ket{i', j'} ~dG \\
        &= \begin{cases}
            - \frac{\alpha^2 t^2 }{\dim + 1} \sum_{(a,b) \neq (i, j)} \sinc^2(\Delta(a,b|i,j) t / 2) & (i,j) = (i', j') \\
    \frac{\alpha^2 t^2 }{\dim + 1} \sinc^2(\Delta(i,j | i', j') t /2) & (i, j) \neq (i', j'),
        \end{cases} \label{eq:second_order_transitions_final_final}
    \end{align}
    where we use the fact that $\lim_{x \to 0} \sinc(x) = 1$ to compute the degenerate contributions (when $\Delta(i, j | i', j') = 0$)
\end{lemma}
However, if we want to compute the effects of the channel on a given input, we have to perform the partial trace over the environment, which clearly depends on initial state of the environment we choose. To proceed, we choose a simple two-level environment $H_E = \begin{bmatrix}
    0 & 0 \\ 0 & \gamma
\end{bmatrix}$, where the eigenvectors $\ket{0}$ and $\ket{1}$ have eigenvalues 0 and $\gamma$ respectively. Here, and throughout the rest of the paper, $\gamma$ should be thought of as a user defined parameter that can be adjusted, or chosen probablistically, throughout the cooling procedure. We use a thermal input state $\rho_E(\beta_E)$, as defined in the preliminaries. 

\begin{lemma} \label{lem:on_and_off_resonance}
Let $\mathcal{T}$ denote the second order correction to the thermalizing channel $\Phi$. 
\end{lemma}

\begin{lemma} \label{lem:t_2_system_only}
    The second order correction to the channel $\Phi$ with two level environment as described above, differs if the output state $j$ is less than, equal to, or greater than $i$. For $i < j$, meaning we are transitioning the systme from a lower energy state to a higher energy and the environment is losing energy, we have
    \begin{equation}
        \left| \bra{j}\mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \parens{(\Delta_S(i,j) + \gamma) t/ 2} \right| \leq 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
    \end{equation}
    For $i > j$, meaning we are transitioning the system from a high energy state to a lower energy state, we have
    \begin{equation}
        \left| \bra{j}\mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 \parens{(\Delta_S(i,j) - \gamma) t/ 2} \right| \leq 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
    \end{equation}
    For $i = j$, where the state does not gain or lose energy, we get contributions from all other system states that are weighted based on the gain or loss of energy and is given by
    \begin{align}
        &\left| \bra{i} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{i} + \frac{\alpha^2 t^2}{\dim + 1} \parens{\frac{1}{1 + e^{-\beta_E \gamma}} \sum_{a < i} \sinc^2 ((\Delta_S(a, i) + \gamma) t/ 2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sum_{a > i} \sinc^2((\Delta_S(a, i) - \gamma)t/ 2)} \right| \nonumber \\
        &\leq 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc},
    \end{align}
    where we point out the difference in sign for the $i = j$ terms and the $i \neq j $ terms.
\end{lemma}
\begin{proof}
    For $i < j$ we use Eq. \eqref{eq:second_order_transitions_final_final} straightforwardly
    \begin{align}
        &\bra{j} \mathcal{T}^{(2)} (\ketbra{i}{i}) \ket{j} \nonumber \\
        &= \frac{1}{1 + e^{-\beta_E \gamma}} \int \parens{\bra{j, 0} \mathcal{T}^{(2)}_G(\ketbra{i,0}{i, 0}) \ket{j, 0} + \bra{j, 1} \mathcal{T}^{(2)}_G (\ketbra{i, 0}{i ,0}) \ket{j, 1} } dG \nonumber \\
        &~+ \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \int \parens{\bra{j, 0} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{j, 0} + \bra{j, 1} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{j, 1}} dG \\
        &= \frac{1}{1 + e^{-\beta_E \gamma}} \frac{\alpha^2 t^2}{\dim + 1}\parens{\sinc^2(\Delta_S(i, j) t/ 2) + \sinc^2 (\Delta_S(i, j) - \gamma)t /2} \nonumber \\
        &~+ \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \frac{\alpha^2 t^2 }{\dim + 1} \parens{\sinc^2((\Delta_S(i,j) + \gamma) t/ 2) + \sinc^2(\Delta_S(i,j) t/ 2)} \\
        &= \frac{\alpha^2 t^2}{\dim + 1}\left( \sinc^2(\Delta_S(i,j)t/2) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) \right) \label{eq:t_2_intermediate_1}
        \end{align}
    From this expression, we see that since $i < j \implies \Delta_S(i,j) < 0$ we have that $\sinc^2(\Delta_S(i,j)t/2) \leq \epsilon_{\sinc}$ and $\sinc^2((\Delta_S(i,j) - \gamma)t/2) \le \epsilon_{\sinc}$. We can then isolate the only non-negligible term, which leads to the difference
    \begin{align}
        i < j \implies \left| \bra{j}\mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \parens{(\Delta_S(i,j) + \gamma) t/ 2} \right| &\le 2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
        &= \frac{8 \alpha^2}{\Delta_{\min}^2 (\dim + 1)}.
    \end{align}
    For $i > j$ we can start from Eq. \ref{eq:t_2_intermediate_1} and the only difference is that the sinc term with $\Delta_S(i,j) - \gamma$ is the non-negligible term so we get
    \begin{align}
    i > j \implies \left| \bra{j}\mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \parens{(\Delta_S(i,j) - \gamma) t/ 2} \right| &\le \frac{8 \alpha^2}{\Delta_{\min}^2 (\dim + 1)}.
        \end{align}
     
    For the $i = j$ calculation we have
    \begin{align}
        \bra{i} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{i} &= \frac{1}{1 + e^{-\beta_E \gamma}} \int \bra{i, 0} \mathcal{T}^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i, 0} dG + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \int \bra{i, 0} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i, 0} dG \nonumber \\
        &~+ \frac{1}{1 + e^{-\beta_E \gamma}} \int \bra{i, 1} \mathcal{T}^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i, 1} dG + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \int \bra{i, 1} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i, 1} dG . \label{eq:same_state_transition_with_env}
    \end{align}
    In this expression there are two terms that are same-state transitions of $\ket{i, 0} \to \ket{i, 0}$ and $\ket{i, 1} \to \ket{i,1}$ and the other two terms are negligible. We start by upper bounding the negligible terms and then analyze the same-state transitions.
    \begin{align}
        \int \bra{i, 0} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i, 0} dG &= \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\gamma t/ 2)
    \end{align}
    and 
    \begin{align}
        \int \bra{i, 1} \mathcal{T}^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i, 1} dG &= \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(-\gamma t/ 2),
    \end{align}
    however this is the same as the prior negligible term as $\sinc^2(x) = \sinc^2(-x)$. Adding these two terms, along with their respective coefficients yields
    \begin{align}
        &\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \int \bra{i, 0} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i, 0} dG + \frac{1}{1 + e^{-\beta_E \gamma}} \int \bra{i, 1} \mathcal{T}^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i, 1} dG \\
        &= \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\gamma t/ 2 + \frac{1}{1 + e^{-\beta_E \gamma}}\frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\gamma t/ 2) \\
        &= \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\gamma t/ 2). \label{eq:t_2_same_state_1}
    \end{align}
    We will keep this term as is for now. 

    Moving on to the same state transitions we start with the $\ket{i, 0} \to \ket{i, 0}$ term
    \begin{align}
        &\int \bra{i,0} \mathcal{T}^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i,0} dG \nonumber \\
        &= -\frac{\alpha^2 t^2}{\dim + 1} \sum_{(a,b) \neq (i, 0)} \sinc^2 \parens{\Delta_S(a, b | i, 0) t / 2} \\ 
        &= - \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a \neq i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} } \\
        % &=- \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a < i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} + \sum_{a > i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} } \\
        &=- \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a \neq i} \sinc^2 \left( \frac{\Delta_S(a, i) t}{2} \right) + \sum_{a \neq i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}} }
        % &= - \frac{\alpha^2 t^2}{\dim + 1} \sinc^2 (\gamma t / 2) \nonumber \\
        % &~ - \frac{\alpha^2 t^2}{\dim + 1} \sum_{a > i} \sum_{b = 0, 1} \sinc^2 \parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} \nonumber \\
        % &~ -\frac{\alpha^2 t^2}{\dim + 1} \parens{\sum_{a < i} \sinc^2 \parens{\frac{\Delta_S(a, i) t}{2}} + \sum_{a < i} \sinc^2 \parens{\frac{(\Delta_S(a, i) + \gamma)t}{2}} }. \label{eq:same_state_transition_1}
    \end{align}
    The other same state transition contribution is computed similarly as
        \begin{align}
        &\int \bra{i,1} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i,1} dG \nonumber \\
        &= -\frac{\alpha^2 t^2}{\dim + 1} \sum_{(a,b) \neq (i, 1)} \sinc^2 \parens{\Delta_S(a, b | i, 1) t / 2} \\ 
        &= - \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a \neq i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 1))t}{2}} } \\
        % &=- \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a < i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} + \sum_{a > i} \sum_{b = 0, 1} \sinc^2\parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} } \\
        &=- \frac{\alpha^2 t^2}{\dim  + 1} \parens{\sinc^2 (\gamma t / 2) + \sum_{a \neq i} \sinc^2 \left( \frac{\Delta_S(a, i) t}{2} \right) + \sum_{a \neq i} \sinc^2\parens{\frac{(\Delta_S(a, i) - \gamma)t}{2}} }
        % &= - \frac{\alpha^2 t^2}{\dim + 1} \sinc^2 (\gamma t / 2) \nonumber \\
        % &~ - \frac{\alpha^2 t^2}{\dim + 1} \sum_{a > i} \sum_{b = 0, 1} \sinc^2 \parens{\frac{(\Delta_S(a, i) + \Delta_E(b, 0))t}{2}} \nonumber \\
        % &~ -\frac{\alpha^2 t^2}{\dim + 1} \parens{\sum_{a < i} \sinc^2 \parens{\frac{\Delta_S(a, i) t}{2}} + \sum_{a < i} \sinc^2 \parens{\frac{(\Delta_S(a, i) + \gamma)t}{2}} }. \label{eq:same_state_transition_1}
    \end{align}
    Adding these together with their respective coefficients results in
    \begin{align}
        &\frac{1}{1 + e^{-\beta_E \gamma}} \int \bra{i, 0} \mathcal{T}^{(2)}_G (\ketbra{i, 0}{i, 0}) \ket{i, 0} dG + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \int \bra{i, 1} \mathcal{T}^{(2)}_G (\ketbra{i, 1}{i, 1}) \ket{i, 1} dG \\
        &= - \frac{\alpha^2 t^2}{\dim + 1} \left( \sinc^2\parens{\frac{\gamma t}{2}} + \sum_{a \neq i} \sinc^2\parens{\frac{\Delta_S(a,i)t}{2}} \right) \nonumber \\
        &~ - \frac{\alpha^2 t^2}{\dim + 1} \left( \frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a \neq i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}}  + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a \neq i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \right).
    \end{align}
    Now when combining with the previously two computed terms from Eq. \eqref{eq:t_2_same_state_1} we get a cancellation of the $\sinc^2(\gamma t/ 2)$ term with the final result
    \begin{align}
        &\bra{i} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{i} = - \frac{\alpha^2 t^2}{\dim + 1} \left(\sum_{a \neq i} \sinc^2\parens{\frac{\Delta_S(a,i)t}{2}}  \right) \nonumber \\
        &~ - \frac{\alpha^2 t^2}{\dim + 1} \left( \frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a \neq i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}}  + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a \neq i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \right) \\
        &\bra{i} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{i} + \frac{\alpha^2 t^2}{\dim + 1} \left( \frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}}  + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a > i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \right) = \nonumber \\
        &- \frac{\alpha^2 t^2}{\dim + 1} \left( \sum_{a \neq i} \sinc^2\parens{\frac{\Delta_S(a,i)t}{2}} + \frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a > i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}}  + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \right)  \label{eq:t_2_same_state_2}
    \end{align}
    Now by taking absolute values of the right hand side, along with obvious triangle inequalities, we see that each term is upper bounded by $\epsilon_{\sinc}$. We make the following upper bounds
    \begin{align}
        \left| \sum_{a \neq i}\sinc^2\parens{\frac{\Delta_S(a,i)t}{2}} \right| &\le \dim_S\epsilon_{\sinc} \\
        \left|\frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a > i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}} \right| &\le \dim_S \epsilon_{\sinc} \\
        \left| \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \right| &\le \dim_S \epsilon_{\sinc},
    \end{align}
    where we make simple bounds (such as $\dim_S - i \le \dim_S$) because they preserve the asymptotic behavior and are much easier to work with. Putting these into Eq. \eqref{eq:t_2_same_state_2} yields
    \begin{align}
        &\left| \bra{i} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{i} + \frac{\alpha^2 t^2}{\dim + 1} \parens{\frac{1}{1 + e^{-\beta_E \gamma}} \sum_{a < i} \sinc^2 ((\Delta_S(a, i) + \gamma) t/ 2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sum_{a > i} \sinc^2((\Delta_S(a, i) - \gamma)t/ 2)} \right| \nonumber \\
        &\leq 3 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
    \end{align}
    Plugging in the value for $\epsilon_{\sinc}$ yields the lemma statement. 
 \end{proof}



\begin{restatable}[Second Order Correction]{lemma}{secondOrderChannelHaar} \label{lem:big_one}
    Given a system Hamiltonian $H_{S}$, an environment Hamiltonian $H_{E}$, a simulation time $t$, and coupling coefficient $\alpha$, let $\Phi_G : \hilb_S \otimes \hilb_E \to \hilb_S \otimes \hilb_E$ denote the fixed interaction channel 
    \begin{equation}
        \Phi_G(\rho) = e^{+i (H + \alpha G)t} \rho e^{-i (H + \alpha G)t},
    \end{equation}
    where $H = H_S \otimes \identity + \identity \otimes H_E$. We compute the output of the averaged channel at $\alpha = 0$ for the basis $\ketbra{a}{b}$ of linear operators as:
 \begin{align}
     &\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{k,l})\bigg|_{\alpha = 0} dG \\
     &= -\frac{2  e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2} \nonumber \\
     &~+ \sum_{(a,b): \Delta(k,l|a,b) \neq 0} \frac{1 + i \Delta(k,l|a,b) t - e^{i \Delta(k,l|a,b) t}}{\Delta(k,l|a,b)^2} + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) \ketbra{i,j}{k,l} \nonumber \\
    &~ +\delta_{i,k} \delta_{j,l} \frac{2 e^{i \Delta(i,j|k,l)t}}{\dim+1} \parens{ \sum_{(a,b): \Delta(i,j|a,b) \neq 0 } \frac{2(1- \cos (\Delta(i,j|a,b)t))}{\Delta(i,j|a,b)^2} \ketbra{a,b}{a,b} + t^2 \sum_{(a,b) : \Delta(i,j|a,b) = 0} \ketbra{a,b}{a,b}}
 \end{align}
\end{restatable}
The proof of this is given in Appendix \ref{sec:haar_integral_appendix}. 


Now that we are able to compute the diagonal elements of the transition matrix we turn our attention to the off-diagonal elements of. We show that off-diagonal elements are not introduced by the channel to this order of approximation, meaning if the input state is diagonal then the output state is also diagonal to $\bigo{\alpha^2 t^2}$. Further, any nonzero off-diagonal elements remain in place and don't spread or transfer to other off-diagonal elements. 
\begin{lemma} \label{lem:off_diagonal_elems}
     For the following we are investigating off-diagonal elements, so we assume $i' \neq k'$ and $j' \neq l'$.
    \begin{align}
    &\int \bra{i', j'}  \mathcal{T}^{(2)}_G \left( \ketbra{i, j}{k, l} \right) \ket{k', l'} ~dG \\
    &=\begin{cases}
        -\frac{\alpha^2 e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg( \sigma(i,j) + \sigma(k,l) + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) & (i, j) = (i', j') \text{ and } (k, l) = (k', l') \\
        0 & (i, j) \neq (i', j') \text{ and } (k, l) \neq (k', l')
    \end{cases}
    \end{align}
    where we let $\sigma$ denote
    \begin{equation}
        \sigma(i,j) \coloneqq \sum_{a,b: \Delta(i,j,|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2}
    \end{equation}
    and $\eta(i,j)$ denotes the degeneracy of eigenvalue $\lambda_S(i,j)$ of the joint system-environment Hamiltonian. 
\end{lemma}


The real utility of Lemma \ref{lem:off_diagonal_elems} is that it allows us to approximate the quantum dynamics of the thermalization channel via Markovian dynamics of a probability vector over eigenstates.
\begin{lemma} \label{lem:quantum_to_classical}
    Let $T$ be the matrix defined by $e_i^T T e_j \coloneqq \bra{i} \mathcal{T}^{(2)}(\ketbra{j}{j}) \ket{i}$. The matrix $I + T$ is a stochastic matrix and models the dynamics of a repeated interactions channel up to $\bigo{\alpha^2 t^2}$,
    \begin{equation}
        \bra{j} (\identity + \mathcal{T}^{(2)})^{\circ L} (\ketbra{i}{i}) \ket{j} = e_j^T (I + T)^L e_i.
    \end{equation}
    By linearity of $\identity + \mathcal{T}^{(2)}$ this identity extends to any diagonal density matrix input $\rho = \sum_i p(i) \ketbra{i}{i}$.
\end{lemma}
\begin{proof}
    We prove this inductively on $L$. The base case of $L = 1$ is trivial from the defintion of $T$
    \begin{align}
    \bra{j} (\identity + \mathcal{T}^{(2)})(\ketbra{i}{i}) \ket{j} &= \delta_{i,j} + \bra{j} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{j} \\
    &= e_j^T (I +  T) e_i.
\end{align}
For the inductive step we will rely on the fact that there are no off-diagonal elements for diagonal inputs. In symbols
\begin{align}
    \bra{j} \mathcal{T}^{(2)} (\ketbra{i}{i}) \ket{k} &= \delta_{j,k} \bra{j} \mathcal{T}^{(2)} (\ketbra{i}{i}) \ket{j} \\
    \implies \bra{j} \mathcal{T}^{\circ L} (\ketbra{i}{i}) \ket{k} &= \delta_{j,k} \bra{j} \mathcal{T}^{\circ L} (\ketbra{i}{i}) \ket{j}.
\end{align}
This is again by induction where the case $L = 1$ is given by Lemma \ref{lem:off_diagonal_elems} and the inductive step is 
\begin{align}
    \bra{j} \mathcal{T}^{\circ L} (\ketbra{i}{i}) \ket{k} &= \bra{j} \mathcal{T} \left( \mathcal{T}^{\circ L - 1} (\ketbra{i}{i}) \right) \ket{k} \\
    &= \sum_{m, n} \bra{j} \mathcal{T}\left( \ketbra{m}{m}\mathcal{T}^{\circ L - 1}(\ketbra{i}{i}) \ketbra{n}{n}\right) \ket{k} \\
    &= \sum_{m, n} \delta_{m,n} \bra{m}\mathcal{T}^{\circ L - 1}(\ketbra{i}{i}) \ket{m} \bra{j} \mathcal{T}\left(   \ketbra{m}{m} \right) \ket{k} \\
    &= \sum_{m} \bra{m}\mathcal{T}^{\circ L - 1}(\ketbra{i}{i}) \ket{m} \delta_{j,k} \bra{j} \mathcal{T}\left(   \ketbra{m}{m} \right) \ket{j} \\
    &= \delta_{j,k} \bra{j} \mathcal{T}^{\circ L} (\ketbra{i}{i}) \ket{j}.
\end{align} 
This argument points the way towards how we will prove the inductive step in our stochastic conversion. 
\begin{align}
    \bra{j} (\identity + \mathcal{T}^{(2)})^{\circ L}(\ketbra{i}{i}) \ket{j} &= \bra{j} \left( (\identity + \mathcal{T}^{(2)})^{\circ L - 1} (\ketbra{i}{i}) + \mathcal{T}^{(2)} \circ (\identity + \mathcal{T}^{(2)})^{\circ L - 1} (\ketbra{i}{i}) \right)\ket{j} \\
    &= e_j^T (I + T)^{L - 1} e_i + \bra{j} \mathcal{T}^{(2)} \circ (\identity + \mathcal{T}^{(2)})^{\circ L - 1} (\ketbra{i}{i}) \ket{j} . \label{eq:matrix_reloaded1}
\end{align}
We used the inductive step to compute the $\identity$ term and we now have to break down the $\mathcal{T}^{(2)}$ term. 
\begin{align}
    \bra{j} \mathcal{T} \circ (\identity + \mathcal{T})^{\circ L - 1} (\ketbra{i}{i}) \ket{j} &= \sum_{m, n} \bra{j} \mathcal{T}\left( \ketbra{m}{m} (\identity + \mathcal{T})^{\circ L - 1} (\ketbra{i}{i}) \ketbra{n}{n} \right) \ket{j} \\
    &= \sum_{m} \bra{j} \mathcal{T} \left( \ketbra{m}{m} \right) \ket{j} e_m^T (I + T)^{L - 1} e_i \\
    &= \sum_m e_j^T T e_m e_m^T (I + T)^{L -1} e_i \\
    &= e_j^T T(I + T)^{L-1} e_i.
\end{align}
Substituting this into Eq. \eqref{eq:matrix_reloaded1} yields $e_j^T (I + T)^{L-1} e_i$. The stochasticity of $I + T$ comes from the fact that $\trace{\mathcal{T}(\ketbra{i}{i})} = 0$, along with the fact that the self-transition terms for $\mathcal{T}$ are negative which correspond to diagonal elements of T. Adding in $I$ makes the sum a stochastic matrix.
\end{proof}

Since we will be effectively reducing our quantum dynamics to classical dynamics over the eigenbasis for $H_S$ we will need bounds on the convergence of Markov chains. This is a very deep area of research, with many decades of results, so we will not attempt to provide even a summary of the current understanding. The standard textbook on the mixing times of Markov chains by Levin and Peres \cite{levin2017markov} from 2017 has almost 5000 citations at the time of writing. Instead, as we will be dealing with non-reversible Markov chains we only need the following theorem due to Jerison.
\begin{theorem}(Markov Relaxation Time) \label{thm:markov_chain_bound}
    Let $M : \mathbb{R}^{N} \to  \mathbb{R}^{N}$ be a Markov transition matrix acting on an $N$ dimensional state space with absolute spectral gap $\lambda_{\star} \coloneqq 1 - \max_{i > 1} |\lambda_i(M)|$, where the eigenvalues of $M$ are ordered $1 = \lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_N \geq -1$. Given this gap, if the number of steps $t$ in the Markov chain satisfies the following bound
    \begin{align}
        \frac{N}{\lambda_{\star}} \left( 2\log \frac{1}{\lambda_{\star}} + 4(1 + \log 2)\right) + \frac{1}{\lambda_{\star}} \left(2 \log \left( \frac{1}{\epsilon} \right) - 1\right) \le L,
    \end{align}
    then we can guarantee that the resulting state is $\epsilon$ close to the fixed point
    \begin{equation}
        \forall \vec{x} \text{ s.t. } x_i \ge 0 \text{ and } \sum_i x_i = 1, \quad \norm{\vec{\pi} - M^L \vec{x}}_1 \le \epsilon,
    \end{equation}
    where $\vec{\pi}$ is the unique eigenvector of eigenvalue 1 for $M$.
\end{theorem}


\subsection{Remainder Bound}
We now aim to bound the spectral norm of the remainder term $R_{\Phi}$. This is rather tedious, as even to third order in $\alpha$ we have 24 terms to bound. For example, looking first at the $(A)$ term resulting from the second order derivative in Eq. \eqref{eq:second_order_deriv_intermediate_a} we have 4 multiplicative factors involving $\alpha$, leading to 4 terms from this single term of the second order derivative. As there are six terms in total, this yields 24 terms. We can profit from the fact though that the expressions for the second order derivatives do simplify and the final expression only has 3 terms, as two of the derivatives act similarly. We first will analyze the single term from Eq. \eqref{eq:second_order_deriv_intermediate_a} in detail. 


\begin{lemma} \label{lem:remainder_bound}
    Let $\Phi(\rho)$ be the thermalizing channel as defined in Def. \eqref{eq:phi_g_definition} and $R_{\Phi}$ the remainder term 
    \begin{equation}
        R_{\Phi}(\rho, \alpha) = \Phi(\rho, \alpha) - \rho - \frac{\alpha}{1!} \frac{\partial}{\partial \alpha} \Phi(\rho)\bigg|_{\alpha = 0} - \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0}.
    \end{equation}
    Then we have that $\norm{R_{\Phi}} \leq \epsilon_R$ for a simulation duration of
    \begin{equation}
        \alpha t \in O\parens{\frac{\epsilon_R^{1/3}}{ \sqrt{\log(\dim_S)}}}.
    \end{equation}
\end{lemma}
\begin{proof}
    First I need to write down what exactly we are trying to bound. What we are essentially trying to do is bound the matrix entries:
\begin{equation}
    \Phi(\rho) = \rho + \frac{\alpha}{1} \frac{\partial}{\partial \alpha} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^3}{3!} \frac{\partial^3}{\partial \alpha^3} \Phi(\rho) \bigg|_{\alpha = \alpha^\star},\label{eq:remExpress}
\end{equation}
where $\alpha^\star \in [0, \alpha]$. Our goal is then to bound the spectral norm of the remainder term for all such $\alpha^\star$.

% \begin{align}
%     &\norm{\frac{\partial}{\partial \alpha} (it)^2 \int_0^1 \parens{\int_0^1 e^{i s_1 s_2 (H+\alpha G)t} s_1 G e^{i s_1 (1-s_2) (H+\alpha G)t} ds_2} G e^{i(1-s_1) (H+\alpha G)t} ds_1 \rho e^{-i(H+\alpha G) t}} \\
%     &=t^3 \norm{\int_0^1 \int_0^1 \int_0^1 e^{i s_1 s_2 s_3(H + \alpha G)t} G e^{i s_1 s_2 (1 - s_3)} ds_3 G e^{i s_1 (1-s_2)(H + \alpha G)t} ds_2 G e^{i(1-s_1)(H + \alpha G)t} ds_1 \rho e^{-i(H + \alpha G)t} }  + t^3 \norm{\ldots}\\
%     &\leq t^3 \norm{G}^3 + t^3 \norm{\ldots} \\
%     &\leq 4 t^3 \norm{G}^3,
% \end{align}
% where we used smoothness of the integrand to bring the norm inside the integral via the triangle inequality and submultiplicativity of the operator norm to simply break the norm of the product into the product of each of the norms. The operator norm of the unitary operators and the density matrix are each 1, and the resulting integrals yield only fractional values, which are upper bounded by 1. Now here we see the final simplification that can be made, each term $(B), (C)$, etc., will yield at most a cubic power of $\norm{G}$, so we can upper bound each term in the final sum as $t^3 \norm{G}^3$. In reality we would have terms such as $\norm{G} \norm{G^3}$ and all other polynomials, but we don't care.

% Now as $G$ is a random matrix, we can only bound it's operator norm with a probabilistic guarantee. 

\begin{align}
    \|\partial_\alpha^3 \rho(\alpha) \| &= \left\| \frac{\partial^3}{\partial \alpha^3} {\rm Tr}_{H_E} \int e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t} dG\right\|\nonumber\\
    &= \left\| \frac{\partial^3}{\partial \alpha^3} {\rm Tr}_{H_E} \int e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t} dG\right\|\nonumber\\
    &\le    \int \left\|{\rm Tr}_{H_E}\frac{\partial^3}{\partial \alpha^3}\left( e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t}\right) \right\| dG\label{eq:3derivBd}
\end{align}
Next we can apply Duhamel's formula to see that
\begin{align}
    \partial_\alpha \left( e^{i(H+\alpha G)t} \rho_S \otimes \rho_E e^{-i(H+\alpha G)t}\right) &=\int_0^1 e^{i(H+\alpha G)ts} (iGt)e^{i(H+\alpha G)t(1-s)}  \rho_S \otimes \rho_E e^{-i(H+\alpha G)ts}  ds\nonumber\\
    &\quad+\int_0^1  e^{i(H+\alpha G)t}\rho_S \otimes \rho_Ee^{-i(H+\alpha G)ts} (-iGt)e^{-i(H+\alpha G)t(1-s)}  ds
\end{align}
We can recurse this two more times and observe that because $G$ is independent of $\alpha$
that $12$ terms each appear in the derivative depending on the particular locations where the derivatives land from the use of the product rule.  As an example,
consider the first such example that appears in our norm bound which for unitary matrix valued functions $U_1,U_2,U_3 : \mathbb{R}^3 \mapsto L(\hilb_S \otimes \hilb_E)$
\begin{align}
    &\| {\rm Tr_{H_E}} \int_0^1\int_0^{s}\int_0^{s'} U_1(s,s',s'')(iGt)U_2(s,s',s'') (iGt) U_3(s,s',s'') (iGt) U_4(s,s',s'') d^3s\|\nonumber\\
    &\le \int_0^1 \int_0^s \int_0^{s'} \|{\rm Tr_{\hilb_E}}U_1(s,s',s'')(iGt)U_2(s,s',s'') (iGt) U_3(s,s',s'') (iGt) U_4(s,s',s'')\| d^3s
\end{align}
Next we use the fact from Proposition 1 of~\cite{rastegin2012relations} that for any normal operator $A$ acting on a finite dimensional Hilbert space $\hilb_S\otimes \hilb_E$ that the spectral norm obeys
\begin{equation}
    \|{\rm Tr}_{\hilb_E} A\| \le \|A\|.
\end{equation}
Thus from the unitary invariance of the norm and its sub-multiplicative property
\begin{equation}
    \int_0^1 \int_0^s \int_0^{s'} \|{\rm Tr_{\hilb_E}}U_1(s,s',s'')(iGt)U_2(s,s',s'') (iGt) U_3(s,s',s'') (iGt) U_4(s,s',s'')\| d^3s \le (\|G\|t)^3.\label{eq:t3error}
\end{equation}
Next as there are $12$ such terms that need to be considered we find by combining~\eqref{eq:3derivBd} and~\eqref{eq:t3error}
\begin{equation}
    \|\partial_\alpha^3 \phi(\rho) \| \le 12t^3\int \|G\|^3 dG
\end{equation}
Then an application of~\eqref{eq:remExpress} yields
\begin{equation}
    \norm{\Phi(\rho) - \left(\rho + {\alpha} \frac{\partial}{\partial \alpha} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0} \right)} \leq 12 \alpha^3 t^3 \int \norm{G}^3 dG.\label{eq:remTaylorBd}
\end{equation}

At this stage an interesting statistical problem emerges.  The spectral norm is the largest singular value of a matrix but here the matrix $G$ is a random matrix with Gaussian eigenvalues distributed as a standard normal distribution.  The correct property to look at is the largest and smallest order statistic for the eigenvalues cubed of $G$.  The average value of this quantity will correspond to the norm.  Specifically we can use the theory of order statistics to write this average as
\begin{align}
    \int \max_j \lambda_j(G)^3 dG &= \dim_S\int_{-\infty}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \left({\rm Pr}(x\le \lambda)\right)^{\dim_S -1} d\lambda\\
    &= \dim_S\int_{-\infty}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \left(1-\frac{{\rm erfc}(\lambda/\sqrt{2})}{2} \right)^{\dim_S-1} d\lambda.\\
    &=-\dim_S\int_{-\infty}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \sum_{k=1}^{\dim_S-1} \binom{\dim_S-1}{k}\frac{{\rm erfc}(\lambda/\sqrt{2})^{k}}{2^{k}} d\lambda
\end{align}
Note that because the complementary error function approximates a step function about zero and $\lambda^3$ is an odd function, the above integral is positive despite the overall negative sign.
Now let us define a cutoff on the integral of $\Lambda\ge 1$ and aim to set the value of $\Lambda$ such that the integral beyond this cutoff is at most $\epsilon$ large.  Specifically we aim to find $\Lambda$ such that
\begin{equation}
    0\le -\dim_S\int_{\Lambda}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \sum_{k=0}^{\lfloor \dim_S/2-1\rfloor} \binom{\dim_S-1}{2k+1}\frac{{\rm erfc}(\lambda/\sqrt{2})^{2k+1}}{2^{2k+1}} d\lambda \le \epsilon
\end{equation}
The complementary error function is a rapidly decaying function of $\lambda$ for $\lambda>0$.  Thus in order to simplify the sum to allow only the first order term in the expansion to dominate we wish to take $\Lambda$ large enough so that the total error is at most $\epsilon/2$ which is implied by taking
\begin{equation}
    \frac{{\rm erfc}(\Lambda/\sqrt{2})}{2} \le \frac{\epsilon}{4\dim_S \Lambda^3}.
\end{equation}
The complementary error function has the following asymptotic expansion
\begin{equation}
    {\rm erfc}(\lambda/\sqrt{2}) = \frac{\sqrt{2}e^{-\lambda^2/2}}{\lambda \sqrt{\pi}}\left( 1 + O(\lambda^{-2}) \right)
\end{equation}
Thus using the fact that here $\lambda\ge \Lambda$, $\Lambda \ge 1$ an $e^{-\Lambda^2/2}\Lambda^2 \le 1$ it suffices to take
\begin{equation}
    \Lambda \in O\left(\sqrt{\log(\dim_S/\epsilon)} \right)
\end{equation}
Next we wish to show that the leading order term in the integral is also small we have that
\begin{align}
    -\dim_S^2\int_{\Lambda}^{\infty}  \lambda^3 \frac{\sqrt{2}e^{-\lambda^2/2}}{\sqrt{\pi}} \frac{{\rm erfc}(\sqrt{2}\lambda)}{2} d\lambda&\le \dim_S^2\sqrt{\int_{-\infty}^\infty \lambda^6 \frac{e^{-\lambda^2/2}}{\sqrt{2\pi}}  d\lambda}\sqrt{\int_\Lambda^\infty\left(\frac{{\rm erfc}^2(\sqrt{2}\lambda)}{4}\right)\frac{e^{-\lambda^2/2}}{\sqrt{2\pi}}  d\lambda }\nonumber\\
    &=\sqrt{15} \dim_S^2 \sqrt{\int_\Lambda^\infty\left(\frac{{\rm erfc}^2(\sqrt{2}\lambda)}{4}\right)\frac{e^{-\lambda^2/2}}{\sqrt{2\pi}}  d\lambda }\nonumber\\
    &\le \sqrt{\frac{15}{4}} \dim_S^2 {\rm erfc}(\sqrt{2}\Lambda) \le 4 \dim_S^2 \left(\frac{e^{-2\Lambda^2}}{\Lambda \sqrt{2\pi}} \right) (1+O(\Lambda^{-2})).
\end{align}
As before using $\Lambda \ge 1$ and isolating for $\Lambda$ we see for this term as well that we can make the error at most $\epsilon/2$ for a value of $\Lambda$ that scales as
\begin{equation}
    \Lambda \in O\left(\sqrt{\log(\dim_S/\epsilon)} \right).
\end{equation}
We then have that the remaining integral can be bounded by
\begin{align}
    -\dim_S\int_{-\infty}^{\Lambda}  \lambda^3 \frac{e^{-\lambda^2/2}}{\sqrt{2\pi}} \sum_{k=0}^{\lfloor \dim_S/2-1\rfloor} \binom{\dim_S-1}{2k+1}\frac{{\rm erfc}(\lambda/\sqrt{2})^{2k+1}}{2^{2k+1}} d\lambda&=  \int_{-\infty}^\Lambda \lambda^3 {\rm Pr}(\lambda_{max}(G) = \lambda) d\lambda \nonumber\\
    &\le \Lambda
\end{align}
The value of $\Lambda$ is shown above to be logarithmic in the system dimension, so we can choose $\epsilon \in \Theta(1)$ and not significantly change the value of the integral and hence
\begin{equation}
    \int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{max}(G) = \lambda) d\lambda \in O(\log^{3/2}(\dim_S)).
\end{equation}
From the fact that the probability distribution for the eigenvalues is symmetric about $0$ we have that
\begin{equation}
    \int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\min}(G) = \lambda) d\lambda = - \int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\max}(G) = \lambda) d\lambda
\end{equation}
Thus we have 
\begin{align}
    \int_{-\infty}^\infty \|G\|^3 dG &\le \left|\int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\min}(G) = \lambda) d\lambda \right| + \left|\int_{-\infty}^\infty \lambda^3 {\rm Pr}(\lambda_{\max}(G) = \lambda) d\lambda \right|\nonumber\\
    &\in O\left(\log^{3/2}(\dim_S) \right)
\end{align}
Finally after inserting this into~\eqref{eq:remTaylorBd} we find that the truncation error for the channel at second order in $\alpha$ is
\begin{equation}
    \norm{\Phi(\rho) - \left(\rho + {\alpha} \frac{\partial}{\partial \alpha} \Phi(\rho) \bigg|_{\alpha = 0} + \frac{\alpha^2}{2!} \frac{\partial^2}{\partial \alpha^2} \Phi(\rho) \bigg|_{\alpha = 0} \right)} \in O(\alpha^3 t^3 \log^{3/2}({\rm dim}_S)).
\end{equation}
This gives us that the remainder bound, $\|R_\Phi\|\le \epsilon_R$ for 
\begin{equation}
    \alpha t \in O\left(\frac{\epsilon_R^{1/3}}{\sqrt{\log(\dim_S)}} \right).
\end{equation}
\end{proof}

\section{Single Qubit}
Now that we have all the tools we need to study thermalization we will tackle a single qubit first. Studying a single qubit will allow us to streamline the arguments for thermalization and make the analysis for more general systems easier. To start we take an arbitrary two-level Hamiltonian $H_S = U_H \begin{bmatrix} \lambda & 0 \\0 & \lambda + \Delta \end{bmatrix} U_H^\dagger$, where the eigenvectors are any arbitrary basis for the space and $0 < \Delta$. For simplicity we will set our environment gap to match this exactly, $\gamma = \Delta$, and we will use $\gamma$ to help the reader identify which factors appear from the ancilla qubit versus the system. Our initial state will be the maximally mixed state $\frac{1 }{2} \identity$ and the environment qubits will be prepared in the state $\frac{e^{-\beta H_E}}{1 + e^{-\beta}}$, note that the subscript of $E$ has been dropped from the inverse temperature as there is only one temperature at play. Our goal for this section will be to show that the thermal state $\rho_S(\beta)$ is the fixed point of the dynamics and to bound the rate of convergence to this state.

We will jump straight into the repeated interactions with our first step being to isolate the Markovian dynamics. We will remove the off-resonance terms from the 
\begin{align}
    \norm{\rho_S(\beta) - \Phi^{\circ L}(\rho_S(0))}_1 &\le \norm{\rho_S(\beta) - (\identity + \mathcal{T}_{\on})^{\circ L}(\rho_S(0))}_1 + \norm{(\identity + \mathcal{T}_{\on})^{\circ L}(\rho_S(0)) - \Phi^{\circ L}(\rho_S(0)) }_1 \\
    &\le \norm{\rho_S(\beta) - (\identity + \mathcal{T}_{\on})^{\circ L}(\rho_S(0))}_1 + L \left(\norm{\mathcal{T}_{\off}}_1 + \norm{R_{\Phi}}_1 \right) \\
    &= \norm{\vec{p}_{\beta} - (I + T)^L \vec{e} / 2}_1 + L \left(\norm{\mathcal{T}_{\off}}_1 + \norm{R_{\Phi}}_1 \right),
\end{align}
where $\vec{e}$ is the all ones vector and corresponds to the non-normalized maximally mixed state. In order to show that the overall distance is less than $\epsilon$ we will divide our error budget into $\epsilon / 2$ for the Markov chain error and $\epsilon / 2$ for the remainder and off-resonance terms. As bounding the contributions of the remainder and off-resonance terms depends on a specific value for $L$ we will approach the Markov chain distance first.

To compute the bound on $L$ we first need to compute the on-resonance terms $\mathcal{T}_{\on}$ so that we can compute the stochastic matrix $I + T$. For our two-level system the on-resonance terms are given by
\begin{align}
    \bra{0} \mathcal{T}_{\on}(\ketbra{0}{0}) \ket{0} &= - \widetilde{\alpha}^2 \frac{e^{-\beta}}{1 + e^{-\beta}} \\
    \bra{1} \mathcal{T}_{\on}(\ketbra{0}{0}) \ket{1} &=  \widetilde{\alpha}^2 \frac{e^{-\beta}}{1 + e^{-\beta}} \\
    \bra{0} \mathcal{T}_{\on}(\ketbra{1}{1}) \ket{0} &=  \widetilde{\alpha}^2 \frac{1}{1 + e^{-\beta}} \\
    \bra{1} \mathcal{T}_{\on}(\ketbra{1}{1}) \ket{1} &= - \widetilde{\alpha}^2 \frac{1}{1 + e^{-\beta}}.
\end{align}
This allows us to compute the Markov transition matrix as
\begin{align}
    I + T &= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \frac{\widetilde{\alpha}^2}{1 + e^{-\beta}} \begin{bmatrix} -e^{-\beta} & 1 \\ e^{-\beta} & - 1 \end{bmatrix}.
\end{align}
We first want to show that the thermal state is indeed the fixed point of the process. To do so note that if $(I + T) \vec{\pi} = \vec{\pi}$ then $T \vec{\pi} = 0$, so we just need to show that the thermal state is in the kernel of $T$. 
\begin{align}
\frac{\widetilde{\alpha}^2}{1 + e^{-\beta}} \begin{bmatrix} -e^{-\beta} & 1 \\ e^{-\beta} & - 1 \end{bmatrix} \begin{bmatrix}
    1/(1 + e^{-\beta}) \\ e^{-\beta} / (1 + e^{-\beta}) 
\end{bmatrix} = 0,
\end{align}
which is easily verifiable. The eigenvalues are also fairly straightforward to compute as $1$ and $1 - \widetilde{\alpha}^2$, leading to a gap of $\widetilde{\alpha}^2$. Plugging this into Theorem \ref{thm:markov_chain_bound} yields a lower bound on $L$ as 
\begin{equation}
    L \ge \frac{2}{\widetilde{\alpha}^2} \left( 2 \log \frac{1}{\widetilde{\alpha}^2} + 4( 1 + \log 2) - \frac{1}{2} + \log \frac{2}{\epsilon} \right).
\end{equation}
Now as $L$ needs to be an integer we could use the ceiling of the RHS of the above and that would be sufficient, we will ignore these rounding issues in the rest of the analysis as it does not affect the asymptotics. 

Now that we have a bound on $L$ we can use this to bound the remaider and off-resonance error terms. We will further divide our error-budget for these terms as $\epsilon / 4$ for the two respectively. The off-resonance terms are given as 
\begin{align}
    \norm{\mathcal{T}_{\off}(\rho_S(0))}_1 &=  \frac{1}{2} \left| \bra{0} \mathcal{T}_{\off}(\identity) \ket{0} \right| + \frac{1}{2} \left| \bra{1} \mathcal{T}_{\off}(\identity) \ket{1} \right| \\
    &= \widetilde{\alpha}^2 (q(0) - q(1)) \sinc^2( \Delta t ) \\
    &\le \widetilde{\alpha}^2 \epsilon_{\sinc} \\
    &\le \frac{4}{5} \frac{\alpha^2}{\Delta^2},
\end{align}
where we used Lemma ? to compute the specific matrix elements of $\mathcal{T}_{\off}(\identity)$. In order to have the total off-resonance error meet our budget we require
\begin{align}
    L \norm{\mathcal{T}_{\off}}_1 \le \frac{4}{5} \frac{\alpha^2}{\Delta^2} \left(\frac{2}{\widetilde{\alpha}^2}\left(  2 \log \frac{1}{\widetilde{\alpha}^2} + 4( 1 + \log 2) - \frac{1}{2} + \log \frac{2}{\epsilon} \right) + 1 \right) &\le \epsilon / 4 \\
    \frac{16}{5} \frac{1}{\Delta^2 \epsilon} \left(  2 \log \frac{1}{\widetilde{\alpha}^2} + 4( 1 + \log 2) - \frac{1}{2} + \log \frac{2}{\epsilon} \right) + \frac{4}{5} \frac{\alpha^2}{\Delta^2} \le t^2.
\end{align}
Now we note that this is not a proper inequality, as $\widetilde{\alpha}^2$ depends on $t$ and we cannot treat it as independent of $t$ due to the presence of $\alpha^2 / \Delta^2$.  
\begin{align}
    \mathcal{T}(X) &= \mathcal{T}_{\on}(X) + \mathcal{T}_{\off}(X) \\
    \bra{j}\mathcal{T}_{\on}(\ketbra{i}{i}) \ket{j} &= \begin{cases}
        \widetilde{\alpha}^2 \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2\left(\frac{(\Delta_S(i,j) + \gamma)t}{2}\right) & i < j \text{ and } |\Delta_S(i,j) - \gamma| \le \Delta_{\min} \\
        \widetilde{\alpha}^2 \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2\left(\frac{(\Delta_S(i,j) - \gamma)t}{2}\right) & i > j \text{ and } |\Delta_S(i,j) - \gamma| \le \Delta_{\min} \\
        - \sum_{k \neq i} \bra{k} \mathcal{T}_{\on}(\ketbra{i}{i})\ket{k} & i = j
    \end{cases} \\
    \bra{j}\mathcal{T}_{\off}(\ketbra{i}{i}) \ket{j} &= \begin{cases}
        \widetilde{\alpha}^2 \left( \sinc^2(\Delta_S(i,j) t/2) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2\left( \frac{(\Delta_S(i,j) - \gamma) t}{2}\right) \right) & i < j \\
        \widetilde{\alpha}^2 \left( \sinc^2(\Delta_S(i,j) t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2\left( \frac{(\Delta_S(i,j) + \gamma) t}{2}\right) \right) & i > j \\
        -\widetilde{\alpha}^2 \sum_{k \neq i}\sinc^2\left(\frac{\Delta_S(k, i)t}{2}\right) - \widetilde{\alpha}^2\sum_{k \neq i} q(0) \sinc^2 \left(\frac{(\Delta_S(k, i) + \gamma)t}{2} \right) + q(1) \sinc^2 \left(\frac{(\Delta_S(k, i) - \gamma)t}{2} \right) & i = j
    \end{cases},
\end{align}
where we note that all of the off-diagonal contributions require that $|\Delta_S(i,j) - \gamma | \ge \Delta_{\min}$ and $|\Delta_S(k,j) - \gamma | \ge \Delta_{\min}$, we just ran out of space to impose the condition inline. Now what we can do is remove the $\mathcal{T}_{\off}$ terms from the distance upper bound by including them with the remainder term. 


Now we can use the matrix distance Lemma to argue that $\norm{\rho_S(\beta) - (\identity + \mathcal{T}_{\on})^{\circ L}(\rho_S(0))}_1 = \norm{p_{\beta} - (I + T_{\on})^L e}_1$. Then by showing that $I + T_{\on}$ is a stochastic matrix and that $p_{\beta}$ is the unique fixed point all we need to do is bound the number of interactions $L$ to be above the relaxation time of the Markov chain to upper bound the distance. Computing the relaxtion time requires a lower bound on the spectral gap of $I + T_{\on}$, which is easily computed as $1 - (1 - \widetilde{\alpha}^2) = \widetilde{\alpha}^2$ for the $2 \times 2$ case. Then it will be sufficient for us to use a Theorem on relaxation times of irreducible, aperiodic markov chains:


Now we can construct a pattern for proving convergences. We use the reduction of the channel distance to the Markov chain distance and the remainder contributions (with off-resonance terms). Then we can show that the remainder distance is made small enough by taking $\alpha^2 t^2$ small enough and $\alpha$ as well. Then we bound the Markov chain distance by using the relaxation time argument above. Then once we have a computation or lower bound on the spectral gap of the Markov chain we can give a lower bound on the number of iterations needed and the only remaining work to be done is making sure the lower bound on $L$ is consistent with the bounds on the remainder error.

\subsubsection{For single qubit}
Now I'm going to do the above theorem for a single qubit channel. The first computation is for the off-resonance terms:

Now the remainder term is a bit of a hassle because its all in asymptotic notation so we can't get exact bounds with the current proof. It might be nice to be able to extend the above proof to work regardless of system so that way we don't have to replicate it.

Now we have to analyze the Markov chain. For the $2 \times 2$ case we have
\begin{align}
    T = \begin{bmatrix}
        \bra{0}\mathcal{T}_{\on}(\ketbra{0}{0}) \ket{0} & \bra{0}\mathcal{T}_{\on}(\ketbra{1}{1}) \ket{0} \\
        \bra{1}\mathcal{T}_{\on}(\ketbra{0}{0}) \ket{1} & \bra{1}\mathcal{T}_{\on}(\ketbra{1}{1}) \ket{1} 
    \end{bmatrix} = \widetilde{\alpha}^2 \begin{bmatrix} 
        - q(1) & q(0) \\ q(1) & - q(0)
    \end{bmatrix}.
\end{align}
We can then solve for the eigenvalue gap of $I + T$ straightforwardly as $\widetilde{\alpha}^2$. This matrix can be seen to have a stationary state of $q(0) e_0 + q(1) e_1$, which is the thermal state as we expect. This allows us to solve for the minimum number of interactions, using Theorem \ref{thm:markov_chain_bound},

This allows us to argue that $\norm{\rho_S(\beta) - (\identity + \mathcal{T}_{\on})^{\circ L}(\rho_S(0)) }_1 \le \frac{\epsilon}{2}$. By setting $L$ to the ceiling of the above bound we can then compute the bounds necessary for the remainder terms. We split the remaining $\epsilon / 2$ error into half for the off-resonance term and the remainder term. For the off-resonance term we need the following inequality satisfied 
\begin{align}
    L \norm{\mathcal{T}_{\off}}_1 \le \frac{4}{5} \frac{\alpha^2}{\Delta^2}\frac{2}{\widetilde{\alpha}^2}\left(  2 \log \frac{1}{\widetilde{\alpha}^2} + 4( 1 + \log 2) - \frac{1}{2} + \log \frac{2}{\epsilon} \right) &\le \epsilon / 4 \\
    \frac{16}{5} \frac{1}{\Delta^2 \epsilon} \left(  2 \log \frac{1}{\widetilde{\alpha}^2} + 4( 1 + \log 2) - \frac{1}{2} + \log \frac{2}{\epsilon} \right) \le t^2.
\end{align}
So up to logarithmic factors this is satisfied when $t \in \Omega \left( \frac{1}{\Delta \sqrt{\epsilon}} \right)$. The remainder term is asymptotically negligible so long as $\widetilde{\alpha} \in \Theta \parens{\frac{\epsilon^{1/3}}{\sqrt{\log(\dim_S)}}}$. Since we have bounds on $t$ already this can be thought of as a bound on $\alpha$ but it is up to user preference. 

One major observation is that the convergence time does not depend on $\beta_E$, which is rather odd. Further the overlap of the initial state $\identity / 2 \mapsto [1/2, 1/2]$ for the corresponding classical state with the target state $p_{\beta} = \frac{1}{1 + e^{-\beta}} e_0 + \frac{e^{-\beta}}{1 + e^{-\beta}} e_1$ is computed $$p_{\beta}^T \vec{e} = \begin{bmatrix}
    \frac{1}{1 + e^{-\beta}} & \frac{e^{-\beta}}{1 + e^{-\beta}}
\end{bmatrix} \cdot \begin{bmatrix}
    \frac{1}{2} \\ \frac{1}{2} \end{bmatrix} = \frac{1}{2.}$$

\subsection{Attempt at perturbation theory}

The above highlights our main problem, which is to anlyze the matrix $I + T$. We start by computing the entries of $T$. To compute these we will make use of Lemma \ref{lem:t_2_system_only}, specifically Eq. \eqref{eq:t_2_same_state_2}. Within this expression there is typically one ``on-resonance" term that we would like to isolate and the remainder are small junk terms. We will eventually treat these junk terms as a perturbation for the on-resonance terms, so we collect them into a $J$ matrix.
\begin{align}
    e_0^T T e_0 &= \bra{0} \mathcal{T}(\ketbra{0}{0}) \ket{0} \\
    &= - \widetilde{\alpha}^2 \frac{e^{-\beta}}{1 + e^{-\beta}}\sinc^2((\Delta - \gamma)t/2) - \widetilde{\alpha}^2 \left(\sinc^2(\Delta t/ 2) +  \frac{1}{1 + e^{-\beta}}\sinc^2((\Delta + \gamma)t/2) \right) \\
    &= -\widetilde{\alpha}^2 q(1) - J_{0,0}.
\end{align}
For the remaining terms we have
\begin{align}
    e_0^T T e_1 &=  \widetilde{\alpha}^2 q(1) + J_{0,1} \\
    J_{0,1} &= \widetilde{\alpha}^2 (\sinc^2(\Delta t/2) + q(0) \sinc^2((\Delta + \gamma)t/2)) \\
    e_1^T T e_0 &= \widetilde{\alpha}^2 q(0) + J_{1, 0} \\
    J_{1, 0} &= \widetilde{\alpha}^2 (\sinc^2(\Delta t / 2) + q(1) \sinc^2((\Delta + \gamma)t/2)) \\
    e_1^T T e_1 &= - \widetilde{\alpha}^2 q(0) + J_{1, 1} \\
    J_{1,1} &= -\widetilde{\alpha}^2 (\sinc^2(\Delta t/ 2) + q(1) \sinc^2((\Delta + \gamma)t/2)).
\end{align}

This means for our single qubit case we can write the dynamics as
\begin{align}
    I + T &= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \widetilde{\alpha}^2 \begin{bmatrix} -q(1) & q(0) \\ q(1) & - q(0) \end{bmatrix} + J \\
    &= \begin{bmatrix} 1 - \widetilde{\alpha}^2 q(1) & \widetilde{\alpha}^2 q(0) \\ \widetilde{\alpha}^2 q(1) & 1 - \widetilde{\alpha}^2 q(0) \end{bmatrix} + J.
\end{align}
We have isolated $J$ as each of its terms are upper bounded in magnitude by $\frac{\alpha^2}{\Delta^2}$, so we treat it as a perturbation to the main dynamics. For the $2 \times 2$ case we can solve the main dynamics exactly, the eigenvalues are 1 and $1 - \widetilde{\alpha}^2$, leading to a gap of $\widetilde{\alpha}^2$ with corresponding ($L_1$ normalized) eigenvectors
\begin{align}
    v_1 &= \frac{1}{1 + e^{-\beta}} e_0 + \frac{e^{-\beta}}{1 + e^{-\beta}} e_1,  &(I + T - J) v_1 = v_1 \\
    v_2 &= \frac{-1}{2} e_0 + \frac{1}{2} e_1,  &(I + T - J) v_2 = (1 - \widetilde{\alpha}^2) v_2.
\end{align}

Now we can analyze the total dynamics of $I + T$ by treating $J$ as a perturbation, as the terms are off-resonance and controllably small. As we know the gap of the unperturbed matrix is $\widetilde{\alpha}^2$ and the matrix is diagonalizable we can use the following theorem 
\begin{align}
    \abs{\lambda - \hat{\lambda}} \le \kappa(S) \norm{J},
\end{align}
where $I + T - J = S \Lambda S^{-1}$ is the diagonalization of the ideal dynamics. We can compute $S$ via Gaussian Elimination but I dont want to right now. 

\section{Approximate Detailed Balance}
In classical dynamics thermalization can usually be shown through a Markov chain approach, where the state of a system is shown to be governed by a Markov chain and then a condition known as Detailed Balance allows one to argue that the chain eventually converges to a steady state. Fixed point theorems, such as the Perron-Frobenius theorem, are then used to show when the fixed point is unique and spectral gap bounds on the walk operator of the Markov chain show rates of convergence to this fixed point. For the case of our quantum system we are not dealing with probability distributions exactly, but our quantum states are well approximated by thermal state density matrices, which are probability distributions over the quantum eigenstates of the system Hamiltonian. We show that for the thermalizing channel we have studied the probability distribution over eigenstates satisfies detailed balance in the appropriate limits, indicating that given enough interactions with fresh environment ancillas the state converges to the target thermal state $\rho_S(\beta_E)$.

The conditions for Detailed Balance are straightforward, we assume a Markov chain $\prob{i \to j }$ and assume a distribuion $\prob{i}$. Detailed Balance is said to hold if the following equality holds
\begin{equation}
    \prob{i \to j} \prob{i} = \prob{j \to i} \prob{j}.
\end{equation}
This leads to $\prob{i}$ being a fixed point of the Markov chain fairly straightforwardly, using a subscript of $k$ to denote the probability at the $k$\ts{th} iteration,
\begin{align}
    \prob{i}_k &= \sum_{j} \prob{j \to i} \prob{j}_{k - 1} \\
    &= \sum_j \prob{i \to j} \prob{i}_{k - 1} \\
    &= \prob{i}_{k - 1}.
\end{align}
Ergodicity of the chain is needed to show that this distribution is the unique fixed point. What we would like to show is that our channel satisfies detailed balance for the target thermal state in the appropriate limits of $\alpha \to 0$ and $t \to \infty$, but keeping the product $\alpha t$ a small constant value.


In this section we show the most mild form of thermalization for all possible system Hamiltonians $H_S$. What we will show is that if $\Phi$ has a fixed point, then it will be arbitrarily close to the fixed point of the $\bigo{(\alpha t)^2}$ approximation to the channel $\Phi$. Then we show that the fixed point of this approximation, in the limit as $t \to \infty$, $\alpha \to 0$, and we have perfect knowledge of the distribution of eigenvalue differences $\Delta_S(i,k)$, is the thermal state $\rho_S(\beta_E)$. We say this is the most mild form of thermalization as it provides no guarantees for how long it takes the system to reach this fixed point, or even how long it takes to get close to it's fixed point. 

Given that we are able to rotate the basis we work with into the System Hamiltonian's eigenbasis, and moreover we previously showed that off-diagonal coherences in this basis are negligible, we can treat this process as a stochastic one. Explicitly, if we give a diagonal state as input to the channel $\Phi$, to order $\bigo{\alpha^2}$ we get a diagonal state back. Since diagonal states can be thought of as probability distributions, the map $\Phi$ can roughly be thought of as a Markov process on the eigenstates of the Hamiltonian $H_S$. Since our goal is to show that this map produces a thermal state,
we would need to show that this probability distribution converges to the Boltzmann distribution $e^{-\beta_E \lambda_S(i)} / \partfun_S(\beta_E)$, for state $i$. In classical probability sampling literature this is done through a Detailed Balance calculation, which shows that the desired distribution is a fixed point. Ergodicity arguments of the desired Markov chain are then used to claim that the fixed point is unique and that therefore the process produces samples of the desired distribution.

Given that our map is only approximate, we cannot make rigorous claims regarding detailed balance at fixed $\alpha, t,$ or $\gamma$. However, in this section we show that in the appropriate limits that our mapping satisfies Detailed Balance exactly. This is enough to give solid evidence that for appropriate regimes our channel should converge approximately to this goal distribution.
\begin{align}
    &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \nonumber \\
    &\approx \frac{\alpha^2 t^2}{\dim + 1} \parens {\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}  \sinc^2((\Delta(i,j) + \gamma)t) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 ((\Delta(i,j) - \gamma)t) + 2 \sinc^2(\Delta(i,j) t)} \nonumber
\end{align}

\begin{align}
    &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \nonumber \\
    &= \bra{j} \Phi_{\gamma}(\ketbra{i}{i}) \ket{j} \\
    &= \braket{j}{i}\braket{i}{j} + \sum_{k,l} \tau(i, k | j, l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} + \bra{j} R_{\Phi}(\ketbra{i}{i})\ket{j}
\end{align}
Now as we are studying Detailed Balance, we assume that $i \neq j$. For our purposes, without loss of generality we let $\lambda_S(i) \geq \lambda_S(j)$, so by transitioning from $i \to j$ we are losing energy to the environment. We simplify the non-trivial term from above as
\begin{align}
    &\sum_{k,l} \tau(i,k| j,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} = \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)}(\tau(i,0|j,0) + \tau(i,0|j,1)) + \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} (\tau(i,1|j,0) + \tau(i,1|j,1) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} (\sinc^2(\Delta_S(i,j)t/2) + \sinc^2((\Delta_S(i,j) - \gamma)t/2)) \nonumber \\
    &\quad \quad \quad \quad \quad \quad  +\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} (\sinc^2((\Delta_S(i,j) + \gamma)t/2) + \sinc^2(\Delta_S(i,j) t/2)) \bigg) \\
    &\frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) + \sinc^2(\Delta_S(i,j)t/2)
\end{align}
As $\Delta_S(i,j) \geq 0 $ we expect that only the $\sinc^2((\Delta_S(i,j) - \gamma) t/2)$ term will contribute significantly to this sum. We can similarly write down the probability of the state to transition from $j \to i$ as
\begin{align}
    \prob{\text{System transition } j \to i | \text{ env at } \beta_E} = \sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} + \bra{i} R_{\Phi}(\ketbra{j}{j})\ket{i},
\end{align}
where we write the non-trivial term as
\begin{align}
    \sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} = \frac{\alpha^2 t^2}{\dim + 1} \bigg(&\frac{1}{1 + e^{-\beta_E \gamma}} (\sinc^2(\Delta_S(j,i)t/2) + \sinc^2((\Delta_S(j,i) - \gamma)t/2)) \nonumber \\
    &+\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} (\sinc^2((\Delta_S(j,i) + \gamma)t/2) + \sinc^2(\Delta_S(j,i) t/2)) \bigg).
\end{align}
We simplify this by noting $\Delta_S(i,j) = - \Delta_S(j,i)$ and that $\sinc^2(x) = \sinc^2(-x)$ to get
\begin{align}
&\sum_{k,l} \tau(j,k|i,l) \frac{e^{-\beta_E \lambda_E(k)}}{\partfun_E(\beta_E)} \\
&= \frac{\alpha^2 t^2}{\dim + 1} \bigg(\frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) + \sinc^2(\Delta_S(i,j)t/2)
\end{align}

Now we would like to show some form of Detailed Balance, in appropriate limits, for the thermal state of the system. We will show that Detailed Balance holds in expectation, or with some non-zero probability. The thermal state give $\prob{\text{state in } i} = \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}$. We then need to expand differences
\begin{align}
    \chi(i,j) \coloneqq &\prob{\text{System transition } i \to j | \text{ env at } \beta_E} \prob{\text{state in } i} \nonumber \\
    &- \prob{\text{System transition } j \to i | \text{ env at } \beta_E} \prob{\text{state in } j}.
\end{align}
This expression is written in full glory as
\begin{align}
    \chi(i,j) &= \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \bra{j} R_{\Phi}(\ketbra{i}{i})\ket{j} \nonumber \\
    &+ \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \nonumber \\
    &+\frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma) t/2) \nonumber \\
    &+ \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\Delta_S(i,j)t/2) \nonumber \\
    &-\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \bra{i} R_{\Phi}(\ketbra{j}{j})\ket{i} \nonumber \\
    &- \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma) t/2) \nonumber \\
    &-\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \nonumber \\
    &- \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_S(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} \sinc^2(\Delta_S(i,j)t/2).
\end{align}
In order to simplify this, we will group these expressions with an end goal in mind. As we would like this to hold for all $i \neq j$ but we have a fixed $\gamma$, we need to randomly choose a $\gamma$ and show that this holds in expectation. Further, we would like to bound the absolute value of these differences. So we want $\mathbb{E}_{\gamma}\abs{\chi(i,j)}$. We can then use the triangle inequality, the fact that $\abs{\bra{j}R_{\Phi}(\ketbra{i}{i})\ket{j}} \leq \norm{R_{\Phi}}$, $\abs{\frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}} \leq 1$, 

along with the fact that transitions such as $\sinc^2(\Delta_S(i,j)t/2)$ are suppressed by $\epsilon_{\sinc}$ to get a simplified upper bound:
\begin{align}
    \mathbb{E}_{\gamma} \abs{\chi(i,j)} &\leq \mathbb{E}_{\gamma} 2 \norm{R_{\Phi}} \nonumber \\
    &+ \mathbb{E}_{\gamma} \frac{\alpha^2 t^2}{\dim + 1} 4 \epsilon_{\sinc}  \nonumber \\
    &+ \mathbb{E}_{\gamma} \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta_E \gamma}} \frac{1}{\partfun_S(\beta_E)} \abs{\parens{e^{-\beta_E \lambda_S(i)} - e^{-\beta_E \gamma} e^{-\beta_E \lambda_S(j)}} \sinc^2((\Delta_S(i,j) - \gamma) t/2)} \\
    &\leq 2 \norm{R_{\Phi}} + 4 \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \nonumber \\
    &+ \frac{\alpha^2 t^2}{\dim + 1} \mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \abs{1 - e^{\beta_E (\Delta_S(i,j) - \gamma)}}. \label{eq:detailed_balance_upper_bound}
\end{align}
This is where we have to introduce our distribution over $\gamma$. We note that if $\gamma$ is far away from $\Delta_S(i,j)$, then as $t \to \infty$ we have that this product is trivially 0. We want to show that Detailed Balance is satisfied even when $\gamma \to \Delta_S(i,j)$. To do so we upper bound $\sinc^2 \leq 1$ whenever $\abs{\Delta_S(i,j) - \gamma} \leq \Delta_{\min}$ and $\sinc^2 \leq \epsilon_{\sinc}$ whenever $\abs{\Delta_S(i,j) - \gamma} \geq \Delta_{\min}$. 

We now are at an impasse. Our goal for this argument is to show that if our channel does anything non-trivial, then in the appropriate limits ($t \to \infty, \alpha \to 0, \alpha t \to c_{small}$) it should satisfy detailed balance conditions, or at least get arbitrarily close to it. We see that when our channel has a $\gamma$ that is not close to any $\Delta_S(i,j)$ we do not induce any transitions among states (in the $t \to \infty$ limit). This then trivially satisfies Detailed balance, as $\prob{i \to j} = \prob{j \to i} = 0$ gives $0 = 0$ for Detailed Balance. We would like to show that all $i \neq j$ can get arbitrarily close to satisfying Detailed Balance by choosing a $\gamma$ randomly. To do so there are three obvious candidates for distributions of $\gamma$ that we could analyze theoretically:
\begin{enumerate}
    \item A maximum entropy prior, or choosing $\gamma$ uniformly from 0 to $\norm{H}$,
    \item A minimal entropy prior, or exact knowledge of $\Delta_S(i,j)$ for all $i,j$, where we choose indices or differences uniformly,
    \item Choose a difference $\Delta_S(i,j)$ uniformly and then add in noise, either in the form of a Gaussian or a simple uniform box centered about the gap $\Delta_S(i,j)$.
\end{enumerate}


We now analyze what happens if we are given perfect knowledge of the eigenvalue gaps of our system. In this distribution we pick an eigenvalue difference uniformly at random. We denote the set of eigenvalue gaps as $G_{\gamma}$. Then the expectation can be split into two: $S_{\gamma}$ being the set of gaps that are close to $\gamma$ and $T_{\gamma}$ as those that are far apart. Let $N_{diff}$ denote the number of differences. Specifically, let $S_{\gamma} = \set{\lambda_S(k) - \lambda_S(l) = \Delta_S(k,l) : \sinc^2((\Delta_S(i,j) - \Delta_S(k,l))t/2) \geq \epsilon_{\sinc}}$, and $T_{\gamma} = \set{\Delta_S(k,l) : \sinc^2((\Delta_S(i,j) - \Delta_S(k,l))t/2) < \epsilon_{\sinc}}$. Then the expected value over $\gamma$ becomes
\begin{align}
    &\mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma) t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \nonumber \\
    &=\frac{1}{N_{diff}}\sum_{\gamma \in S_{\gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} + \frac{1}{N_{diff}} \sum_{\gamma \in T_{\gamma}} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \\
    &\leq \frac{1}{N_{diff}} \sum_{\gamma \in S_{\gamma}} \abs{1 - e^{\beta_E(\Delta_S(i,j) -\gamma)}} + \frac{1}{N_{diff}} \epsilon_{\sinc} \sum_{\gamma \in T_{\gamma}}\abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}}.
\end{align}
We see that for the right hand sum the factor of $\epsilon_{\sinc}$, which vanishes as $t \to \infty$, causes the total sum to vanish as there are no explicit $t$ dependent terms. The set $T_{\gamma}$ does change with $t$, but it is upper bounded by a finite value and so is each possible summand. Now the real kicker is what happens to the leftmost summation. We investigate when an eigenvalue gap $\Delta_S(k,l)$ can be included in $S_{\gamma}$ as $t \to \infty$. 
\begin{align}
    \lim_{t \to \infty} \sinc^2((\Delta_S(i,j) - \Delta_S(k,l)) t/2) = \begin{cases}
        0 & \Delta_S(i,j) \neq \Delta_S(k,l) \\
        1 & \Delta_S(i,j) = \Delta_S(k,l).
    \end{cases}
\end{align}
Because of this $S_{\gamma} = \set{\Delta_S(i,j)}$ becomes a multiset consisting solely of $\Delta_S(i,j)$ with the number of degeneracies of the eigenvalue $S_{\gamma}$. This means that any term in the summation then becomes $\abs{1 - e^{\beta_E(\Delta_S(i,j) - \Delta_S(i,j)}} = 0$. Then given that $\epsilon_{\sinc} = 1/(\Delta_(\min)^2 t^2) \to 0$, we have that $\lim_{t \to \infty} \mathbb{E}_{\gamma} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} = 0$. Looking at Eq. \eqref{eq:detailed_balance_upper_bound}, along with $\epsilon_{\sinc} = 1/(\Delta_{\min}^2 t^2)$ and $\alpha = \epsilon_{\alpha} / t$, we see that 
\begin{align}
    &\lim_{t \to \infty} \mathbb{E}_{\gamma} \abs{\chi(i,j)} \leq 2 \lim_{t \to \infty} \norm{R_{\Phi}} + 4 \lim_{t \to \infty} \frac{\epsilon_{\alpha}^2}{t^2 \Delta_{\min}^2 (\dim + 1)} \nonumber \\
    &+ \frac{\epsilon_{\alpha}^2}{\dim + 1} \lim_{t \to \infty} \sinc^2((\Delta_S(i,j) - \gamma)t/2) \abs{1 - e^{\beta_E(\Delta_S(i,j) - \gamma)}} \\
    &= 2 \lim_{t \to \infty} \norm{R_{\Phi}}.
\end{align}
As 

\begin{align}
    \frac{\prob{i \to j}}{\prob{j \to i}} &\approx \frac{\frac{1}{1 + e^{-\beta_E \gamma}}\sinc^2 ((\Delta(i,j) - \gamma)t)+ 3 \epsilon_{\sinc}}{\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((-\Delta(i,j) + \gamma)t) + 3 \epsilon_{\sinc}}  \nonumber \\
    \lim_{t \to \infty} &\implies e^{\beta_E \gamma} = e^{-\beta_E \Delta(i,j)} \nonumber \\
    &= \frac{\prob{\text{System in } j}}{ \prob{\text{System in } i}} \nonumber
\end{align}

% \section{Two Level Systems}
% The system we would like to analyze the most is a single qubit with an energy gap $\Delta$. This is the smallest possible system and therefore should be the easiest for us to analyze directly. 




% \begin{lemma} \label{lem:single_qubit_one_interaction}
%     Let $H_S = \diag(\lambda_S(1), \lambda_S(2))$ be a qubit Hamiltonian such that $0 < \Delta \coloneqq \lambda_S(2) - \lambda_S(1)$. Given an environment in the Gibbs state $\rho_E(\beta_E) = \frac{e^{-\beta_E H_E}}{\partfun_E(\beta_E)}$ with $H_E = \diag(0, \Delta)$. In one interaction the system, initially given by a Gibbs state $\rho_S(\beta) = \frac{e^{-\beta H_S}}{\partfun_S(\beta)}$, is $\epsilon$ close in trace distance to the environment inverse temperature provided $\beta$ is sufficiently close. Defining $\delta \coloneqq \beta_E - \beta$ we have that the following conditions
% \begin{align}
%     \frac{\delta}{\beta_E} &\le 1 - \frac{1}{\beta_E \Delta}  \ln \left( \frac{1 - \widetilde{\alpha}^2 - \epsilon/ 6(1  + e^{-\beta_E \Delta})}{e^{-\beta_E \Delta}(1 - \widetilde{\alpha}^2) + \epsilon / 6 (1 + e^{-\beta_E \Delta})}\right), \\
%     \alpha &\leq \sqrt{\frac{ \epsilon \Delta (\dim + 1) }{48}} \\
%     \norm{R_{\Phi}}_1 &\leq \epsilon / 3,
% \end{align}
% imply that 
% $$\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \leq \epsilon.$$
% \end{lemma}
% \begin{proof}
% We first solve for our transition term, which has four elements
% \begin{align}
%     \bra{0} \mathcal{T}^{(2)}(\ketbra{0}{0}) \ket{0} &= - \frac{\alpha^2 t^2}{\dim + 1} \left( \sinc^2 \left( \frac{\Delta t}{2}\right) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(\Delta + \gamma) t}{2}\right) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(\Delta - \gamma) t}{2}\right) \right) \\
%     \bra{1} \mathcal{T}^{(2)}(\ketbra{0}{0})\ket{1} &= \frac{\alpha^2 t^2}{\dim + 1} \left( \sinc^2 \left( \frac{\Delta t}{2}\right) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(-\Delta - \gamma) t}{2}\right) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(-\Delta + \gamma) t}{2}\right)\right) \\
%     \bra{0} \mathcal{T}^{(2)}(\ketbra{1}{1})\ket{0} &= \frac{\alpha^2 t^2}{\dim + 1} \left( \sinc^2 \left( \frac{\Delta t}{2}\right) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(\Delta - \gamma) t}{2}\right) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(\Delta + \gamma) t}{2}\right)\right) \\
%     \bra{1} \mathcal{T}^{(2)}(\ketbra{1}{1}) \ket{1} &= - \frac{\alpha^2 t^2}{\dim + 1} \left( \sinc^2 \left( \frac{\Delta t}{2}\right) + \frac{1}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(-\Delta + \gamma) t}{2}\right) + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2 \left( \frac{(-\Delta - \gamma) t}{2}\right) \right) 
% \end{align}
% By setting $\gamma = \Delta$ we get only one non-negligible term in each one, leading to approximations. We will also use the shorthand $T_{i, j} = \bra{j}\mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j}$. However we will ignore these for now and study the rates of convergence for the terms exactly. 

% \begin{align}
%     &\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \le \sum_k \left| p_{\beta_E}(k) - p_{\beta}(k) - \sum_i p_{\beta}(i) \bra{k} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{k} \right| + \norm{R_{\Phi}}_1 \\
%     &=\left| p_{\beta_E}(0) - p_{\beta}(0) - (p_{\beta}(0) T_{0, 0} + p_{\beta}(1) T_{1, 0})\right| + \left| p_{\beta_E}(1) - p_{\beta}(1) - (p_{\beta}(0) T_{0, 1} + p_{\beta}(1) T_{1, 1})\right|+ \norm{R_{\Phi}}_1.
% \end{align}
% Now we 


% In order to simplify this further we will make heavy use of trace preservation. Conservation of probability allows us to write $p_{\beta_E}(1) - p_{\beta}(1) = 1 - p_{\beta_E}(0) - (1 - p_{\beta}(0)) = -(p_{\beta_E}(0) - p_{\beta}(0))$. The trace preserving nature of our channel (even to second order), along with direct inspection, shows that $T_{0, 0} + T_{0, 1} = 0$ and $T_{1, 0} + T_{1, 1} = 0$. This allows us to say $p_{\beta}(0) T_{0, 1} + p_{\beta}(1) T_{1, 1} = -(p_{\beta}(0) T_{0, 0} + p_{\beta}(1) T_{1, 0})$. Now we can clearly see that the second term in Eq. \eqref{eq:single_qubit_1} is equal to the first as the absolute value kills the overall minus sign. We therefore have 
% \begin{equation}
%     \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \le 2 \left|p_{\beta_E}(0) - p_{\beta}(0) - (p_{\beta}(0) T_{0,0} + p_{\beta}(1) T_{1, 0}) \right| + \norm{R_{\Phi}}_1. \label{eq:single_qubit_2}
%  \end{equation}

% Our task is clear, we now have to simplify $p_{\beta}(0) T_{0,0} + p_{\beta}(1) T_{1,0}$, which we proceed now. 
% \begin{align}
%     p_{\beta}(0) T_{0,0} + p_{\beta}(1) T_{1,0} = &\frac{\alpha^2 t^2}{\dim +1} \frac{1}{1 + e^{-\beta \Delta}} \sinc^2\left(\frac{\Delta t}{2}\right)(-1 + e^{-\beta \Delta}) \nonumber \\
%     +  &\frac{\alpha^2 t^2}{\dim +1} \frac{1}{1 + e^{-\beta \Delta}}\sinc^2\parens{\frac{(\Delta + \gamma) t}{2}} \left(-\frac{1}{1 + e^{-\beta_E \gamma}} + e^{-\beta \Delta} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \right) \nonumber \\
%     +  &\frac{\alpha^2 t^2}{\dim +1} \frac{1}{1 + e^{-\beta \Delta}}\sinc^2\parens{\frac{(\Delta - \gamma) t}{2}} \left(-\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} + \frac{e^{-\beta \Delta}}{1 + e^{-\beta_E \gamma}} \right). \label{eq:single_qubit_4}
% \end{align}
% Now we note that the first two terms in the above expression, with factors of $\sinc^2(\Delta t/ 2)$ and $\sinc^2((\Delta + \gamma)t/2)$ can be made as negligible as we want by extending $t$. So we can pull these terms out of the absolute value using the triangle inequality and upper bound the probability factors by 1. This results in
% \begin{align}
%     &\left| p_{\beta_E}(0) - p_{\beta}(0) - (p_{\beta}(0) T_{0, 0} + p_{\beta}(1) T_{1, 0})\right| \nonumber \\
%     &\le \left| \frac{1}{1 + e^{-\beta_E \Delta}} - \frac{1}{1 + e^{-\beta \Delta}} \left(1 + \frac{\alpha^2 t^2}{\dim + 1} \left(-\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} + \frac{e^{-\beta \Delta}}{1 + e^{-\beta_E \gamma}} \right) \right) \right| + 2 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
% \end{align}
% To approach this, we note that $p_{\beta_E}(0) \ge p_{\beta}(0)$, which implies that there is a small enough value of $\alpha^2 t^2$ such that we can just remove the absolute value. After doing this and plugging in to Eq. \eqref{eq:single_qubit_2} our inequality to prove becomes
% \begin{align}
%     \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \le \frac{2}{1 + e^{-\beta_E \Delta}} -  \frac{2}{1 + e^{-\beta \Delta}} \left(1 + \frac{\alpha^2 t^2}{\dim + 1} \left(-\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} + \frac{e^{-\beta \Delta}}{1 + e^{-\beta_E \gamma}} \right) \right) + 4 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + \norm{R_{\Phi}}_1. \label{eq:single_qubit_3}
% \end{align}
% As we would like the RHS to be less than $\epsilon$ we take the standard approach of dividing our error budget up equally among the terms of interest. For us we have three pieces, $\epsilon / 3$ for the left two terms that constitute the ``contraction", $\epsilon /3 $ for the fixed-point error, and $\epsilon / 3$ for the remainder bound. This will be the focus for the remainder of the proof.

% We start with the simplest bound, the fixed-point error budget. For this we only require $16 \frac{\alpha^2}{(\dim + 1) \Delta} \le \epsilon / 3$, which is easily converted into an upper bound on $\alpha$ that scales like $\bigo{\sqrt{\epsilon}}$. The contraction terms are a bit trickier, we first will simplify them with a bit of algebra
% \begin{align}
%     \frac{1}{1 + e^{-\beta_E \Delta}} - \frac{1}{1 + e^{-\beta \Delta}} \left(1 + \frac{\alpha^2 t^2}{\dim + 1} \left(-\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} + \frac{e^{-\beta \Delta}}{1 + e^{-\beta_E \gamma}} \right) \right) &\le \epsilon /6 \\ 
%     \frac{1}{1 + e^{-\beta_E \Delta}} - \frac{1}{1 + e^{-\beta \Delta}} + \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + e^{-\beta \Delta}} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} - \frac{\alpha^2 t^2}{\dim + 1} \frac{e^{-\beta \Delta}}{1 + e^{-\beta \Delta}} \frac{1}{1 + e^{-\beta_E \gamma}} &\le \epsilon / 6 \\
%     \frac{1}{1 + e^{-\beta \Delta}}\left(-1 + \frac{\alpha^2 t^2}{\dim + 1} \right) + \frac{1}{1 + e^{-\beta_E \Delta}} - \frac{\alpha^2 t^2}{\dim  +1 } \frac{1}{1+ e^{-\beta_E \gamma}} &\leq \epsilon / 6 \\
% \end{align}
%     Now we substitute in $\Delta$ for $\gamma$, as we only left it around to help with organizing terms, and solve for $\beta$ as there is only one term involving it
% \begin{align}
%     \frac{1}{1 + e^{-\beta_E \Delta}} -\frac{1}{1 + e^{-\beta \Delta}} &\leq \frac{\epsilon}{6(1 - \frac{\alpha^2 t^2}{\dim + 1})} \\
%     \frac{1}{1 + e^{-\beta_E \Delta}} - \frac{\epsilon}{6(1 - \frac{\alpha^2 t^2}{\dim + 1})} &\leq \frac{1}{1+e^{-\beta \Delta}} \\
%     1 + e^{-\beta \Delta } &\leq \left( \frac{1}{1 + e^{-\beta_E \Delta}} - \frac{\epsilon}{6(1 - \frac{\alpha^2 t^2}{\dim + 1})} \right)^{-1} \\
%     \beta &\geq \frac{1}{\Delta} \ln \left( \frac{1 - \frac{\alpha^2 t^2}{\dim + 1} - \epsilon/ 6 - \epsilon/6 e^{-\beta_E \Delta}}{e^{-\beta_E \Delta}(1 - \frac{\alpha^2 t^2}{\dim + 1} + \epsilon/6) + \epsilon / 6} \right).
% \end{align}
% It may not be pretty, but this is the best that we can simplify things. We see that as $\beta_E$ gets very large the log approaches $\frac{2 - 2\alpha^2 t^2 / (\dim + 1) - \epsilon}{\epsilon}$. Or another way we can phrase this is by recasting our variables from $\beta$ and $\beta_E$ to $\beta_E$ and $\delta \coloneqq \beta_E - \beta$. This then shows that 
% \begin{equation}
%     \delta \le \beta_E - \frac{1}{\Delta}  \ln \left( \frac{1 - \frac{\alpha^2 t^2}{\dim + 1} - \epsilon/ 6 - \epsilon/6 e^{-\beta_E \Delta}}{e^{-\beta_E \Delta}(1 - \frac{\alpha^2 t^2}{\dim + 1} + \epsilon/6) + \epsilon / 6} \right).
% \end{equation}
% However this screams to be reinterpreted in terms of $\delta / \beta_E$
% \begin{equation}
%     \frac{\delta}{\beta_E} \le 1 - \frac{1}{\beta_E \Delta}  \ln \left( \frac{1 - \frac{\alpha^2 t^2}{\dim + 1} - \epsilon/ 6 - \epsilon/6 e^{-\beta_E \Delta}}{e^{-\beta_E \Delta}(1 - \frac{\alpha^2 t^2}{\dim + 1} + \epsilon/6) + \epsilon / 6} \right),
% \end{equation}
% and now the RHS only depends on the product $\beta_E \Delta$ and the Lemma is proved.
% \end{proof}

% Now I know that the above proof works. How well does a cooling schedule work?


% As a sanity check, as $\epsilon \to 0$ we should observe a much smaller range of temperatures that can cool in a single step to trace distance less than $\epsilon$, directly evaluating the limit above shows
% \begin{equation}
%     1 - \frac{1}{\beta_E \Delta} \ln \frac{1 - \widetilde{\alpha}^2}{e^{-\beta_E \Delta} (1 - \widetilde{\alpha}^2)} = 0,
% \end{equation}
% as expected. The other limit that is of interest is the $\widetilde{\alpha^2} \to 0$ limit, because in this case our channel does not do anything. In this limit we should also expect that $\delta$ should be small as well, but there may be a natural ``thermal radius" around $\beta_E$ that is within distance $\epsilon$
% \begin{align}
%     1 - \frac{1}{\beta_E \Delta} \ln \frac{1 - \epsilon /2 - \epsilon / 2 e^{-\beta_E \Delta}}{e^{-\beta_E \Delta}(1 + \epsilon / 2) + \epsilon/ 2} &= 1 - \frac{1}{\beta_E \Delta} \ln \frac{1 - \epsilon /2(1 + e^{-\beta_E \Delta})}{e^{-\beta_E \Delta} + \epsilon/ 2(1 + e^{-\beta_E \Delta}) }\\
%     &= 1 - \frac{1}{\beta_E \Delta} \ln \frac{1 - \epsilon /2}{\frac{e^{-\beta_E \Delta}}{1 +e^{-\beta \Delta}} + \epsilon/ 2 }.
% \end{align}
% I'm not sure how to interpret this one, but I do know its nonzero which is what we expect for nonzero $\epsilon$.

% \subsection{Repeated Interactions}

%  We prove this via induction. The base case of $L=1$ is true via our Taylor series decomposition of $\Phi$, so in order to give a sense of how the induction step works we will prove the $L=2$ scenario as our base case.    
%  \begin{align}
%     &\norm{X - \Phi \circ \Phi (Y)}_1\\
%     & = \norm{X - \Phi(Y) - \Phi(\mathcal{T}^{(2)}(Y) - \Phi(R_{\Phi}(Y))}_1 \\
%     &\leq \norm{X - Y - \mathcal{T}^{(2)}(Y) - \mathcal{T}^{(2)}(Y) - \mathcal{T}^{(2)}(\mathcal{T}^{(2)}(Y))}_1 + \norm{R_{\Phi}(Y)}_1 + \norm{\Phi(R_{\Phi}(Y))}_1 \\
%     &\leq \norm{X - Y - 2 \mathcal{T}^{(2)}(Y)}_1 + \norm{\mathcal{T}^{(2)} \circ \mathcal{T}^{(2)}(Y)}_1 + 2 \norm{R_{\Phi}}_1.
% \end{align}
% Now we know that $\bra{i}\mathcal{T}^{(2)}(\ketbra{k}{l})\ket{j} \in \bigo{\widetilde{\alpha}^2}$, so that means $\bra{i}\mathcal{T}^{(2)}\circ \mathcal{T}^{(2)}(\ketbra{k}{l})\ket{j} \in \bigo{\widetilde{\alpha}^4}$. So we get at most an extra factor of $\dim_S$ from the Schatten 1-norm but I don't think it really matters.  
% Now we prove the induction step. We assume that for all $L \le n$ we know that $\norm{X - \Phi^{\circ L}(Y)}_1 \le L \norm{R_{\phi}}_1 + \bigo{\widetilde{\alpha}^4}$ and our goal is to show this holds for $L = n + 1$. We start with unwrapping the outermost application of $\Phi$
% \begin{align}
%     \norm{X - \Phi^{\circ n + 1}(Y)}_1 &\le \norm{X - \Phi^{\circ n}(Y) - \mathcal{T}^{(2)}(\Phi^{\circ n}(Y))} + \norm{R}_1.
% \end{align}
% Now we need to simplify $\mathcal{T}^{(2)}(\Phi^{\circ n}(Y))$. Now each layer of $\Phi$ has a single term of the identity channel being applied so the term $\mathcal{T}^{(2)}(Y)$ will be important. Any layers of $\Phi$ that contribute an $R$ term will be of order $\widetilde{\alpha}^4$, as $R \in \bigo{\widetilde{\alpha}^3}$ and $\mathcal{T}^{(2)} \in \bigo{\widetilde{\alpha}^2}$, so $\norm{\mathcal{T}^{(2)} \circ R(Y)} \in \bigo{\widetilde{\alpha}^4}$. The same rationale works for any of the inner layers of $\Phi$ contributing another $\mathcal{T}^{(2)}$, so we can write
% \begin{align}
%     \norm{X - \Phi^{\circ n}(Y) - \mathcal{T}^{(2)}(\Phi^{\circ n}(Y))} + \norm{R}_1 &\le \norm{X - \Phi^{\circ n}(Y) - \mathcal{T}^{(2)}(Y)} + \norm{R}_1 + \bigo{\widetilde{\alpha }^4}.
% \end{align}
% Now this is where we use the induction step, as we can identify $X' = X - \mathcal{T}^{(2)}(Y)$ and we have
% \begin{align}
%     \norm{X - \Phi^{\circ n + 1}(Y)}_1 &\le \norm{X - \mathcal{T}^{(2)}(Y) - \Phi^{\circ n}(Y) } + \norm{R}_1 + \bigo{\widetilde{\alpha }^4} \\
%     &\le \norm{X - \mathcal{T}^{(2)}(Y) - n \mathcal{T}^{(2)}(Y)} + n \norm{R}_1 + \norm{R}_1 + \bigo{\widetilde{\alpha}^4} \\
%     &=\norm{X - (n + 1) \mathcal{T}^{(2)}(Y)} + (n + 1) \norm{R}_1 + \bigo{\widetilde{\alpha}^4},
% \end{align}
% which completes the proof.
% \end{proof}
% Now there is another argument that shows the exact same result as above but is a bit more difficult to prove. The way this argument goes is instead of constructing a Taylor Series of each $\Phi$ term and arguing that composition of low order terms gives you a high order term, you can simply compute the Taylor series of the norm directly. This is a lot more work for the same result, so we will stick to the above proof.


% \begin{theorem} \label{thm:single_qubit_repeated_interactions}
%     Let $H_S = U \diag(\lambda_S(1), \lambda_S(2)) U^\dagger$ be a single qubit Hamiltonian and define $0 < \Delta \coloneqq \lambda_S(2) - \lambda_S(1)$. The thermalizing channel given in Def. \ref{def:second_order_approx} repeated $L$ times with an initial state of $\rho_S(0) = \frac{1}{2} \identity$ prepares the system in the thermal state provided
% \end{theorem}
% \begin{proof}
% First we use Lemma \ref{lem:repeated_interactions_l} to decompose the trace distance into the more maneagable
% \begin{align}
%     \norm{\rho_S(\beta_E) - \Phi^{\circ L}(\rho_S(0))}_1 &\le \norm{\rho_S(\beta_E) - \rho_S(0) - L \mathcal{T}^{(2)}(\rho_S(0))}_1 + L \norm{R}_1 + \bigo{\widetilde{\alpha}^4} \\
%     &= \left| p_{\beta_E}(0) - \frac{1}{2} - L \frac{1}{2}(T_{0, 0} +T_{1, 0}) \right| + \left| p_{\beta_E}(1) - \frac{1}{2} - L \frac{1}{2}(T_{0, 1} +T_{1, 1}) \right| + 2\norm{R}_1 + \bigo{\widetilde{\alpha}^4},
% \end{align}
% where we make use of the fact that we start off with $\beta = 0$ and substitute in $1/2$ for the $p_{\beta}(0)$ and $p_{\beta}(1)$ factors. Now we can use the fact that for a single qubit (at $\beta = 0$) we know $T_{0,0} + T_{1, 0} = -(T_{0,1} + T_{1,1})$ from the arguments in Lemma \ref{lem:single_qubit_one_interaction}. This allows us to simplify the above sum into 
% \begin{align}
%     \norm{\rho_S(\beta_E) - \Phi^{\circ L}(\rho_S(0))}_1 \le 2\left| p_{\beta_E}(0) - \frac{1}{2} - L \frac{1}{2}(T_{0, 0} +T_{1, 0}) \right| + 2\norm{R}_1 + \bigo{\widetilde{\alpha}^4}.
% \end{align}
% We now tackle the $T_{0,0} + T_{1, 0}$ factor. Substituting in $\beta = 0$ and ignoring the resulting probability out front (we've already factored it above) into \eqref{eq:single_qubit_4} yields 
% \begin{align}
%     T_{0,0} + T_{1,0} = &\frac{\alpha^2 t^2}{\dim +1} \sinc^2\parens{\frac{(\Delta + \gamma) t}{2}} \left(-\frac{1}{1 + e^{-\beta_E \gamma}} +  \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \right) \nonumber \\
%     +  &\frac{\alpha^2 t^2}{\dim +1} \sinc^2\parens{\frac{(\Delta - \gamma) t}{2}} \left(-\frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} + \frac{1}{1 + e^{-\beta_E \gamma}} \right) \\
%     = &\widetilde{\alpha^2} \tanh \frac{\beta_E \gamma}{2} \left( \sinc^2\parens{\frac{(\Delta - \gamma) t}{2}} - \sinc^2\parens{\frac{(\Delta + \gamma) t}{2}} \right).
% \end{align}
% Now by setting $\gamma \to \Delta$ we see that only the sinc term with argument $\Delta - \gamma$ will be non-negligible, the other can be upper bounded by $\epsilon_{\sinc}$. So we have
% \begin{align}
%     \norm{\rho_S(\beta_E) - \Phi^{\circ L}(\rho_S(0))}_1 \le 2\left| p_{\beta_E}(0) - \frac{1}{2} - \frac{L\widetilde{\alpha}^2 }{2} \tanh \frac{\beta_E \gamma} {2} \right| + 2\widetilde{\alpha}^2 \epsilon_{\sinc} \tanh \frac{\beta_E \gamma}{2} + 2\norm{R}_1 + \bigo{\widetilde{\alpha}^4}.
% \end{align}

% To proceed we take a similar route as the single interaction case; we divide our error budget of $\epsilon$ into 3 parts, one for the contraction, one for the fixed point, and the other for the remainder. The remainder budget is easily met (PROVE) and the fixed point error is rather straightforward
% \begin{align}
%     2\widetilde{\alpha}^2 \epsilon_{\sinc} \tanh \frac{\beta_E \gamma}{2} &\le 8 \frac{\alpha^2}{\Delta^2 \dim + 1} \le \frac{\epsilon}{3},
% \end{align}
% so requiring $\alpha \le \Delta \sqrt{\epsilon} \sqrt{\frac{5}{24}}$ is sufficient for the fixed-point term to meet the error budget of $\epsilon / 3$.
% The harder term is the contraction term, which we turn to now. For this we will again make the argument that we require
% \begin{equation}
%     \frac{L \widetilde{\alpha}^2}{2} \tanh \frac{\beta_E \Delta}{2} \le p_{\beta_E}(0) - \frac{1}{2}.
% \end{equation}
% Now we make a simplification of the RHS
% \begin{equation}
%     p_{\beta_E}(0) - \frac{1}{2} = \frac{1}{1 + e^{-\beta_E \Delta}} - \frac{1}{2} = \frac{1 - e^{-\beta_E \Delta}}{2(1 + e^{-\beta_E \Delta})} = \frac{\tanh \frac{\beta_E \Delta}{2}}{2},
% \end{equation}
% which simplifies our inequality to $L \widetilde{\alpha }^2 \le 1$. This is acceptable for now and allows us to remove the absolute value.
% We now require
% \begin{align}
%     p_{\beta_E}(0) - \frac{1}{2} - \frac{L \widetilde{\alpha}^2}{2} \tanh{\frac{\beta_E \Delta}{2}} &\le \frac{\epsilon}{3} \\
%     \frac{\tanh \frac{\beta_E \Delta}{2}}{2} - \frac{L \widetilde{\alpha}^2}{2} \tanh{\frac{\beta_E \Delta}{2}}  &\le \frac{\epsilon}{3} \\
%     1 - L \widetilde{\alpha}^2 &\le \frac{2 \epsilon}{3 \tanh \frac{\beta_E \Delta}{2}} \\ 
%     L\widetilde{\alpha}^2 &\geq 1 - \frac{2 \epsilon}{3 \tanh \frac{\beta_E \Delta}{2}}.
% \end{align}
% Once these conditions are met we have completed the proof.
% \end{proof}


 
\section{Harmonic Oscillator Gap of 1}

In this section we show thermalization of a truncated Harmonic Oscillator with an environment tuned exactly to the gap of the oscillator. Specifically, the eigenvalues of the system are given by $\lambda_S(i) = i$ and $\gamma = 1$. In order to show thermalization we need to show that the thermal state is a fixed point $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_1 \leq \epsilon_{fix}$ and that there is a range of temperatures in which the channel cools the system $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1$. Showing that the thermal state at the environment temperature is a fixed point is easier and demonstrates the techniques we will use to show cooling, so we start with that.

\subsection{Fixed point}
\begin{lemma}
    For a truncated harmonic oscillator with $\dim_S \geq 4$, the thermalizing channel $\Phi$ has an approximate fixed point of temperature $\beta$, assuming an environment qubit at temperature $\beta$ and an environment gap $\gamma = 1$, that is controlled by the coupling constant $\alpha$, or specifically
    \begin{equation}
        \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 \le 12 \dim_S \alpha^2 + \bigo{\alpha^3 t^3 \dim_S \log^{3/2}(\dim_S)}
    \end{equation}
    \todo{Need to include units of $1/\omega$ next to $\alpha^2$ from $\Delta_{\min}$}
\end{lemma}
\begin{proof}
Using the Taylor series results from Section \ref{sec:taylor_series_phi}
\begin{align}
    \norm{\rho_S(\beta) - \Phi(\rho_S(\beta))}_1 &= \norm{\mathcal{T}^{(2)}(\rho_S(\beta)) + R_{\Phi}(\rho_S(\beta))}_1 \\
    &\leq \norm{\mathcal{T}^{(2)}(\rho_S(\beta))}_1 + \norm{R_{\Phi}(\rho_S(\beta)}_1 \\
    &\le \norm{\mathcal{T}^{(2)}(\rho_S(\beta))}_1 + \dim_S \norm{R_{\Phi}(\rho_S(\beta)}. \label{eq:fixed_pt_harmonic_osc_1}
\end{align}
Now recalling that since $\rho_S(\beta)$ is diagonal in the $H_S$ eigenbasis and that $\mathcal{T}^{(2)}$ maps diagonal elements to diagonal elements only, we can compute the norm of $\mathcal{T}^{(2)}$ by summing the absolute values of the diagonals
\begin{align}
    &\norm{\mathcal{T}^{(2)}(\rho_S(\beta))}_1 = \sum_j \left| \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)} \sum_i \bra{j} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{j} \right| \\
    &= \left|\sum_{i}  p(i) \bra{1} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{1}\right| + \sum_{j = 2}^{\dim_S - 1} \left|  \sum_{i} p(i) \bra{j} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{j}\right| + \left|  \sum_{i} p(i)\bra{\dim_S} \mathcal{T}^{(2)}(\ketbra{i}{i}) \ket{\dim_S}\right| \label{eq:fixed_pt_harmonic_osc_2}
\end{align}
We split the sum into these three parts because each of these terms will have different nontrivial contributions. We will start with the easiest, the leftmost term. The main tool we will use is Lemma \ref{lem:t_2_system_only}, and to do so we will add and subtract terms that will essentially allow us to substitute in expressions for $\bra{1}\mathcal{T}^{(2)}(\ketbra{1}{1})\ket{1}$ that utilize sinc and probabilities. 
\begin{align}
    &\left| p(1) \bra{1} \mathcal{T}^{(2)}(\ketbra{1}{1})\ket{1} + \sum_{i > 1} p(i) \bra{1} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{1} \right| \\
    &= \bigg| p(1) \bra{1} \mathcal{T}^{(2)}(\ketbra{1}{1})\ket{1} + p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) - p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) \nonumber \\ 
    &~ + \sum_{i > 1} p(i) \bra{1} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{1} \bigg| \\
    &\le \left|- p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) + \sum_{i > 1} p(i) \bra{1} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{1} \right| +  4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \left|- p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) + \frac{\alpha^2 t^2}{\dim + 1} \sum_{i > 1} p(i) q(0) \sinc^2((i - 2)t/2) \right| \nonumber \\
    &~+\sum_{i > 1} p(i) 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \left|- p(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) + \frac{\alpha^2 t^2}{\dim + 1}  p(2) q(0) \right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \parens{ \sum_{i > 2} |-p(1) q(1) + p(i) q(0)| + (3 + 4 \dim_S)} \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \left|- p(1)  q(1) +  p(2) q(0) \right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \parens{ 2(\dim_S - 2) + (3 + 4 \dim_S)} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1} \parens{\left| -\frac{e^{-\beta}}{\partfun_S(\beta)} \frac{e^{-\beta}}{1 + e^{-\beta}} +\frac{e^{-\beta 2}}{\partfun_S(\beta)} \frac{1}{1 + e^{-\beta}}\right| + \epsilon_{\sinc} 6 \dim_S} \\
    &= 6 \dim_S \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \\
    &\le 3 \epsilon_{\sinc} \alpha^2 t^2 \\
    &= 12 \alpha^2.
\end{align}
In the second to last step we used the fact that $2 \dim_S = \dim$ and $\dim / (\dim + 1) \le 1$, and in the last step we used the fact that $\epsilon_{\sinc} = \frac{4}{\Delta_{\min}^2 t^2} = \frac{4}{t^2}$ where $\Delta_{\min} = 1$ for the harmonic oscillator. 

Next we can compute the last term where $j = \dim_S$, for this computation we will use $d = \dim_S$ to save space.
\begin{align}
    &\left| p(d) \bra{d} \mathcal{T}^{(2)}(\ketbra{d}{d})\ket{d} + \sum_{i < d} p(i) \bra{d} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{d} \right| \\
    &= \bigg| p(d) \bra{d} \mathcal{T}^{(2)}(\ketbra{d}{d})\ket{d} + p(d) \frac{\alpha^2 t^2}{\dim + 1} q(0) \sum_{a < d} \sinc^2((d - a  + 1)t/2) - p(d) \frac{\alpha^2 t^2}{\dim + 1} q(0) \sum_{a < d} \sinc^2((d - a + 1)t/2) \nonumber \\ 
    &~ + \sum_{i < d} p(i) \bra{d} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{d} \bigg| \\
    &\le \left|- p(d) \frac{\alpha^2 t^2}{\dim + 1} q(0) \sum_{a < d} \sinc^2((d - a + 1)t/2) + \sum_{i < d} p(i) \bra{d} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{d} \right| +  4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1}\left|- p(d)  q(0) \sum_{a<d} \sinc^2((d - a + 1)t/2) + \sum_{i < d} p(i) q(1) \sinc^2((d - i + 1)t/2) \right| \nonumber \\
    &~+\sum_{i < d} p(i) 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1} \left|- p(d)q(0) +  p(d-1) q(1) \right| +  \epsilon_{\sinc}\frac{\alpha^2 t^2}{\dim + 1} \parens{ \sum_{i < d - 1} |-p(d) q(0) + p(i) q(1)| + (3 + 4 \dim_S)} \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \left| -\frac{e^{-\beta d}}{\partfun_S(\beta)} \frac{1}{1 + e^{-\beta}} +\frac{e^{-\beta (d - 1)}}{\partfun_S(\beta)} \frac{e^{-\beta}}{1 + e^{-\beta}}\right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \parens{ 2(\dim_S - 2) + (3 + 4 \dim_S)} \\
    &\le \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} 6 \dim_S \\
    &\le 12 \alpha^2.
\end{align}
Finally we can compute the middle terms for $2 \le j \le \dim_S - 1$. This time we will skip the explicit adding and subtracting of terms and triangle inequalities used to match the results of Lemma \ref{lem:t_2_system_only}
\begin{align}
&\left|p(j) \bra{j} \mathcal{T}^{(2)}(\ketbra{j}{j})\ket{j} + \sum_{i < j} p(i) \bra{j} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} + \sum_{i > j} p(i) \bra{j} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} \right| \\
&\leq \left| p(j) \bra{j} \mathcal{T}^{(2)}(\ketbra{j}{j})\ket{j} + \frac{\alpha^2 t^2}{\dim + 1} \left( \sum_{i < j} p(i) q(1) \sinc^2((i - j + 1)t/2) + \sum_{i > j} p(i)q(0) \sinc^2((i - j - 1)t/2) \right) \right| \nonumber \\
&\quad + 6 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\leq \left| p(j) \bra{j} \mathcal{T}^{(2)}(\ketbra{j}{j})\ket{j} + \frac{\alpha^2 t^2}{\dim + 1} \left( p(j-1) q(1) + p(j + 1)q(0) \right) \right| + 7 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\le \frac{\alpha^2 t^2}{\dim + 1} \left| - p(j) \parens{q(0)\sum_{a < j} \sinc^2((a - j + 1)t/2) + q(1) \sum_{a > j} \sinc^2((a - j - 1)t/2)} +  \left( p(j-1) q(1) + p(j + 1)q(0) \right) \right| \nonumber \\
&~+ \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}\parens{7 + 4 \dim_S} \\
&\le \frac{\alpha^2 t^2}{\dim + 1} \left| -p(j) (q(0) + q(1)) + p(j-1)q(1) + p(j+1) q(0) \right| +\frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}\parens{7 + 4 \dim_S + (\dim_S - 3) } \\
&= \frac{\alpha^2 t^2}{\dim + 1} \left| - \frac{e^{-\beta j}}{\partfun_S(\beta)} + \frac{e^{-\beta (j - 1)}}{\partfun_S(\beta)} \frac{e^{-\beta}}{1 + e^{-\beta}} + \frac{e^{-\beta (j + 1)}}{\partfun_S(\beta)} \frac{1}{1+e^{-\beta}} \right| +\frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}\parens{7 + 4 \dim_S + (\dim_S - 3) } \\
&\le 6 \dim_S \frac{\alpha^2 t^2}{\dim +1} \epsilon_{\sinc} \\
&\le 12 \alpha ^2,
\end{align}
where we assumed that $\dim_S \geq 4$ for the second to last substitution.

Now that we have computed every term, we return to Eq. \eqref{eq:fixed_pt_harmonic_osc_2} and use the substitution that each term is less than $12 \alpha^2$ to write
\begin{equation}
    \norm{\mathcal{T}^{(2)} (\rho_S(\beta))}_1 \leq 12 \dim_S \alpha^2.
\end{equation}
which, along with the remainder error bound from Lemma \ref{lem:remainder_bound}, when plugged into Eq. \eqref{eq:fixed_pt_harmonic_osc_1} yields the Lemma statement. 
\end{proof}

\subsection{Convergence}
We now want to analyze the converge of this channel for the Harmonic oscillator and environment tuned to the harmonic oscillator gap. We are trying to show
\begin{equation}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1
\end{equation}
We use $p_{\beta}(k) = \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}$ and $q(i) = \frac{e^{-\beta_E \lambda_E(i)}}{1 + e^{-\beta_E \gamma}}$, and due to the fact that the environment is a two level system with eigenvalues 0 and 1 we have $q(0) = \frac{1}{1 + e^{-\beta_E}}$ and $q(1) = \frac{e^{-\beta_E}}{1 + e^{-\beta_E}}$.
We continue by expanding
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 &\leq \sum_j \left| p_{\beta_E}(j) - p_{\beta}(j) - \sum_i p_{\beta}(i) \bra{j}\mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} \right| + \norm{R_{\Phi}}_1.
\end{align}
What we are going to do is plug in terms to allow us to utilize Lemma \ref{lem:t_2_system_only} and we will do so term by term over $j$, starting with the $j=1$, or ground state, term. The next step is to split the summation on $i$ into two parts, one part where $i = j$ and we have to use the more complicated self-transition expression from Lemma \ref{lem:t_2_system_only} and the other part where $i > j$. These steps are essentially the same as the proof for the fixed point theorem shown above, so we will skip some of the more explicit steps and refer the reader to the above proof for intermediate steps.

\begin{align}
    &\left| p_{\beta_E}(1) - p_{\beta}(1) - p_{\beta}(1) \bra{1} \mathcal{T}^{(2)}(\ketbra{1}{1})\ket{1} - \sum_{i > 1} p_{\beta}(i) \bra{1} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{1} \right| \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) + p_{\beta}(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) - \sum_{i > 1} p_{\beta}(i) \bra{1} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{1} \right| \nonumber \\
    & \quad +4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) + p_{\beta}(1) \frac{\alpha^2 t^2}{\dim + 1} q(1) \sum_{a > 1} \sinc^2((a  - 2)t/2) - \frac{\alpha^2 t^2}{\dim + 1} \sum_{i > 1} p_{\beta}(i) q(0) \sinc^2((i - 2)t / 2)  \right| \nonumber \\
    & \quad +4 \dim_S \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \sum_{i > 1} p(i) \\ 
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) +  \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(1)q(1) - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(2) q(0) \right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \parens{(4 \dim_S + 3) + \sum_{i > 2} |p_{\beta}(1) q(1) - p_{\beta}(i) q(0)|} \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) +  \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(1)q(1) - p_{\beta}(2) q(0)) \right| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} 6 \dim_S \\
    &\leq \left| p_{\beta_E}(1) - p_{\beta}(1) +  \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(1)q(1) - p_{\beta}(2) q(0)) \right| + 12 \alpha^2.
\end{align}
Now we will call the reducing term in the absolute value $s_1 \coloneqq \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(1)q(1) - p_{\beta}(2) q(0))$. Unlike the fixed point proof, this term is nonzero as $q(0)$ and $q(1)$ depend on $\beta_E$ and $p_{\beta}(1), p_{\beta}(2)$ depend on $\beta$. 


We now compute the $j = \dim_S$ term as this computation is similar to $j = 1$ in that we only have to deal with transitions to one side of $j$, which simplifies the intermediate steps. Following the fixed point proof we will temporarily denote $\dim_S \eqqcolon d$ to save space
\begin{align}
    &\left| p_{\beta_E}(d) - p_{\beta}(d) - p_{\beta}(d) \bra{d} \mathcal{T}^{(2)}(\ketbra{d}{d})\ket{d} - \sum_{i < d} p_{\beta}(i) \bra{d} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{d} \right| \\
    &\leq \left| p_{\beta_E}(d) - p_{\beta}(d) + p_{\beta}(d)  q(0) \frac{\alpha^2 t^2}{\dim + 1} \sum_{a< d} \sinc^2((a - d + 1)t/2) - \sum_{i < d} p_{\beta}(i) \bra{d} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{d} \right| + 4 \dim_S \frac{\alpha^2t^2}{\dim + 1} \epsilon_{\sinc} \\ 
    &\le  \left| p_{\beta_E}(d) - p_{\beta}(d) + p_{\beta}(d)  q(0) \frac{\alpha^2 t^2}{\dim + 1} - \sum_{i < d} p_{\beta}(i) \bra{d} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{d} \right| + \frac{\alpha^2t^2}{\dim + 1} \epsilon_{\sinc} (4 \dim_S + (\dim_S - 2)) \\
    &\le  \left| p_{\beta_E}(d) - p_{\beta}(d) + p_{\beta}(d)  q(0) \frac{\alpha^2 t^2}{\dim + 1} - \frac{\alpha^2 t^2}{\dim + 1} \sum_{i < d} p_{\beta}(i)q(1) \sinc^2((i - d + 1)t/2) \right| + \frac{\alpha^2t^2}{\dim + 1} \epsilon_{\sinc} (5 \dim_S + 1) \\
    &\le \left| p_{\beta_E}(d) - p_{\beta}(d) + p_{\beta}(d)  q(0) \frac{\alpha^2 t^2}{\dim + 1} - \frac{\alpha^2 t^2}{\dim + 1}  p_{\beta}(d-1)q(1) \right| + \frac{\alpha^2t^2}{\dim + 1} \epsilon_{\sinc} (5 \dim_S + 1 + (\dim_S - 2)) \\ 
    &\le \left| p_{\beta_E}(d) - p_{\beta}(d) + p_{\beta}(d)  q(0) \frac{\alpha^2 t^2}{\dim + 1} - \frac{\alpha^2 t^2}{\dim + 1}  p_{\beta}(d-1)q(1) \right| + \frac{\alpha^2t^2}{\dim + 1} \epsilon_{\sinc} 6 \dim_S \\
    &\le \left| p_{\beta_E}(d) - p_{\beta}(d) +  \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(d)  q(0) - p_{\beta}(d-1)q(1)) \right| + 12 \alpha^2.
\end{align}
Following the $j=1$ computation we denote the reducing term in the absolute value $s_{\dim_S} \coloneqq \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(\dim_S) q(0) - p_{\beta}(\dim_S - 1) q(1))$. 

Now we compute the middle terms where $1 < j < \dim_S$. 
\begin{align}
    &\left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j) \bra{j} \mathcal{T}^{(2)}(\ketbra{j}{j})\ket{j} - \sum_{i < j} p_{\beta}(i) \bra{j} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} - \sum_{i > j} p_{\beta}(i) \bra{j} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{j} \right| \\
&\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j) T_{j,j} - \frac{\alpha^2 t^2}{\dim + 1} \left( \sum_{i < j} p_{\beta}(i) q(1) \sinc^2((i - j + 1)t/2) + \sum_{i > j} p_{\beta}(i)q(0) \sinc^2((i - j -1)t/2) \right) \right| \nonumber \\
&\quad + 6 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
&\leq \left| p_{\beta_E}(j) - p_{\beta}(j) - p_{\beta}(j) \bra{j} \mathcal{T}^{(2)}(\ketbra{j}{j})\ket{j} - \frac{\alpha^2 t^2}{\dim + 1} \left( p_{\beta}(j - 1) q(1) + p_{\beta}(j + 1)q(0) \right) \right| \nonumber \\
&\quad + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} (6 + (\dim_S - 3)) \\
&\leq \bigg| p_{\beta_E}(j) - p_{\beta}(j) + \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j) \left( q(0) \sum_{a < j} \sinc^2((a - j + 1)t/2) + q(1) \sum_{a > j} \sinc^2((a - j - 1)t/2)   \right)  \nonumber \\
&\quad - \frac{\alpha^2 t^2}{\dim + 1} \left( p_{\beta}(j - 1) q(1) + p_{\beta}(j + 1)q(0) \right) \bigg| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} (\dim_S + 3 + 4 \dim_S) \\
&\leq \bigg| p_{\beta_E}(j) - p_{\beta}(j) + \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(j) (q(0) + q(1)) - p_{\beta}(j - 1) q(1) - p_{\beta}(j + 1)q(0) ) \bigg| + \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} 6\dim_S \\
&\leq \bigg| p_{\beta_E}(j) - p_{\beta}(j) + \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(j) - p_{\beta}(j - 1) q(1) - p_{\beta}(j + 1)q(0) ) \bigg| + 12 \alpha^2.
\end{align}
Again we denote the reducing term in the absolute value $s_j \coloneqq \frac{\alpha^2 t^2}{\dim + 1}(p_{\beta}(j) - p_{\beta}(j - 1) q(1) - p_{\beta}(j + 1)q(0) )$. 

We can now rewrite our initial inequality as
\begin{align}
    \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 &\leq \sum_j \left| p_{\beta_E}(j) - p_{\beta}(j) + s_j \right| + 12 \dim_S \alpha^2 + \norm{R_{\Phi}}_1.
\end{align}
To tackle this we divide our error budget of $\epsilon$ into two pieces, one for the distance reduction $\sum_j |p_{\beta_E}(j) - p_{\beta}(j) + s_j| \le \epsilon / 2$ and the other for the noisy interaction error $12 \dim_S \alpha^2 + \norm{R_{\Phi}}_1 \le \epsilon / 2$. Clearly the second inequality is straightforward to satisfy, we just require $\alpha \le \frac{\epsilon}{24 \dim_S} - \bigo{\alpha^3 t^3}$ and we ignore the recursion introduced by the remainder bound. 

The distance reduction inequality is much harder. We will start off by imposing upper bounds on $\alpha^2 t^2$ that allow us to reason about the contributions of $s_j$ to each term. We start off by making the observation that each $s_i$ is less than $\frac{\alpha^2 t^2}{\dim + 1}$ by using simple upper bounds, such as $p_{\beta}(i) \le 1$ and dropping negative terms. We next have to determine the sign of $p_{\beta_E}(j) - p_{\beta}(j)$, which can be determined by a first order Taylor expansion of the Boltzmann distribution
\begin{align}
    p_{\beta}(j) &= p_{\beta_E}(j) + (\beta - \beta_E) \frac{\partial}{\partial \beta} p_{\beta}(j) \bigg|_{\beta = \beta^{\star}} \\
    &= p_{\beta_E}(j) + \delta p_{\beta^{\star}}(j) (j - \trace{\rho_S(\beta^{\star}) H_S}),
\end{align}
where the guaranteed value of $\beta^{\star}$ that allows the above to hold varies for each value of $j$ and must lie in the interval $\beta^{\star} \in (\beta, \beta_E)$. In Lemma \ref{lem:magic_index} we show that for $\delta \le \frac{1}{2 \dim_S^3}$ we have that $j \le j^\star$ if and only if $p_{\beta_E}(j) \ge p_{\beta}(j)$.

Now that we know our summation can be split into terms with $j \le j^\star$ and those greater than $j^\star$ we make our second simplification. For this, we note that each $s_j$ is upper bounded in absolute value to $\alpha^2 t^2$, which is a tuneable, user-defined parameter. This means we can choose $\alpha^2 t^2$ small enough such that $\text{sign}(p_{\beta_E}(j) - p_{\beta}(j) + s_j) = \text{sign}(p_{\beta_E}(j) - p_{\beta}(j))$. This upper bound on $\alpha^2 t^2$ is proportional to $\min_j |p_{\beta_E}(j) - p_{\beta}(j)|$, which is clearly positive but is unclear if it is nonzero. To address this, we note that by slightly shifting $\beta_E$ (the temperature of an ancilla qubit) we can change this minimum to something nonzero. We further remark that since we already have upper bounds on on $\alpha^2$, this can be converted to an upper bound on $t$.

These two simplifications allow us to fully capture the effects of the channel on reducing our distance as follows
\begin{align}
    |p_{\beta_E}(j) - p_{\beta}(j) + s_j| &= (- \text{sign}(p_{\beta_E}(j) - p_{\beta}(j) + s_j))(p_{\beta_E}(j) - p_{\beta}(j) + s_j) \\
    &= (- \text{sign}(p_{\beta_E}(j) - p_{\beta}(j)))(p_{\beta_E}(j) - p_{\beta}(j) + s_j) \\
    &= |p_{\beta_E}(j) - p_{\beta}(j)| - \text{sign}(p_{\beta_E}(j) - p_{\beta}(j) )s_j.
\end{align}
So our summation simplifies to the following
\begin{align}
     \sum_j |p_{\beta_E}(j) - p_{\beta}(j) + s_j| &= - \sum_j \text{sign}(p_{\beta_E}(j) - p_{\beta}(j) + s_j) (p_{\beta_E}(j) - p_{\beta}(j) + s_j) \\
     &= - \sum_j \text{sign}(p_{\beta_E}(j) - p_{\beta}(j)) (p_{\beta_E}(j) - p_{\beta}(j)) -  \text{sign}(p_{\beta_E}(j) - p_{\beta}(j)) \sum_j s_j \\
     &= \sum_j |p_{\beta_E}(j) - p_{\beta}(j)|  + \sum_{j \le j^\star} s_j - \sum_{j > j^\star} s_j.
\end{align}
We need to show that the RHS of the above is less than $\epsilon / 2$. We note that this will not be possible for arbitrary $\epsilon, \beta, $ and $\beta_E$. Instead, our goal is to show for what regimes of $\epsilon$ and $\delta$ it is possible and then later to use this, along with a cooling schedule, to show that with enough interactions we can reach arbitrary $\epsilon$ and $\beta_E$. 

We proceed with our single shot analysis now. To do so, we first note that $s_1$ will always contribute in a beneficial way towards our objective. What we will do is split $s_1$ from the other $s_j$ terms and show that the signed sum of $s_j$ with $j > 1$ is upper bounded by 0. Then we use the bound $\sum_j |p_{\beta_E}(j) - p_{\beta}(j)| \le 2 \dim_S^2 \delta$ from Lemma \ref{lem:thermal_state_diff_bound} to get the thermal state difference in terms of $\delta$. Now the inequality we need to prove becomes
\begin{align}
    \sum_j |p_{\beta_E}(j) - p_{\beta}(j) + s_j| &\le 2 \dim_S^2 \delta + s_1 + \sum_{j = 2}^{j^\star} s_j - \sum_{j=j^\star + 1}^{\dim_S} s_j \le \epsilon / 2.
\end{align}
As long as we can produce an upper bound of $s_1 + \sum_{j = 2}^{j^\star} s_j - \sum_{j=j^\star + 1}^{\dim_S} s_j \le - \epsilon_s$ then we are set; we can then take $\delta$ to be sufficiently small to satisfy the inequality. That may seem like a bad gambit in the single-interaction case, as we can then only prepare thermal states incredibly close to the target, but in the multi-interaction case this will only dictate how small our cooling schedule step size is.

We start by bounding the $s_1$ term
\begin{align}
    s_1 &= \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(1)q(1) - p_{\beta}(2) q(0)) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(1)\left( \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} - \frac{e^{-\beta}}{1 + e^{-\beta_E}}\right) \\
    &= - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(1) e^{-\beta} \frac{1 - e^{- \delta}}{1 + e^{-\beta_E}} \\
    &\le - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(1) (1 - e^{-\delta}) \frac{e^{-\beta_E}}{1 + e^{-\beta_E}},
\end{align}
where we used the fact that $\beta \le \beta_E \implies -e^{-\beta} \le -e^{-\beta_E}$ in the last step. What remains is to lower bound $1 - e^{-\delta}$ and $\frac{e^{-\beta_E}}{1 + e^{-\beta_E}}$. To do this, we introduce upper and lower bounds on $\delta, \beta_E$. The bounds we use are
\begin{equation}
    \ln \left( \frac{1}{1 - c} \right) \le \delta \le \beta_E \le \ln \left( \frac{1}{c'} - 1 \right).
\end{equation}
The rationale behind these strange parametrizations is that it makes the resulting bounds a bit easier to manipulate. These requirements lead to the bounds
$1 - e^{-\delta} \ge c$ and $\frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \geq c'$, which can be plugged in to the expression for $s_1$, resulting in
\begin{equation}
s_1 \le - \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(1)c c'.
\end{equation}
For future reference, we can propogate the above bounds to the following useful quantities
\begin{align}
    \frac{c'}{1 - c'} \leq ~&e^{-\delta} \leq 1 - c \\
    \frac{1}{2 - c} \le ~&\frac{1}{1 + e^{-\beta_E}} \le 1 - c' \\
    c' \le ~&\frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \le \frac{1 - c}{2 - c}.
\end{align}
For expressions involving $\beta$ we will use the bounds $0 \le \beta \le \beta_E$.
% The lower bound on $\delta$ we introduce is $\delta \ge -\ln (1 - c) \implies 1 - e^{-\delta} \ge c$, where we simply take $c$ to be some constant value, clearly less than 1. Similarly, it is rather straightforward to show that $\beta_E \le \ln \parens{\frac{1}{c'} - 1} \implies \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \ge c'$. We will combine these bounds on $\delta$ and $\beta_E$ with other bounds later to find valid ranges of $c, c'$ but for now we can think of these as sufficiently small constants. The final result for $s_1$ is 

Our next task is to determine when the two sums above are less than zero. To do so we need to introduce bounds on the $s_j$'s. We first simplify the expression
\begin{align}
    s_j &= \frac{\alpha^2 t^2}{\dim + 1}(p_{\beta}(j) - p_{\beta}(j - 1) q(1) - p_{\beta}(j + 1)q(0) ) \\
    &= \frac{\alpha^2 t^2}{\dim + 1}p_{\beta}(j) \parens{1 - e^{\beta} q(1) - e^{-\beta} q(0)} \\
    &= \frac{\alpha^2 t^2}{\dim + 1}p_{\beta}(j) \parens{1 -\frac{e^{-\delta}}{1 + e^{-\beta_E}} - \frac{e^{-\beta}}{1 + e^{-\beta_E}}},
\end{align}
and now we work on bounding the rightmost factor. As we have terms involving $s_j$ and $-s_j$ we need both lower and upper bounds on this factor. Starting with the upper bound
\begin{align}
    1 -\frac{e^{-\delta}}{1 + e^{-\beta_E}} - \frac{e^{-\beta}}{1 + e^{-\beta_E}} &= 1 - \frac{e^{-\delta}}{1 + e^{-\beta_E}} - e^{\delta} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \\
    &\le 1 - \frac{1 - c}{1 + e^{-\beta_E}} - \frac{1}{1 - c} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \\
    &\le 1 - \frac{1-c}{2-c} - \frac{c'}{1 - c}.
\end{align}
Now we turn to the lower bound, which works out to
\begin{align}1 -\frac{e^{-\delta}}{1 + e^{-\beta_E}} - \frac{e^{-\beta}}{1 + e^{-\beta_E}} &= 1 - \frac{e^{-\delta}}{1 + e^{-\beta_E}} - e^{\delta} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \\
&\ge 1 - \frac{1- c}{1 + e^{-\beta_E}} - \frac{1 - c'}{c'} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \\
&\ge 1 - (1- c) (1 - c') - (1 - c')\frac{1- c}{2 - c} \\
&= 1 - \frac{(1 - c)^2(1-c')}{2 - c}.
\end{align}
The last bound we will need is a lower bound on $s_{\dim}$, which is given by
\begin{align}
    s_{\dim_S} &= \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(\dim_S) q(0) - p_{\beta}(\dim_S - 1) q(1)) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(\dim_S) \left( \frac{1}{1 + e^{-\beta_E}} - \frac{e^{-\delta}}{1 + e^{-\beta_E}} \right) \\
    &\geq \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(\dim_S) \frac{c}{1 + e^{-\beta_E}} \\
    &\geq \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(\dim_S) \frac{c}{2 - c}.
\end{align}

Now that we have appropriate bounds for each $s_j$, we can attempt to plug these in to our previous bound.
\begin{align}
    &s_1 + \sum_{j = 2}^{j^\star} s_j - \sum_{j=j^\star + 1}^{\dim_S} s_j \nonumber \\
    &\le \frac{\alpha^2 t^2}{\dim + 1} \parens{- p_{\beta}(1) c c' + \parens{1 - \frac{1 - c}{2 - c} - \frac{c'}{1 - c}} \sum_{j = 2}^{j^\star} p_{\beta}(j) - \left( 1 - \frac{(1 - c)^2 (1 - c')}{2 - c}\right) \sum_{j = j^\star  + 1}^{\dim_S - 1} p_{\beta}(j) - \frac{c}{2 - c} p_{\beta}(\dim_S) } 
    \end{align}
Focusing on the middle piece and substituting $\sum_{j = j^\star  + 1}^{\dim_S - 1} p_{\beta}(j) = 1 - p_{\beta}(1) - \sum_{j = 2}^{j^\star} p_{\beta}(j) - p_{\beta}(\dim_S)$
\begin{align}
    &= \left(1 - \frac{(1 - c)^2(1 - c')}{2 - c} - c c'\right) p_{\beta}(1) + \left(2 - \frac{1-c + (1-c)^2(1-c')}{2 - c} + \frac{c'}{1 - c} \right) \sum_{j = 2}^{j^\star} p_{\beta}(j) \nonumber \\
    &~+ \left( 1 - \frac{(1 - c)^2(1- c') - c}{2 - c} \right)p_{\beta}(\dim_S) - \left(1 - \frac{(1 - c)^2(1 - c')}{2 - c} \right) \\
    &= \eta_1 p_{\beta}(1) + \eta_2 \sum_{j = 2 }^{j^\star} p_{\beta}(j) + \eta_3 p_{\beta}(\dim_S)- \eta_4
\end{align}
where $\eta_1 = 1 - \frac{(1 - c)^2(1 - c')}{2 - c} - c c', \eta_2 = 2 - \frac{1-c + (1-c)^2(1-c')}{2 - c} + \frac{c'}{1 - c}$, $\eta_3 = 1 - \frac{(1 - c)^2(1- c') - c}{2 - c} $, and $\eta_4 = 1 - \frac{(1 - c)^2(1 - c')}{2 - c}$


\section{Marked State}
In this section we explore how the ground state of a ``marked state" Hamiltonian can be found using our thermalizing channel. A marked state Hamiltonian is a reflection operator $H_S = \identity - 2 \ketbra{\psi}{\psi}$. This is the same reflection used in Grover Search and therefore we can test how close to the lower bound of $\Omega(\sqrt{\dim_S})$ our channel is.  

\section{Harmonic Oscillator random $\gamma$}
In this section we explore how to prepare the thermal state of a truncated harmonic oscillator with randomly chosen values of $\gamma$. As the spectrum of the harmonic oscillator is known, we can sample $\gamma$ from the appropriately weighted differences. Once we have computed this distribution we will adapt Lemma \ref{lem:t_2_system_only} to being randomly chosen over our computed distribution. Then we will compute the output of the channel compared to a target state and show when our channel gets us closer to our goals.

\subsection{Transition Elements}
The distribution we will randomly choose $\gamma$ over is $\prob{\gamma = k} = \frac{1}{\binom{\dim_S}{2}} \sum_{i < j} I[j - i = k]$. For the harmonic oscillator this is pretty easily computable. For a given difference $k$, we know that there must be an eigenstate with that difference starting from the ground state. Then we can imagine shifting the lower state up in energy until the higher energy state hits the truncation. This gives $\prob{\gamma = k} = \frac{\dim_S - k}{\binom{\dim_S}{2}}$ and we will use this for the remainder of this section.

Now we need to compute the transition elements $\mathbb{E}_{\gamma} \mathcal{T}^{(2)}_{\gamma}$. Starting from Eq. \eqref{eq:t_2_intermediate_1} we have for $i < j$, 
\begin{align}
    \mathbb{E}_{\gamma} \bra{j} \mathcal{T}^{(2)}_{\gamma}(\ketbra{i}{i})\ket{j} &= \frac{\alpha^2 t^2}{\dim + 1}\mathbb{E}_{\gamma}\left( \sinc^2(\Delta_S(i,j)t/2) + \frac{\sinc^2((\Delta_S(i,j) - \gamma)t/2)}{1 + e^{-\beta_E \gamma}}  + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) \right) \\
    &\le \frac{\alpha^2 t^2}{\dim + 1}\left( 2\epsilon_{\sinc} + \mathbb{E}_{\gamma} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) \right)
\end{align}
The term of $2 \epsilon_{\sinc}$ comes from the fact that the leftmost term is independent of $\gamma$ and is upper bounded by $\epsilon_{\sinc}$ and that the middle term is dependent on $\gamma$, but because $i < j$ this implies $\Delta_S(i,j) - \gamma < -\Delta_{\min}$. Therefore each of these terms in the expectation can be upper bounded by $\epsilon_{\sinc}$. 
The rightmost term is where we turn our attention to
\begin{align}
    \mathbb{E}_{\gamma} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}} \sinc^2((\Delta_S(i,j) + \gamma)t/2) &= \sum_{k = 1}^{\dim_S - 1} \frac{\dim_S - k}{\binom{\dim_S}{2}} \frac{e^{-\beta_E k}}{1 + e^{-\beta_E k}} \sinc^2((\Delta_S(i,j) + k)t/2) \\
    &=\sum_{k = 1}^{\dim_S - 1} \frac{\dim_S - k}{\binom{\dim_S}{2}} \frac{e^{-\beta_E k}}{1 + e^{-\beta_E k}} \sinc^2((|\Delta_S(i,j)| - k)t/2) \\
    &\le \frac{\dim_S - \Delta_S(i,j)}{\binom{\dim_S}{2}} \frac{e^{-\beta_E \Delta_S(i,j)}}{1 + e^{-\beta_E \Delta_S(i,j)}} + \left(1 - \frac{\dim_S - \Delta_S(i,j)}{\binom{\dim_S}{2}}\right)\epsilon_{\sinc} \\
    &\le \frac{\dim_S - \Delta_S(i,j)}{\binom{\dim_S}{2}} \frac{e^{-\beta_E \Delta_S(i,j)}}{1 + e^{-\beta_E \Delta_S(i,j)}} + \epsilon_{\sinc}.
\end{align}
Now plugging these in we get
\begin{equation}
    i < j \implies \mathbb{E}_{\gamma} \bra{j} \mathcal{T}^{(2)}_{\gamma}(\ketbra{i}{i})\ket{j} - \frac{\dim_S - \Delta_S(i,j)}{\binom{\dim_S}{2}} \frac{e^{-\beta_E \Delta_S(i,j)}}{1 + e^{-\beta_E \Delta_S(i,j)}} \le 3 \epsilon_{\sinc}.
\end{equation}
We could show the exact same thing but with an absolute value on the lefthand side of the inequality but the proof would be redundant. Similarly, we can show for $i > j$ the result is
\begin{equation}
    i > j \implies \mathbb{E}_{\gamma} \bra{j} \mathcal{T}^{(2)}_{\gamma}(\ketbra{i}{i})\ket{j} - \frac{\dim_S - \Delta_S(i,j)}{\binom{\dim_S}{2}} \frac{1}{1 + e^{-\beta_E \Delta_S(i,j)}} \le 3 \epsilon_{\sinc}.
\end{equation}

We now turn our attention to the harder part, which is the self-transition element. We can jump in from Eq. \eqref{eq:t_2_same_state_2}, which is repeated below
\begin{align}
    &\bra{i} \mathcal{T}^{(2)}(\ketbra{i}{i})\ket{i} + \frac{\alpha^2 t^2}{\dim + 1} \left( \frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}}  + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a > i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \right) = \nonumber \\
        &- \frac{\alpha^2 t^2}{\dim + 1} \left( \sum_{a \neq i} \sinc^2\parens{\frac{\Delta_S(a,i)t}{2}} + \frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a > i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}}  + \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \right).
\end{align}
Now even when randomizing with respect to $\gamma$ we can upper bound the right hand side by $2 \dim_S$, the interesting part of randomizing comes from the two summations on the left hand side. We will focus on these pieces individually and then combine them to get a full expression for $\mathbb{E}_{\gamma} \mathcal{T}^{(2)}_{\gamma}$.
\begin{align}
    \mathbb{E}_{\gamma} \frac{1}{1 + e^{-\beta_E \gamma}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) +\gamma)t}{2}} &= \sum_{k = 1}^{\dim_S - 1} \frac{\dim_S - k}{\binom{\dim_S}{2}} \frac{1}{1 + e^{-\beta_E k}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) + k)t}{2}} \\
    &=\sum_{k = 1}^{i - 1}\sum_{a=1}^{i - 1} \frac{\dim_S - k}{\binom{\dim_S}{2}} \frac{1}{1 + e^{-\beta_E k}} \sinc^2\parens{\frac{(\Delta_S(a, i) + k)t}{2}} \nonumber \\
    &~+\sum_{k=i}^{\dim_S - 1} \sum_{a < i}\prob{k} \frac{1}{1 + e^{-\beta_E k}} \sinc^2\parens{\frac{(\Delta_S(a, i) + k)t}{2}} \\
    &\le \sum_{k = 1}^{i - 1}\sum_{a=1}^{i - 1} \frac{\dim_S - k}{\binom{\dim_S}{2}} \frac{1}{1 + e^{-\beta_E k}} \sinc^2\parens{\frac{(\Delta_S(a, i) + k)t}{2}} + \dim_S \epsilon_{\sinc} \\
    &\le \sum_{k = 1}^{i - 1}\frac{\dim_S - k}{\binom{\dim_S}{2}} \frac{1}{1 + e^{-\beta_E k}} + 2 \dim_S \epsilon_{\sinc}.
    \end{align}
A similar proof works for the other summation
\begin{align}
    \mathbb{E}_{\gamma} \frac{e^{-\beta_E \gamma}}{1 + e^{-\beta_E \gamma}}\sum_{a < i} \sinc^2\parens{\frac{(\Delta_S(a, i) -\gamma)t}{2}} \le \sum_{k = i + 1}^{\dim_S - 1} \frac{\dim_S - k}{\binom{\dim_S}{2}} \frac{e^{-\beta_E k}}{1 + e^{-\beta_E k}} + 2 \dim_S \epsilon_{\sinc}.
\end{align}

 
   \section{leftover rubbish}

\subsubsection{Idk what attempt that is below}
Similarly we can show a lower bound of 0
\begin{align}
    0 &\leq p_{\beta}(j) - p_{\beta}(j-1) q(1) - p_{\beta}(j+1) q(0) \\
    0 &\leq e^{-\beta j} - e^{-\beta(j - 1)} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} - e^{-\beta(j+1)} \frac{1}{1 + e^{-\beta_E}} \\
    0 &\leq 1 - e^{\beta} \frac{e^{-\beta_E}}{1 + e^{-\beta_E}} - e^{-\beta} \frac{1}{1 + e^{-\beta_E}} \\
    0&\leq 1 + e^{-\beta_E} -e^{-\delta} - e^{-\beta} \\
    0 &\leq 1 + e^{-\delta} e^{-\beta} - e^{-\delta } - e^{-\beta} \\
    0 &\leq 1 - e^{-\beta} - e^{-\delta} (1 - e^{-\beta}) \\
    0 &\leq (1-e^{-\beta})(1 - e^{-\delta}) \\
    \iff 0 &\leq \delta \text{ and } 0 \leq \beta,
\end{align}
which implies $s_j \ge 0$. Now I need something better than nonzero. 
\begin{align}
    p_{\beta}(j) - p_{\beta}(j-1) q(1) - p_{\beta}(j+1) q(0) = p_{\beta}(j) \left( 1 - \frac{e^{-\delta}}{1 + e^{-\beta_E}} - \frac{e^{-\beta}}{1 + e^{-\beta_E}} \right) 
\end{align}
Now using the bounds from before:
\begin{align}
    \delta &\geq - \ln(1 - c) \\
    - e^{-\delta} &\geq c - 1. 
\end{align}
This prompts us to introduce the bound $c - e^{-\beta} \ge 0$, or $\beta \geq \ln(1/c)$. This reduces our problem to
\begin{align}
    p_{\beta}(j) - p_{\beta}(j-1) q(1) - p_{\beta}(j+1) q(0) &\ge p_{\beta}(j)\parens{1 + \frac{c - 1}{1 + e^{-\beta_E}} - \frac{e^{-\beta}}{1 + e^{-\beta_E}}} \\
    &= p_{\beta}(j)\parens{1 - \frac{1}{1 + e^{-\beta_E}} \frac{c - e^{-\beta}}{1 + e^{-\beta_E}}} \\
    &\ge p_{\beta}(j) \parens{1 - \frac{1}{1 + e^{-\beta_E}}}.
\end{align}
Through basic algebra one can rearrange our inequality $\beta_E \le \ln \left( \frac{1}{c'} - 1\right)$ to the form $-\frac{1}{1 + e^{-\beta_E}} \geq c' - 1$, which plugging in to the above yields
\begin{align}
    s_j \ge \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j) c'.
\end{align}
The last bound we need is for $s_{\dim_S}$, for which we can just drop the negative term for now
\begin{align}
    s_{\dim_S} &= \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(\dim_S) q(0) - p_{\beta}(\dim_S - 1) q(1)) \\
    &\ge \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(\dim_S) \frac{1}{1 + e^{-\beta_E}}.
\end{align}
Now using the fact that $\beta_E \ge \beta \ge \ln (1/ c)$ we get 
\begin{align}
    s_{\dim_S} &\ge \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(\dim_S) \frac{1}{1 + c}.
\end{align}

The above bounds mean we now have to determine the value of $j^{\star}$
\begin{align}
    \sum_{j = 2}^{j^\star} s_j - \sum_{j=j^{\star} + 1}^{\dim_S} s_j &\le \frac{\alpha^2 t^2}{\dim + 1} \sum_{j = 2}^{j^\star} p_{\beta}(j) - \frac{\alpha^2 t^2}{\dim + 1} c' \sum_{j=j^{\star} + 1}^{\dim_S  -1} p_{\beta}(j) - \frac{\alpha^2 t^2}{\dim + 1} \frac{1}{1 + c} p_{\beta}(\dim_S) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} \parens{1 - (1 + c')\sum_{j=j^{\star} + 1}^{\dim_S  -1} p_{\beta}(j) - \frac{2 + c}{1 + c} p_{\beta}(\dim_S) - p_{\beta}(1) }
\end{align}

\subsubsection{The below is linearization which I think does not work}

By expanding $p_{\beta}(j)$ about $\beta_E$ and using our analytic expression for the partition function of the harmonic oscillator we get the following linearization in terms of $\delta$
\begin{align}
    p_{\beta}(j) &= \frac{e^{-\beta j}}{\partfun_S(\beta)} 
    \end{align}
\begin{align}
    \partfun_S(\beta) &= \trace{e^{-\beta H_S}} \\
    &= \trace{e^{-\beta_E H_S} e^{\delta H_S}} \\
    &= \trace{e^{-\beta_E H_S}} + \delta \trace{e^{-\beta_E H_S} H_S} + \bigo{\delta^2}
\end{align}
which allows us to compute the linearization
\begin{align}
    \frac{1}{\partfun_S(\beta)} &= \frac{1}{\partfun_S(\beta_E)} - \delta \frac{1}{\partfun_S(\beta_E)^2} \cdot \trace{e^{-\beta_E H_S} H_S} + \bigo{\delta^2},
\end{align}
which when combined with the linearization of $e^{-\beta j}$ gives us
\begin{align}
    \frac{e^{-\beta j}}{\partfun_S(\beta)} &= e^{-\beta_E j} (1 + \delta j + \bigo{\delta^2}) \frac{1}{\partfun_S(\beta_E)} \parens{1 - \delta \trace{\rho_S(\beta_E) H_S} + \bigo{\delta^2}} \\
    &= p_{\beta_E}(j)\parens{1 + \delta(j - \trace{\rho_S(\beta_E) H_S}) + \bigo{\delta^2}}
\end{align}

Now we have to linearize the $s_j$ terms. We start with $s_1$
\begin{align}
    s_1 &= \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(1)q(1) - p_{\beta}(2) q(0)) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta_E}(1)(1 + \delta(1 - \trace{\rho_S(\beta_E)H_S}))q(1) - p_{\beta_E}(2)(1 + \delta(2 - \trace{\rho_S(\beta_E)H_S})) q(0)) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(1) \left( \frac{e^{-\beta_E}}{1 + e^{-\beta_E}}(1 + \delta (1 - \trace{\rho_S(\beta_E) H_S})) - \frac{e^{-\beta_E}}{1 + e^{-\beta_E}}(1 + \delta(2 - \trace{\rho_S(\beta_E) H_S})) \right) \\ 
    &= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(1)\frac{e^{-\beta_E}}{1 + e^{-\beta_E}} \left( - \delta \right) + \bigo{\delta^2}.
\end{align}
Now we get the other easy contribution
\begin{align}
    s_{\dim_S} &= \frac{\alpha^2 t^2}{\dim + 1} (p_{\beta}(\dim_S) q(0) - p_{\beta}(\dim_S - 1) q(1)) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(\dim_S)((1 + \delta(\dim_S - \trace{\rho_S(\beta_E)H_S})) q(0) - e^{\beta_E}(1 + \delta(\dim_S - 1 - \trace{\rho_S(\beta_E) H_S})) q(1)) \\
    &= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(\dim_S) \frac{1}{1 + e^{-\beta_E}} (+ \delta) + \bigo{\delta^2}.
\end{align}
And lastly we tackle the more complicated middle expressions $s_j$ for $1 < j < \dim_S$
\begin{align}
    s_j &= \frac{\alpha^2 t^2}{\dim + 1}(p_{\beta}(j) - p_{\beta}(j - 1) q(1) - p_{\beta}(j + 1)q(0) ) \\
&= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta}(j) \left( 1 - \frac{p_{\beta}(j - 1)}{p_{\beta}(j)} q(1) - \frac{p_{\beta}(j + 1)}{p_{\beta}(j)}q(0) \right) \\
&= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(j)( 1 + \delta(j - \trace{\rho_S(\beta_E) H_S})) \left( 1 - e^{\beta} q(1) - e^{-\beta} q(0) \right) \\
&= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(j)( 1 + \delta(j - \trace{\rho_S(\beta_E) H_S})) \left( 1 - \frac{e^{-\delta}}{1 + e^{-\beta_E}} - \frac{e^{-\beta_E} e^{\delta}}{1 + e^{-\beta_E}} \right) \\
&= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(j)( 1 + \delta(j - \trace{\rho_S(\beta_E) H_S})) \left( \delta \frac{1 - e^{-\beta_E}}{1 + e^{-\beta_E}} \right) \\
&= \frac{\alpha^2 t^2}{\dim + 1} p_{\beta_E}(j) \delta \tanh{\beta_E / 2} + \bigo{\delta^2}.
\end{align}

Using the same linearization, we see that
\begin{align}
    p_{\beta_E}(j) - p_{\beta}(j) = \delta (\trace{\rho_S(\beta_E) H_S} - j) + \bigo{\delta^2},
\end{align}
so if $j < \trace{\rho_S(\beta_E) H_S}$, we know $|p_{\beta_E}(j) - p_{\beta}(j)| = p_{\beta_E}(j) - p_{\beta}(j)$ and if $j > \trace{\rho_S(\beta_E) H_S}$ then $|p_{\beta_E}(j) - p_{\beta}(j)| = p_{\beta}(j) - p_{\beta_E}(j)$. Since we are choosing an $\alpha^2 t^2$ small enough such that $\text{sign} (p_{\beta_E}(j) - p_{\beta}(j) + s_j) = \text{sign} (p_{\beta_E}(j) - p_{\beta}(j))$

\subsection{Final convergence}
This section is going to summarize the results and runtime needed to reach a given thermal state $\beta$. Our approach for this is to show that by taking a cooling schedule with short enough steps we can guarantee that the output will be controllably close to the desired state. For this, we will imagine starting the system off in the maximally mixed state $\beta_0 = 0$ and an environment at temperature $\beta_1 = \frac{\beta}{L}$. With each interaction channel we cool the environment down by $\beta / L$, and we do this $L$ times until we reach an environment of $\beta_L = \beta$. Let $\Phi_i$ denote the channel with environment at inverse temperature $\beta_i$. Our goal is to show
\begin{align}
    \norm{\rho_S(\beta) - \Phi_L \circ \Phi_{L-1} \circ \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \leq \epsilon.
\end{align}
To do this, we add and subtract terms of the form $\Phi_{L}(\rho_S(\beta_{L-1}))$ and use the fact that the trace norm is non-increasing under the action of channels $\norm{\Psi(X)}_1 \leq \norm{X}_1$. 
\begin{align}
    \norm{\rho_S(\beta) - \Phi_L \circ  \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 &= \norm{\rho_S(\beta) - \Phi_L(\beta_{L-1}) + \Phi_{L}(\beta_{L - 1}) - \Phi_L \circ  \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \\
    &\leq \norm{\rho_S(\beta) - \Phi_L(\beta_{L-1})}_1 + \norm{\beta_{L-1} - \Phi_{L-1} \circ \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \\
    &\le \sum_{i = 1}^{L} \norm{\rho_S(\beta_i) - \Phi_{i}(\rho_S(\beta_{i - 1}))}_1
\end{align}
Now using the result we have that $\norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta))}_1 \le 12 \dim_S \alpha^2 $ we have that 
$$\norm{\rho_S(\beta) - \Phi_L \circ  \ldots \circ \Phi_{1} (\rho_S(\beta_0))}_1 \le 12 L \dim_S \alpha^2$$
Now this serves as an upper bound on $L$ as $L \le \frac{\epsilon}{12 \dim_S \alpha^2}$. Now to see if there exists an $L$ that satisfies these two bounds simultaneously we require


\section{Fixed points and convergences}

In this first section we introduce the conditions required for more detailed analysis. The first is 
\begin{definition}[Well-separated Hamiltonian] \label{def:separated_hamiltonians}
    A Hamiltonian $H_S$ is $(\Delta_{\sinc}, \Delta_{\min}$)-separated if there are not only no degeneracies, but every eigenvalue difference is bounded from below and eigenvalue differences are sufficiently spaced.
    \begin{align}
        &\forall i \neq j : \Delta_{S}(i,j) \geq \Delta_{\min}, \\
        &\forall (i,j) \neq (k,l) : \abs{\Delta_S(i,j) - \Delta_S(k,l)} \geq \Delta_{\sinc} + \Delta_{\min}.
    \end{align}
    This rather strong condition on the spectrum is needed as we would like to make the guarantee that for a provided $\gamma$ there is at most 1 pair of indices $(i,j)$ such that $\sinc^2((\Delta_S(i,j) - \gamma)t/2) \geq 1 - \epsilon_{\sinc}$.
    \begin{equation}
        \abs{\set{(i,j): \abs{\Delta_S(i,j) - \gamma} \leq \Delta_{\sinc}}} \in \set{0, 1}.
    \end{equation}
    It will be useful when working from this to sample from these differences uniformly. Given a Hamiltonian with eigenvalues that are well separated there are $\frac{\dim_S (\dim_S - 1)}{2}$ positive differences. We create the following distribution which is a mixture of uniform distributions over the the eigenvalue differences, where each uniform distribution is of width $2 \Delta_{\sinc}$, so we can guarantee that any sampled value of $\gamma$ is close enough to an eigenvalue difference to satisfy the requirements of Corollary \ref{cor:gamma_difference_reqs}.

    \begin{equation}
        \prob{\gamma = x} = \begin{cases}
            \frac{2}{\dim_S (\dim_S - 1)}
            & x = \Delta_S(i,j) \text{ for some i,j} \\
            0 & \forall (i,j), x \neq \Delta_S(i,j).
        \end{cases}
    \end{equation}
    We will refer to this distribution as the \emph{perfect knowledge} distribution.
\end{definition}

The following lemma is purely a technical one to allow for easier computation of sums that appear from the second order correction of $\Phi$.
\begin{lemma} \label{lem:transition_idx_sub}
    Let $p, q$ denote probability distributions on the system and environment indices respectively and $\tau$ the transition amplitude from Theorem \ref{thm:second_order_transition_coeffs}. Then we have
    \begin{equation}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | k,l) = \sum_{i \neq k} \sum_{j,l} (p(i) q(j) - p(k) q(l)) \tau(i,j |k,l)
    \end{equation}
\end{lemma}
\begin{proof}
    \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | k,l) &= \sum_{i \neq k} \sum_{j,l} \parens{p(i) q(j) \tau(i,j|k,l)} + \sum_{j,l} p(k) q(j) \tau(k,j | k,l) \label{eq:n_qubit_fixed_pt_intermediate_1}.
    \end{align}
    We expand the simpler sum on the right:
    \begin{align}
        \sum_{j,l}p(k) q(j) \tau(k,j| k,l) &= p(k) q(0) (\tau(k,0|k,0) + \tau(k,0|k,1)) + p(k) b(1) (\tau(k,1|k,0) + \tau(k,1|k,1)) \\
        &= - p(k) q(0) \sum_{c \neq k}\sum_{l} \tau(k,0 | c, l) - p(k) b(1) \sum_{c \neq k} \sum_{l}\tau(k,1|c,l) \\
        &= - \sum_{c \neq k} \sum_{j,l} p(k) q(j) \tau(k,j |c,l).
    \end{align}
    Plugging this into Eq. \ref{eq:n_qubit_fixed_pt_intermediate_1} allows us to simplify as follows
    \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j|k,l) &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) \tau(i,j|k,l)) - \sum_{c \neq k} \sum_{j,l} p(k) q(j) \tau(k,j|c,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) \tau(i,j|k,l)) - \sum_{i \neq k} \sum_{j,l} p(k) q(l) \tau(i,j|k,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) - p(k) q(l)) \tau(i,j | k,l).
    \end{align}
\end{proof}

This lemma is also a purely technical one that allows us to substitute in simple values for the sums that appear in the transition terms. 
\begin{lemma} \label{lem:big_tau_sum_simplifier}
    Assume that $H_S$ is a $(\Delta_{\min}, \Delta_{\sinc}, \epsilon_{\sinc}$)-separated Hamiltonian satisfying Def. \ref{def:separated_hamiltonians}. Let $(\gamma_U,\gamma_L)$ denote the unique index pair such that $|\Delta_S(\gamma_U,\gamma_L) - \gamma| \leq \Delta_{\sinc}$ and for all $(i,j) \neq (\gamma_U, \gamma_L)$ we have $|\Delta_S(i,j) - \gamma| \geq \Delta_{\min}$. We let $\gamma_U$ denote the higher energy state, i.e $\lambda_S(\gamma_L) < \lambda_S(\gamma_U)$. Using $\delta \coloneqq \beta_E - \beta$, we show that the assumptions on $H$ and $\gamma$ along with the requirement $\delta \geq \beta_E \frac{\Delta_{\min}}{\Delta_S(u,l)} + \frac{1}{\Delta_S(u,l)} \ln \parens{\frac{1}{(2\dim + 1) \epsilon_{\sinc}}}$ imply the following bounds.
    Let $p(i) = \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)}$ and $q(j) = \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)}$. 
    \begin{align}
        \abs{\sum_{i, j, l} p(i) q(j) \tau(i,j| \gamma_L, l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} &\leq 4 \alpha^2 t^2 \epsilon_{\sinc}  \\
        \abs{\sum_{i, j, l} p(i) q(j) \tau(i,j| \gamma_U, l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_L) q(1)} &\leq 4  \alpha^2 t^2 \epsilon_{\sinc} \\
        k \neq \gamma_L, \gamma_U \implies \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j| k, l) } &\leq 4 \alpha^2 t^2 \epsilon_{\sinc}.
    \end{align}
\end{lemma}
\begin{proof}
    \todo{update this lemma to use $p,q$ instead of $a,b$. }
    For simplicity, we let $a(i') = \frac{e^{-\beta \lambda_S(i')}}{\partfun_S(\beta)}$ and $b(j') = \frac{e^{-\beta_E \lambda_E(j')}}{\partfun_E(\beta_E)}$. Using Lemma \ref{lem:transition_idx_sub} we write the summation as
    \begin{align}
        &\sum_{i,j,k} \sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) = \sum_{i \neq l} \sum_{j,k} (a(i) b(j) - a(l) b(k)) \tau(i,j|l,k) \\
        &= \sum_{j,k} (a(u) b(j) - a(l) b(k)) \tau(u,j | l,k) + \sum_{i \neq l,u} \sum_{j,k} (a(i) b(j) - a(l) b(k)) \tau(i,j| l,k).
    \end{align}
    Now we utilize the fact that $i \neq u,l$ implies that $\tau(i,j|l,k) \leq \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}$, and further that $\tau(u,j|l,k) \geq \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc}$ if and only if $k = 1$ and $j = 0$. We also have that $-1 \leq a(i') b(j') - a(k') b(l') \leq 1$ for all $i', j', k', l'$ as $a,b$ are probabilities. These observations yield the upper bound
    \begin{align}
        &\sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) \nonumber \\
        &\leq (a(u) b(0) - a(l) b(1)) \tau(u,0|l,1) + 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} + 4 (\dim_S - 2) \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
        &\leq (a(u) b(0) - a(l) b(1)) \tau(u,0|l,1) + 2 \dim \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1} \\
        &\leq (a(u) b(0) - a(l) b(1)) \tau(u,0|l,1) + 2 \epsilon_{\sinc} \alpha^2 t^2.
    \end{align}
    For the purposes of the upper bound, all we require is that $e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 0$, which implies $a(u) b(0) - a(l) b(1) \leq \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)}$. This, combined with the fact that $\tau(u,0|l,1) \leq \frac{\alpha^2 t^2}{\dim + 1}$, yields our final upper bound as
    \begin{equation}
        \sum_{i,j,k} \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_E(\beta_E)} \tau(i,j|l,k) \leq \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \frac{\alpha^2 t^2}{\dim + 1} + 2 \epsilon_{\sinc} \alpha^2 t^2. \label{eq:transition_prob_upper_bound_final}
    \end{equation}
    As constant factors are relatively unimportant, we will upper bound $2 \epsilon_{\sinc} \alpha^2 t^2 \leq 4 \epsilon_{\sinc} \alpha^2 t^2$ in the final lemma statement.

    We turn our attention now to the lower bound. This proceeds in a similar manner, of isolating the indices in the sum that are active in the transitions and those that aren't.
    \begin{align}
        &\sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) \nonumber \\
        &= \sum_{j,k} (a(u) b(j) - a(l) b(k)) \tau(u,j | l,k) + \sum_{i \neq l,u} \sum_{j,k} (a(i) b(j) - a(l) b(k)) \tau(i,j| l,k) \\
        &\geq (a(u) b(0) - a(l) b(1)) \tau(u,0 | l,1) -(\tau(u,0|l,0) + \tau(u,1|l,0) + \tau(u,1|l,1)) - \sum_{i \neq l,u} \sum_{j,k} \tau(i,j| l,k) \\
        &\geq (a(u) b(0) - a(l) b(1))\tau(u,0|l,1) - \frac{\alpha^2 t^2}{\dim + 1} \parens{ 3 \epsilon_{\sinc}  + 4 (\dim_S - 2) \epsilon_{\sinc} } \\
        &\geq (a(u) b(0) - a(l) b(1))\tau(u,0|l,1) - 2 \epsilon_{\sinc} \alpha^2 t^2. \label{eq:transition_prob_lower_bound}
    \end{align}
    We now return to the prefactor $a(u) b(0) - a(l) b(1)$ in a bit more detail. We write this as follows
    \begin{align}
        a(u) b(0) - a(l) b(1) &= \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)} - \frac{e^{-\beta \lambda_S(l)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} \\
        &= \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \parens{1 - e^{-\beta_E \gamma - \beta \lambda_S(l) + \beta \lambda_S(u)}}.
    \end{align}
    

    We need to understand the function $f(\beta) = 1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)}$ a bit better. We see that in the limit as $\gamma \to \Delta_S(u,l)$ and $\beta \to \beta_E$, $f(\beta) \to 0$. In the other regime we see as $\beta_E \to \infty$ then $f(\beta) \to 1$. This suggests that we need to look at the regime where $\delta \coloneqq \beta_E - \beta$ is sufficiently large. We first investigate when this function is positive
    \begin{align}
        1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 0 &\iff \beta_E \gamma \geq \beta \Delta_S(u,l) \\
        &\iff 0 \leq \beta_E (\gamma - \Delta_S(u.l)) + (\beta_E - \beta) \Delta_S(u,l).
    \end{align}
    Now using $\delta = \beta_E - \beta$ and $|\Delta_S(u,l) - \gamma| \leq \Delta_{\sinc} \leq \Delta_{\min}$, which implies $\gamma - \Delta_S(u,l) \geq - \Delta_{\min}$, we get that 
    \begin{equation}
        -\beta_E \Delta_{\min} + \delta \Delta_S(u,l) \leq \beta_E (\gamma - \Delta_S(u,l)) + (\beta_E - \beta) \Delta_S(u,l).
    \end{equation}
    We then conclude that $0 \leq -\beta_E \Delta_{\min} + \delta \Delta_S(u,l)$, or $\delta \geq \beta_E \frac{\Delta_{\min}}{\Delta_S(u,l)}$, implies that $1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 0$. Using this, we show that $\delta \geq \frac{\beta_E \Delta_{\min} + \ln(1/\epsilon_{exp})}{\Delta_S(u,l)}$ yields the following
    \begin{align}
        \delta \geq \frac{\beta_E \Delta_{\min} + \ln(1/\epsilon_{exp})}{\Delta_S(u,l)} &\implies \beta_E \gamma - \beta \Delta_S(u,l) \geq \ln 1/ \epsilon_{exp} \\
        &\implies e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \leq \epsilon_{exp} \\
        &\implies 1 - e^{-\beta_E \gamma + \beta \Delta_S(u,l)} \geq 1 - \epsilon_{exp}.
    \end{align}

    We can profit off of the above upper bound, as well as the fact that $|\Delta_S(u,l) - \gamma| \leq \Delta_{\sinc}$ which implies $\tau(u,0|l,1) \geq \frac{\alpha^2 t^2}{\dim + 1} (1 - \epsilon_{\sinc})$, by simply plugging it into Eq. \eqref{eq:transition_prob_lower_bound} and simplifying
    \begin{equation}
        \sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) \geq \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)}\frac{\alpha^2 t^2}{\dim + 1} (1 - \epsilon_{exp})(1 - \epsilon_{\sinc}) - 2 \epsilon_{\sinc} \alpha^2 t^2.
    \end{equation}
    Using simple bounds allows us to rewrite this as
    \begin{equation}
        \sum_{i, j, k} \frac{e^{-\beta \lambda_S(i)} e^{-\beta_E \lambda_E(j)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \tau(i,j|l,k) - \frac{e^{-\beta \lambda_S(u)}}{\partfun_S(\beta) \partfun_E(\beta_E)}\frac{\alpha^2 t^2}{\dim + 1} \geq - \alpha^2 t^2 \parens{\frac{\epsilon_{exp} + \epsilon_{\sinc}}{\dim + 1} + 2 \epsilon_{\sinc}}. \label{eq:transition_prob_lower_bound_final}
    \end{equation}
    We now use the fact that we only need to control $\epsilon_{exp}$ relatively loosely to simplify the bound. Setting $\epsilon_{exp} = (2 \dim + 1) \epsilon_{exp}$ for Eq. \eqref{eq:transition_prob_lower_bound_final}, along with the inequality from Eq. \eqref{eq:transition_prob_upper_bound_final}, yields the Lemma statement.

    The last inequality we need is the rather straightforward one where $k \neq \gamma_L, \gamma_U$
    \begin{align}
        \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|k,l)} &\leq \abs{\sum_{i\neq k} \sum_{j,l} (p(i)q(j) - p(k) q(l)) \tau(i,j|k,l)} \\
        &\leq \sum_{i \neq k} \sum_{j,l} \abs{p(i) q(j) - p(k) q(l)} \tau(i,j|k,l) \\
        &\leq \sum_{i \neq k} \sum_{j,l} \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
        &\leq 4 \dim \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} \\
        &\leq 4 \alpha^2 t^2 \epsilon_{\sinc}
    \end{align}
\end{proof}

From the Detailed Balance arguments in Section \ref{sec:detailed_balance} we expect that in the infinite time limit, with perfect knowledge of the eigenvalue gaps, that the thermal state $\rho_S(\beta_E)$ is a fixed point of the channel $\Phi$. Here we give rigorous bounds showing that for all choices of the environment gap $\gamma$ we can make $\rho_S(\beta_E)$ as close to fixed as we want. As we know that $\gamma$ being far away from all eigenvalue differences yields a channel that is approximately the identity channel, aka it does nothing, we should aim to show that it is arbitrarily close to being a fixed point even when $\gamma$ is close to an eigenvalue gap $\Delta_S(i,j)$. 
\begin{theorem}[Approximate Fixed Points]
    Let $H_S$ be a well-separated Hamiltonian, per Def. \ref{def:separated_hamiltonians}. For all values of $\gamma \geq \Delta_{\min}$ we have that 
    \begin{align}
        2 \sqrt{2} \Delta_{\min} \dim \leq t& \text{ \emph{and} } \alpha \leq \min \set{\frac{\sqrt{\epsilon_{fix}}}{4 \sqrt{2} \Delta_{\min} \dim}, \parens{\frac{\epsilon_{fix}}{4 \dim^5}}^{1/3} \frac{1}{2 \sqrt{2} \Delta_{\min} }} \nonumber \\
        &\implies \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_1 \leq \epsilon_{fix}. \label{eq:beta_e_fixed_point_reqs}
    \end{align}
\end{theorem}
\begin{proof}
    We start out with the expansion of the channel
    \begin{align}
        \norm{\rho_S(\beta_E) - \Phi(\rho_S(\beta_E))}_1 &= \norm{\mathcal{T}^{(2)}(\rho_S(\beta_E), \alpha) + R_{\Phi}(\rho_S(\beta_E))}_1 \\
        &\leq \norm{\mathcal{T}^{(2)}(\rho_S(\beta_E))}_1 + \norm{R_{\Phi}(\rho_S(\beta_E))}_1 \\
        &\leq \norm{\mathcal{T}^{(2)}(\rho_S(\beta_E))}_1 + \dim \norm{R_{\Phi}} \\
        &\leq \norm{\mathcal{T}^{(2)}(\rho_S(\beta_E))}_1 + \dim \norm{R_{\Phi}}.
    \end{align}
    Taking the straightforward approach of dividing our error budget equally among these two terms will be sufficient. Starting with the $\norm{R_{\Phi}}_1$ bound we have from Lemma \ref{lem:remainder_bound} that
    \begin{align}
        \alpha t \leq \parens{\frac{\epsilon_{fix}}{4 \dim^2}}^{1/3} \implies \dim \norm{R_{\Phi}} \leq \frac{\epsilon_{fix}}{2}. \label{tmp:first_alpha_t_req_fixed_pt}
    \end{align}

    We now bound the other half of our error budget, the term resulting from $\norm{\mathcal{T}^{(2)}(\rho_S(\beta_E))}_1$. Plugging in the second order correction for the channel from Eq. \eqref{eq:second_order_channel_with_tau} gives us
    \begin{align}
        \norm{\mathcal{T}^{(2)}(\rho_S(\beta_E))}_1 &= \sum_k \abs{\sum_{i,j,l} \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)} \frac{e^{-\beta_E \lambda_S(j)}}{\partfun_E(\beta_E)} \tau(i,j|k,l)}.
    \end{align}
    For compactness, let $p(i) = \frac{e^{-\beta_E \lambda_S(i)}}{\partfun_S(\beta_E)}$ and $q(j) =\frac{e^{-\beta_E \lambda_S(j)}}{\partfun_E(\beta_E)}$. Using Lemma \ref{lem:big_tau_sum_simplifier} we can substitute this 
    \begin{align}
        &\sum_{k \neq \gamma_L, \gamma_U} \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|k,l) } + \abs{\sum_{i,j,l} p(i)q(j) \tau(i,j | \gamma_L, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j| \gamma_U, l)} \\
        &\leq (\dim_S - 2) 4 \alpha^2 t^2 \epsilon_{\sinc} + \abs{\sum_{i,j,l} p(i)q(j) \tau(i,j | \gamma_L, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j| \gamma_U, l)}. \label{int:fixed_pt_1}
    \end{align}
    Now we use the following upper bound
    \begin{align}
        \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|\gamma_L ,l)} &= \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|\gamma_L ,l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0) + \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} \\
        &= \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|\gamma_L ,l) - \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} + \abs{\frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0)} \\
        &\leq 4 \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} p(\gamma_U) q(0).
    \end{align}
    Plugging this into Eq. \eqref{int:fixed_pt_1} yields
    \begin{align}
        \norm{\mathcal{T}^{(2)} (\rho_S(\beta_E))}_1 &\leq 2 \dim \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1}(p(\gamma_U) q(0) + p(\gamma_L) q(1)) \\
        &\leq 2 \dim \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} \\
        &= \alpha^2 t^2 \parens{2 \dim \epsilon_{\sinc} + \frac{1}{\dim + 1}}.
    \end{align}
    Now here we use the requirement that $\epsilon_{\sinc} \leq \frac{1}{\dim + 1} \frac{1}{2 \dim}$ to yield the inequality
    \begin{equation}
        \norm{\mathcal{T}^{(2)}(\rho_S(\beta_E))}_1 \leq 2 \alpha^2 t^2. \label{eq:upper_bound_on_T_2}
    \end{equation}
    We note that $\epsilon_{\sinc} \leq \frac{1}{2 \dim (\dim + 1)}$ is equivalent to the lower bound on $t$ of $2 \sqrt{2} \Delta_{\min} \dim \leq t$. Scaling of $t$ with dimension is expected in the worst case. 

    It is then straightforward to see that $\alpha^2 t^2 \leq \frac{\epsilon_{fix}}{4}$ implies $\norm{\mathcal{T}^{(2)}(\rho_S(\beta_E))} \leq \epsilon_{fix} / 2$, which along with Eq. \eqref{tmp:first_alpha_t_req_fixed_pt} implies Eq. \eqref{eq:beta_e_fixed_point_reqs}. 
\end{proof}

We now have shown that the thermal state at the same temperature as the environment, $\rho_S(\beta_E)$, remains approximately fixed even when we choose a $\gamma$ that causes transitions in the system. This argument relies on taking a very small value of $\alpha$. However, as $\alpha$ gets too small and approaches zero we have that the channel acts as time evolution by non-interacting Hamiltonians $H_S$ and $H_E$, meaning that every state that is diagonal in the $H_S$ basis could also be an approximate fixed point! We show that this is not the case and that there is a temperature $\beta$ that causes $\rho_S(\beta)$ to not be an approximate fixed point. This relies on $\gamma$ being close to an eigenvalue difference $\Delta_S(i,j)$, as otherwise our channel approximates the identity on matrices diagonal in the $H_S$ basis. 
\begin{theorem}[Not Fixed Points] \label{thm:which_beta_not_fixed}
    Let $H_S$ be a well-separated Hamiltonian, $\gamma$ be $\Delta_{\sinc}$ close to $\Delta_S(\gamma_U, \gamma_L)$, and $\delta = \beta_E - \beta$. We show that
    \begin{equation}
        \delta \geq \delta_{LB} \implies \norm{\rho_S(\beta) - \Phi^{(2)}(\rho_S(\beta))}_1 \geq \epsilon_{fix}.
    \end{equation}
\end{theorem}
\begin{proof}
\textbf{THIS SHOULD NOT BE A PROOF IT IS INCOMPLETE}


     We use the expansion of $\Phi^{(2)}(X) = X + \mathcal{T}^{(2)} (X)$, along with defining $p(i) = \frac{e^{-\beta \lambda_S(i)}}{\partfun_S(\beta)}$ and $q(j) = \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)}$ to write
     \begin{align}
        &\norm{\rho_S(\beta) - \Phi^{(2)} (\rho_S(\beta))}_1 = \norm{\mathcal{T}^{(2)}(\rho_S(\beta))}_1 = \sum_k \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | k,l)} \\
        &= \sum_{k \neq \gamma_U, \gamma_L} \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j|k,l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_U, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l)} \\
        &\geq \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_U, l)} + \abs{\sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l)} \\
        &\geq \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_U, l) + \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l)
     \end{align}
     
     It will prove simpler to lower bound one of the two terms and the other will follow almost the exact same manner.
     \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j| \gamma_U,l) &= \sum_{i \neq \gamma_U} \sum_{j,l} (p(i) q(j) - p(\gamma_U) q(l)) \tau(i,j|\gamma_U, l) \\
        &= (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) + \sum_{(j,l) \neq (1, 0)} (p(\gamma_L) q(j) - p(\gamma_U) q(l)) \tau(\gamma_L, j| \gamma_U , l) + \sum_{i \neq \gamma_U, \gamma_L} \sum_{j,l} (p(i) q(j) - p(\gamma_U) q(l)) \tau(i,j | \gamma_U, l) \\
        &\geq (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) + \sum_{(j,l) \neq (1, 0)} (-1) \tau(\gamma_L, j | \gamma_U, l) + \sum_{i \neq \gamma_U, \gamma_L} \sum_{j,l} (-1) \tau(i,j | \gamma_U,l) \\
        &\geq (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) - 3 \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc} - (\dim_S - 2) 4 \frac{\alpha^2 t^2}{\dim + 1}\epsilon_{\sinc} \\
        &\geq (p(\gamma_L) q(1) - p(\gamma_U) q(0)) \tau(\gamma_L, 1| \gamma_U, 0) - 2 \dim \frac{\alpha^2 t^2}{\dim + 1} \epsilon_{\sinc}.
     \end{align}
     Now we note that $\gamma$ is within $\Delta_{\sinc}$ of $\Delta_S(\gamma_U, \gamma_L)$ which allows us to say $1 - \epsilon_{\sinc} \leq \tau(\gamma_U, 0 | \gamma_L, 1) \frac{\dim + 1}{\alpha^2 t^2} \leq 1$ per Corollary \ref{cor:gamma_difference_reqs}. This simplifies the above bound to
     \begin{equation}
        (p(\gamma_L) q(1) - p(\gamma_U)q(0)) \frac{\alpha^2 t^2}{\dim + 1} - (2 \dim + 1) \epsilon_{\sinc} \frac{\alpha^2 t^2}{\dim + 1}.
     \end{equation}
     This is rather unfortunate, because it is the wrong term we should analyze first. To see this, as $\beta_E \to \infty$ we have $q(0) \to 1, q(1) \to 0$. This means the above quantity is negative, implying it only contributes to the norm lower bound because of the absolute value. We instead look for the opposite term.

     Writing out the same for $\gamma_L$ yields
     \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l) &\geq \parens{(p(\gamma_U) q(0) - p(\gamma_L) q(1))  - (2 \dim + 1) \epsilon_{\sinc}} \frac{\alpha^2 t^2}{\dim + 1}.
     \end{align}
     Now we see that this should be positive. To do so, we analyze the probability differences in more detail.
     \begin{align}
        p(\gamma_U) q(0) - p(\gamma_L) q(1) &= \frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(0)}}{\partfun_E(\beta_E)} - \frac{e^{-\beta \lambda_S(\gamma_L)}}{\partfun_S(\beta)} \frac{e^{-\beta_E \lambda_E(1)}}{\partfun_E(\beta_E)} \\
        &= \frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta) \partfun_E(\beta_E)} \parens{1 - e^{-\beta_E \gamma + \beta \Delta_S(\gamma_U, \gamma_L)}}.
     \end{align}
     For now, let $\Delta_S = \Delta_S(\gamma_U, \gamma_L)$ to save space. We show that $\delta \geq \frac{\beta_E \Delta_{\min} + \ln (1 / \epsilon_{exp})}{\Delta_S} \geq \beta_E \frac{\Delta_{\min}}{\Delta_S}$ implies $1 - e^{-\beta_E \gamma + \beta \Delta_S} \geq 1 - \epsilon_{exp}$. Note the assumption $\ln (1/ \epsilon_{exp}) \geq 0$. 
     \begin{align}
        \ln (1 / \epsilon_{exp}) &\leq \delta \Delta_S - \beta_E \Delta_{\min} \\
        \iff \ln (1 / \epsilon_{exp}) &\leq \beta_E (\Delta_S - \Delta_{\min}) - \beta \Delta_S \\
        &\leq \beta_E \gamma - \beta \Delta_S \\
        \iff \epsilon_{exp} &\geq e^{-\beta_E \gamma + \beta \Delta_S} \\
        \iff 1 - e^{-\beta \gamma + \beta \Delta_S}  &\geq 1 - \epsilon_{exp}.
     \end{align}

     Plugging this in, along with a simple substitution that $\epsilon_{exp} = \epsilon_{\sinc}$ yields 
     \begin{align}
        \sum_{i,j,l} p(i) q(j) \tau(i,j | \gamma_L, l) &\geq \parens{\frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta) \partfun_E(\beta_E)} (1 - \epsilon_{exp}) - (2 \dim + 1) \epsilon_{\sinc}} \frac{\alpha^2 t^2}{\dim + 1} \\
        &\geq \parens{\frac{e^{-\beta \lambda_S(\gamma_U)}}{\partfun_S(\beta) \partfun_E(\beta_E)} - 2 (\dim + 1) \epsilon_{\sinc}} \frac{\alpha^2 t^2}{\dim + 1} 
     \end{align}
\end{proof}


\bibliographystyle{unsrt}
\bibliography{bib}

\appendix



\section{Thermal State Bounds}


\begin{lemma} \label{lem:thermal_state_diff_bound}
    Let $0 \leq \delta = \beta_E - \beta$ and $H_S$ be a Hamiltonian such that $\lambda_S(k) - \lambda_S(i) \geq \Delta_{\min}$ for all $k > i$. The trace distance between the thermal states $\rho_S(\beta)$ and $\rho_S(\beta_E)$ can be bounded from above and below as
    \begin{align}
        \frac{\Delta_{\min} \delta^2 \epsilon_{\rho}}{2 \dim_S} \leq \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 \leq 2 \dim_S \norm{H} \delta,
    \end{align}
    provided that $\beta_E$ satisfies the upper bound $\beta_E \leq \frac{1}{2 \norm{H}} \parens{\ln (\dim_S - 1) + \ln \left( \frac{1}{\epsilon_{\rho}} - 1\right) }$/. 
\end{lemma}
\begin{proof}
    We first decompose the trace distance using the fact that thermal states are diagonal in the $H_S$ basis
    \begin{equation}
        \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 = \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}}. \label{eq:thermal_diffs_1}
    \end{equation}
    This suggests a Taylor Series for the Boltzmann factors $\frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}$ about $\beta = \beta_E$ would be useful. We will only need the zeroth order approximation,
    \begin{equation}
        \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} = \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} + R_0(\beta, \beta_E, k),
    \end{equation}
    and will use the integral form of $R_0$ for the lower bound and the Lagrange or mean-value form for the upper bound. 
    
    As the upper bound is easier we will compute it first. The remainder in this case is given by
    \begin{equation}
        R_0(\beta, \beta_E, k) = (\beta - \beta_E) \frac{d}{d\beta} \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \bigg|_{\beta = \beta_{k}},
    \end{equation}
    where $\beta_{k} \in (\beta, \beta_E)$ is the arbitrary point guaranteed to exist by the Taylor's Theorem. To compute the derivative of the Boltzmann coefficient we first compute the derivative of the partition function
    \begin{align}
        \frac{d}{d\beta} \partfun_S(\beta) &= \frac{d}{d\beta} \sum_i e^{-\beta \lambda_S(i)} \\
        &= - \sum_i \lambda_S(i) e^{-\beta \lambda_S(i)} \\
        &= - \trace{H e^{-\beta H}}.
    \end{align}
    This gives the total derivative of the Boltzmann coefficient is computed as
    \begin{align}
        \frac{d}{d \beta}\frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} &= - \lambda_S(k) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)^2} \frac{d}{d\beta}\partfun_S(\beta) \\
        &= \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \parens{\trace{H \frac{e^{-\beta H}}{\partfun_S(\beta)}} - \lambda_S(k)}. \label{eq:boltzmann_derivative}
    \end{align}
    We can plug this in to Eq. \eqref{eq:thermal_diffs_1} to get
    \begin{align}
        \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 &= \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}} \\
        &= \sum_k \abs{R_0(\beta, \beta_E, k)} \\
        &= \sum_k \abs{\beta - \beta_E} \frac{e^{-\beta_{k} \lambda_S(k)}}{\partfun_S(\beta_{k})} \abs{\trace{\rho_S(\beta_{k})H} - \lambda_S(k)} \\
        &\leq \delta \sum_k \parens{\abs{\sum_{j} \frac{e^{-\beta_k \lambda_S(j)}}{\partfun_S(\beta_k)} \lambda_S(j)} + |\lambda_S(k)|} \\
        &\leq \delta \sum_k \parens{\norm{H} \sum_j \frac{e^{-\beta_k \lambda_S(j)}}{\partfun_S(\beta_k)} + \norm{H}} \\
        &\leq 2 \delta \dim_S \norm{H}.
    \end{align}
    This gives the upper bound required in the lemma statement.

    We now move on to the lower bound. We use the same breakdown in the eigenbasis of $H_S$, but this time we drop all terms that are not the ground state 
    \begin{align}
        \norm{\rho_S(\beta_E) - \rho_S(\beta)}_1 &= \sum_k \abs{\frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}} \\
        &\geq \abs{\frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)}} \\
        &= \abs{R_0(\beta, \beta_E, 0)} \\
        &\geq R_0(\beta, \beta_E, 0),
    \end{align}
    and we will stop writing the implicit argument of 0 at the end from now on. 
    
    We drop the higher order contributions for a few reasons. The first being that thermal states are typically used as close approximations to ground states, which approaches the exact ground state as $\beta_E \to \infty$, and we are imagining a scenario in which $\beta$ is somewhat close to $\beta_E$. In this situation, the dominant contribution to the trace distance comes from the difference in the ground state Boltzmann factors. The other rationale behind just studying the ground state contribution is the fact that $\beta_1 \leq \beta_2$ implies that $\frac{e^{-\beta_1 \lambda_S(0)}}{\partfun_S(\beta_1)} \leq \frac{e^{-\beta_2 \lambda_S(0)}}{\partfun_S(\beta_2)}$, or that the overlap that the thermal state has with the ground state, $\trace{\Pi_0 \rho_S(\beta)}$, is monotonically increasing with $\beta$. This is shown using a straightforward lower bound on the first derivative computed in Eq. \eqref{eq:boltzmann_derivative}:
    \begin{align}
        \frac{d}{d\beta} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} &= \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{ \sum_k \left(\lambda_S(k) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \right) - \lambda_S(0) } \\
        &= \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{ \sum_k \left(\lambda_S(k) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} \right) - \lambda_S(0) \sum_k \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(k)} } \\
        &= \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \sum_k (\lambda_S(k) - \lambda_S(0)) \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)}  \\
        &\geq \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \Delta_{\min} \sum_{k > 0} \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(k)} \\
        &= \Delta_{\min} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{1 - \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)}}, \label{eq:boltzmann_derivative_lower_bound}
    \end{align}
    where we see that this is always positive for finite $\beta$ and 0 in the limit $\beta \to \infty$.  
    
    To compute this lower bound we will use the integral version of the remainder $R_0$, which is given as 
    \begin{align}
        R_0(\beta, \beta_E) = \int_{\beta}^{\beta_E} (x - \beta) \frac{d}{d\beta} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)}\bigg|_{\beta = x} dx ~.
    \end{align}
    Plugging in the derivative computed in Eq. \eqref{eq:boltzmann_derivative} gives
    \begin{align}
        R_0(\beta, \beta_E) = \int_{\beta}^{\beta_E} (x - \beta) \frac{e^{-x \lambda_S(0)}}{\partfun_S(x)}\parens{\trace{H \frac{e^{- x H}}{\partfun_S(x)}} - \lambda_S(0)} dx,
    \end{align}
    where we note that this is positive as $x > \beta$ and the derivative is always positive, as noted above. Plugging in the lower bound computed in Eq. \eqref{eq:boltzmann_derivative_lower_bound} yields
    \begin{align}
        R_0(\beta, \beta_E) &\geq \Delta_{\min} \int_{\beta}^{\beta_E} (x - \beta) \frac{e^{-x \lambda_S(0)}}{\partfun_S(x)} \parens{1 - \frac{e^{-x \lambda_S(0)}}{\partfun_S(x)}} dx \\
        &\geq \Delta_{\min} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{1 - \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)}} \int_{\beta}^{\beta_E} (x - \beta) dx \\
        &= \frac{\Delta_{\min} (\beta_E - \beta)^2}{2} \frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \parens{1 - \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)}}. \label{eq:boltzmann_lower_bound_penultimate}
    \end{align}

    The last step is to lower bound the the Boltzmann factors as seen above. We can use the simplistic bound $\frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \geq \frac{1}{\dim_S}$ to take care of the one factor, and the other we need to show a bound of $\parens{1 - \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E}} \geq \epsilon_{\rho}$
    This will be done through a bound on the partition function, seen below
    \begin{align}
        \partfun_S(\beta_E) &=  \sum_k e^{-\beta_E \lambda_S(k)} = e^{-\beta_E \lambda_S(0)} \parens{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))}}.
    \end{align}
    Rearranging yields
    \begin{align}
        \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} &= \frac{1}{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))}}.
    \end{align}
    To upper bound this quantity we use fact that $\lambda_S(k) - \lambda_S(0) \leq 2 \norm{H}$ holds for all $k$, which yields
    \begin{align}
        -\beta_E(\lambda_S(k) - \lambda_S(0)) &\geq -\beta_E 2 \norm{H} \\
        \implies e^{-\beta_E (\lambda_S(k) - \lambda_S(0))} &\geq e^{-\beta_E 2 \norm{H}} \\
        \implies \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))} &\geq (\dim_S - 1) e^{-\beta_E 2 \norm{H}} \\
        \implies \frac{1}{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))} } &\leq \frac{1} {1 + (\dim_S - 1) e^{-\beta_E 2 \norm{H}} } .
    \end{align}
    Now this serves as an upper bound on $\frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)}$. We would therefore like that 
    \begin{align}
        \frac{e^{-\beta_E \lambda_S(0)}}{\partfun_S(\beta_E)} = \frac{1}{1 + \sum_{k > 0} e^{-\beta_E (\lambda_S(k) - \lambda_S(0))}} &\leq \frac{1}{1 + (\dim_S - 1) e^{-\beta_E 2 \norm{H}}} \\
        &\leq 1 - \epsilon_{\rho} \label{eq:epsilon_rho_inequality}
    \end{align}
    Through straightforward algebra, it can be seen that requiring 
    \begin{equation}
        \beta_E \leq \frac{1}{2 \norm{H}} \parens{\ln (\dim_S - 1) + \ln \left( \frac{1}{\epsilon_{\rho}} - 1\right) } \label{eq:beta_e_upper_bound_thermal_diffs}
    \end{equation}
    is sufficient to satisfy the above inequality in Eq. \eqref{eq:epsilon_rho_inequality}. Plugging Eq. \eqref{eq:epsilon_rho_inequality}, along with the inequality $\frac{e^{-\beta \lambda_S(0)}}{\partfun_S(\beta)} \geq \frac{1}{\dim_S}$, into Eq. \eqref{eq:boltzmann_lower_bound_penultimate} yields
    \begin{align}
        R_0(\beta, \beta_E) \geq \frac{\epsilon_{\rho} \Delta_{\min} \delta^2}{2 \dim_S},
    \end{align}
    which gives the lemma statement. 

\begin{lemma} \label{lem:magic_index}
    Given a Hamiltonian $H$ with minimum eigenvalue difference $\Delta_{\min}$, if $0 \le \beta_E - \beta \le \frac{\Delta_{\min}}{2 \dim_S \norm{H}^2}$ then the difference in Boltzmann distributions changes signs once and only once, or in other words there exists an index $j^\star$ such that
    % \begin{equation}
    %      j \le j^\star \iff \frac{e^{-\beta_E \lambda(j)}}{\partfun(\beta_E)} -\frac{e^{-\beta \lambda(j)}}{\partfun(\beta)} \ge 0 .
    % \end{equation}
\end{lemma}
\begin{proof}
\begin{align}
    R_0(\beta, \beta_E, k) &= (\beta - \beta_E) p_{\beta_{\star}}(k) \parens{\parens{\sum_i p_{\beta_{\star}}(i) \lambda_S(i)} - \lambda_S(k)}.
\end{align}
Now we know that $\beta_{\star} \in (\beta, \beta_E)$. What we want to do is squeeze this range so small such that the difference in the mean energies $\anglebrackets{H}_{\beta_{\star}} \coloneqq \sum_i p_{\beta_{\star}}(i) \lambda_S(i)$ at $\beta$ and $\beta_E$ are very close. Below, let $\beta_i$ denote the mean value temperature guaranteed for $R_0(\beta, \beta_E, i)$. Also, it will prove sufficient to upper bound the absolute value
\begin{align}
    \abs{\anglebrackets{H}_{\beta} - \anglebrackets{H}_{\beta_E}} &= \left| \sum_i (p_{\beta}(i) - p_{\beta_E}(i)) \lambda_S(i) \right| \\
    &\leq \sum_i \abs{R_0(\beta, \beta_E, i)} \abs{\lambda_S(i)} \\
    &\leq \norm{H} \sum_i |\beta - \beta_E| p_{\beta_i}(i) \abs{\sum_j p_{\beta_i}(j) \lambda_S(j) - \lambda_S(i)} \\
    &\leq \norm{H} \delta \sum_i p_{\beta_i}(i) \parens{\sum_{j} p_{\beta_i}(j) \abs{\lambda_S(j)} + \abs{\lambda_S(i)}} \\
    &\leq 2 \norm{H}^2 \delta \sum_i p_{\beta_i}(i) \\
    &\leq 2 \dim_S \delta \norm{H}^2.
\end{align}

Now we profit, let $\beta_j$ denote the magic $\beta$ for index $j$ and $\beta^\star$ likewise for $j^\star$. 
\begin{align}
    \anglebrackets{H}_{\beta_j} - \lambda_S(j) &= \anglebrackets{H}_{\beta_j} - \anglebrackets{H}_{\beta^\star} + \anglebrackets{H}_{\beta^\star} - \lambda_S(k_1) + \lambda_S(k_1) - \lambda_S(k_2) \\
    &\leq \anglebrackets{H}_{\beta_2} - \anglebrackets{H}_{\beta_1} + \lambda_S(k_1) - \lambda_S(k_2) \\
    &\leq \anglebrackets{H}_{\beta} - \anglebrackets{H}_{\beta_E} - \Delta_{\min} \\
    &\leq \abs{\anglebrackets{H}_{\beta} - \anglebrackets{H}_{\beta_E}} - \Delta_{\min} \\
    &\leq 0.
\end{align}

 \end{proof}

To determine the value of $j^\star$, it turns out that the integral form of the remainder is most helpful.
$$p_{\beta_E}(j) - p_{\beta}(j) = \int_{\beta}^{\beta_E} (x - \beta) \frac{e^{-x \lambda_S(j)}}{\partfun_S(x)} \parens{\lambda_S(j) - \trace{H_S \frac{e^{-x \lambda_S(j)}}{\partfun_S(x)}}} dx$$
actually no its not. I still have no clue how to approach this shit. 
\begin{align}
    \frac{\partial}{\partial j} \frac{e^{-\beta_E j}}{\partfun_S(\beta_E)} - \frac{e^{-\beta j }}{\partfun_S(\beta)} &= -\beta_E \frac{e^{-\beta_E j}}{\partfun_S(\beta_E)} + \beta \frac{e^{-\beta j }}{\partfun_S(\beta)} \\
    &= \beta_E \frac{e^{-\beta_E j}}{\partfun_S(\beta_E)} \left( \frac{\beta}{\beta_E} e^{\delta j} \frac{\partfun_S(\beta_E)}{\partfun_S(\beta)} - 1 \right)
\end{align}
I think we may have to make the $\delta$ small restriction in order to say that the thermal state differences only change once? 

 
    \begin{lemma}
        Let $H_S$ be the truncated harmonic oscillator Hamiltonian. We will compute a Taylor's series for the Boltzmann distribution at $\beta$ about an environment temperature of $\beta_E$. The difference between these inverse temperatures is denoted $\delta = \beta_E - \beta$
        \begin{align}
            \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} &= \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \delta \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} \parens{\trace{H_S \frac{e^{-\beta_E H_S}}{\partfun_S(\beta_E)}} - \lambda_S(k)} + R_{\delta}
        \end{align}
        where we bound $|R_{\delta}| \in \bigo{\delta^2 \dim_S^2}$.
    \end{lemma}
    now what we need from this lemma is when is this remainder positive or negative. when is
    \begin{align}
        \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} - \frac{e^{-\beta \lambda_S(k)}}{\partfun_S(\beta)} &\geq 0 \\
        \delta \frac{e^{-\beta_E \lambda_S(k)}}{\partfun_S(\beta_E)} \parens{\trace{H_S \frac{e^{-\beta_E H_S}}{\partfun_S(\beta_E)}} - \lambda_S(k)} &\geq R_{\delta} \\
        \trace{H_S \frac{e^{-\beta_E H_S}}{\partfun_S(\beta_E)}} - \frac{R_{\delta}}{\delta p_{\beta_E}(k)} &\geq k.
    \end{align}
    Now we can plug in for the Harmonic oscillator to solve for the average energy at a given environment temperature $\beta_E$. To do so we need an expression for the partition function, which can be computed analytically for this system:
    \begin{equation}
        \partfun_S(\beta) = \trace{e^{-\beta H_S}} = \sum_{i = 1}^{\dim_S} e^{-\beta i} = \frac{e^{-\beta} (1 - e^{-\beta \dim_S})}{1 - e^{-\beta}}.
    \end{equation}
    Now we can use this to compute the average energy of a Gibbs state using the derivative of the partition function
    \begin{align}
        \trace{H_S e^{-\beta H_S}} &= - \frac{\partial}{\partial \beta} \partfun_S(\beta) \\
        &= - \frac{\partial}{\partial \beta} \frac{e^{-\beta} (1 - e^{-\beta \dim_S})}{1 - e^{-\beta}} \\
        &= \frac{1}{(e^{\beta} - 1)^2} - e^{-\beta \dim_S} \parens{\frac{\dim_S}{e^{\beta} - 1} + \frac{1}{(e^{\beta} - 1)^2}}.
    \end{align}
    Now dividing by the partition function yields the expected energy
    \begin{align}
        \trace{H_S \frac{e^{-\beta H_S}}{\partfun_S(\beta )}} &= \left( \frac{1 - e^{-\beta \dim_S}}{e^{\beta} - 1} \right)^{-1} \parens{\frac{1}{(e^{\beta} - 1)^2} - e^{-\beta \dim_S} \parens{\frac{\dim_S}{e^{\beta} - 1} + \frac{1}{(e^{\beta} - 1)^2}}} \\
        &= \frac{1}{(e^{\beta} - 1)(1 - e^{-\beta \dim_S} )} - \frac{e^{-\beta \dim_S}}{1 - e^{-\beta \dim_S}} \parens{\dim_S + \frac{1}{e^{\beta} - 1}} \\
        &= \frac{1}{e^{\beta} - 1} - \frac{\dim_S}{e^{\beta \dim_S} - 1}.
    \end{align}
    We see in the large beta limit this approaches $e^{-\beta} - e^{-\beta \dim_S}$ as it should. This expression is invalid in the $\beta \to 0$ limit as our use of the geometric series becomes faulty. 
\end{proof}



\section{Some Scratch}
\begin{lemma} \label{lem:expected_second_order_term}
    Let $T_{\gamma}^{(2)}(\rho_S(\beta))$ denote the second order term for a thermalizing channel, with a fixed value of $\gamma$, as defined in Eq. \eqref{def:taylor_series_terms}. We compute the expectation value of the $k$-th component of this matrix with respect to $\gamma$ as chosen from the perfect knowledge distribution of a well-separated Hamiltonian $H_S$, per Definition \ref{def:separated_hamiltonians}, as 
    \begin{align}
        \abs{\mathbb{E}_{\gamma} \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} - \frac{\alpha^2 t^2}{\dim + 1} \frac{2}{\dim_S (\dim_S - 1)} \sum_{i \neq k} \parens{ p(i) \frac{1}{1 + e^{-\beta_E |\Delta_S(i,k)|}}  - p(k) \frac{e^{-\beta_E |\Delta_S(i,k)|}}{1 + e^{-\beta_E |\Delta_S(i,k)|}} }} \leq  \parens{\frac{4 \alpha}{\Delta_{\min}}}^2
    \end{align}
\end{lemma}
\begin{proof}
    We start by decomposing $T_{\gamma}^{(2)}$ for a \emph{fixed} value of $\gamma$, using the results of Lemma \ref{lem:transition_idx_sub}
    \begin{align}
        \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} &= \sum_{i,j,l} p(i) q(j) \tau(i,j|k,l) \\
        &= \sum_{i \neq k} \sum_{j,l} (p(i) q(j) - p(k) q(l)) \tau(i,j| k,l) \\
        &= \frac{\alpha^2 t^2}{\dim + 1} \sum_{i \neq k} \sum_{j,l} \parens{ p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2).
    \end{align}
    To tackle the sum over $i$, we note that $\Delta_S(i,k)$ is positive if $i > k$ and negative if $i < k$, due to the sorting of eigenvalues. We will first show the $i > k$ condition and the other case is the same argument.

    We expand the $j,l$ summation and note that 3 of the resulting terms are in the tail end of the sinc function and can therefore be upper bounded by $\epsilon_{\sinc}$, as $\Delta_S(i,k) \geq \Delta_{\min}$
    \begin{align}
        & \sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &= \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)}} \sinc^2(\Delta_S(i,k)t/2) \nonumber \\
        &\quad + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2) \nonumber \\
        &\quad + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \gamma) t/2) \nonumber \\
        &\quad + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k)) t/2) \label{tmp:expected_second_order_coeff_1} \\
        &\leq 1 \cdot \sinc^2(\Delta_S(i,k) t/2) + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2) \nonumber \\
        &\quad + 1 \cdot \sinc^2((\Delta_S(i,k) + \gamma)t/2) + 1 \cdot \sinc^2(\Delta_S(i,k) t/2) \\
        &\leq 3 \epsilon_{\sinc} + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2).
    \end{align}
    Similar arguments show that Eq. \eqref{tmp:expected_second_order_coeff_1} is lower bounded by $-3 \epsilon_{\sinc} + \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2)$. 

    Now we look at the case in which $i < k$. We have the same setup in Eq. \eqref{tmp:expected_second_order_coeff_1}, but $\Delta_S(i,k) \leq -\Delta_{\min}$. This gives virtually the same result as above, but the term that is not small is the term with $+ \gamma$ in the argument of the sinc function as opposed to the $-\gamma$ term. So for $i < k$ we have
    \begin{align}
        & \sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &\leq 3 \epsilon_{\sinc} + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)} } \sinc^2((\Delta_S(i,k) + \gamma) t/2) \\
        &= 3 \epsilon_{\sinc} + \parens{p(i) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)} - p(k) \frac{1}{\partfun_E(\beta_E)} } \sinc^2((|\Delta_S(i,k)| - \gamma) t/2).
    \end{align}
    Notice how the factor of $e^{-\beta_E \gamma}$ switched in the $i < k$ case from the $i > k$ one. Similar arguments give the same lower bound as before. 

    Now we introduce the expectation over $\gamma$, as expectations are linear we can do this term by term over the sum on $i \neq k$ in Eq. idk. As $H_S$ is well-separated, and $\gamma$ is distributed assuming perfect knowledge, then for each $i \neq k$ we will sample $\gamma = \Delta_S(i,k)$ exactly and every other term will be $\gamma \sim \Delta_S(a,b)$ for $(a,b) \neq (i,k)$. These other terms leave the resulting sinc function to be $\epsilon_{\sinc}$ small. We again will analyze this expectation first for terms in which $i > k$ and then show the $i < k$ case. \matt{This is where we need to tackle the $H_S$ well-separated condition.}

    \begin{align}
        & \mathbb{E}_{\gamma}\sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &\leq \mathbb{E}_{\gamma} 3 \epsilon_{\sinc} + \mathbb{E}_{\gamma} \parens{p(i) \frac{1}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \gamma}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) - \gamma) t/2) \\
        &= 3 \epsilon_{\sinc} + \sum_{(a,b) : a > b} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(a,b)}} - p(k) \frac{e^{-\beta_E \Delta_S(a,b)}}{1 + e^{-\beta_E \Delta_S(a,b)}}} \sinc^2((\Delta_S(i,k) - \Delta_S(a,b)) t/2) \\
        &= 3 \epsilon_{\sinc} \nonumber \\
        &+ \sum_{(a,b): a > b, (a,b) \neq (i,k)} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(a,b)}} - p(k) \frac{e^{-\beta_E \Delta_S(a,b)}}{1 + e^{-\beta_E \Delta_S(a,b)}}} \sinc^2((\Delta_S(i,k) - \Delta_S(a,b)) t/2) \nonumber \\
        &+ \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}}} \sinc^2((\Delta_S(i,k) - \Delta_S(i,k)) t/2) \\
        &\leq 3 \epsilon_{\sinc} + \epsilon_{\sinc} \frac{2}{\dim_S (\dim_S - 1)} \sum_{(a,b): a > b, (a,b) \neq (i,k)} 1 \nonumber \\
        &\quad + \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}}} \\
        &\leq 4 \epsilon_{\sinc} + \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}}}.
    \end{align}
    Similar lower bounds simply swap the sign of $4 \epsilon_{\sinc}$ to $- 4 \epsilon_{\sinc}$. As with the above, in the situation with $i < k$ we simply move the $e^{-\beta_E \gamma}$ factor, as the sinc function evaluates to 1 in term in the expectation that actually contributes
    \begin{align}
        & \mathbb{E}_{\gamma}\sum_{j,l} \parens{p(i) \frac{e^{-\beta_E \lambda_E(j)}}{\partfun_E(\beta_E)} - p(k) \frac{e^{-\beta_E \lambda_E(l)}}{\partfun_E(\beta_E)}} \sinc^2((\Delta_S(i,k) + \Delta_E(j,l)) t/2) \nonumber \\
        &\leq 4 \epsilon_{\sinc} + \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}}}.
    \end{align}
    This then leads to the bounds for the expectation value of $T_{\gamma}^{(2)}$ as 
    \begin{align}
        \mathbb{E}_{\gamma} \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} &\leq \frac{\alpha^2 t^2}{\dim + 1} \sum_{i \neq k} 4 \epsilon_{\sinc} \nonumber \\
        &\quad + \frac{\alpha^2 t^2}{\dim + 1} \sum_{i < k} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}}} \nonumber \\
        &\quad + \frac{\alpha^2 t^2}{\dim + 1} \sum_{i > k} \frac{2}{\dim_S (\dim_S - 1)} \parens{p(i) \frac{1}{1 + e^{-\beta_E \Delta_S(i,k)}} - p(k) \frac{e^{-\beta_E \Delta_S(i,k)}}{1 + e^{-\beta_E \Delta_S(i,k)}} } \\
        &\leq 4 \alpha^2 t^2 \epsilon_{\sinc} + \frac{\alpha^2 t^2}{\dim + 1} \frac{2}{\dim_S (\dim_S - 1)} \sum_{i \neq k} \parens{ p(i) \frac{1}{1 + e^{-\beta_E |\Delta_S(i,k)|}}  - p(k) \frac{e^{-\beta_E |\Delta_S(i,k)|}}{1 + e^{-\beta_E |\Delta_S(i,k)|}} }.
    \end{align}
    The exact same lower bound but with $4 \alpha^2 t^2 \epsilon_{\sinc}$ goes to $-4\alpha^2 t^2 \epsilon_{\sinc}$ holds. This, along with $\epsilon_{\sinc} = \frac{4}{t^2 \Delta_{\min}^2}$, gives the final result
    \begin{align}
        \abs{\mathbb{E}_{\gamma} \bra{k} T_{\gamma}^{(2)}(\rho_S(\beta)) \ket{k} - \frac{\alpha^2 t^2}{\dim + 1} \frac{2}{\dim_S (\dim_S - 1)} \sum_{i \neq k} \parens{ p(i) \frac{1}{1 + e^{-\beta_E |\Delta_S(i,k)|}}  - p(k) \frac{e^{-\beta_E |\Delta_S(i,k)|}}{1 + e^{-\beta_E |\Delta_S(i,k)|}} }} \leq  \parens{\frac{4 \alpha}{\Delta_{\min}}}^2
    \end{align}
\end{proof}

\section{Haar Integral Proofs} \label{sec:haar_integral_appendix}

In this section we present the more technical work needed to state our results in Section \ref{sec:taylor_series_phi}. The first two results are Lemmas that compute the effects of the randomized interaction in a form that are usable in the main result of Theorem \ref{lem:big_one}.

\begin{lemma} \label{lem:two_heisenberg_interactions}
    Let $G(t)$ denote the Heisenberg evolved random interaction $G(t) = e^{iHt} G e^{-iHt}$ for a total Hamiltonian $H$. After averaging over the interaction measure the product $G(x) G(y)$ can be computed as
    \begin{equation}
        \int G(x) G(y) dG = \frac{1}{\dim + 1} \parens{\sum_{(i,j),(k,l)} e^{i \Delta(i,j|k,l) (x-y)} \ketbra{i,j}{i,j} + \identity}.
    \end{equation}
\end{lemma}
\begin{proof}
The overall structure of this proof is to evaluate the product in the Hamiltonian eigenbasis and split the product into three factors: a phase contribution from the time evolution, a Haar integral from the eigenvalues of the random interaction, and the eigenvalue contribution of the random interaction. Since this involves the use of multiple indices, it will greatly simplify the proof to use a single index over the total Hilbert space $\hilb$ as opposed to two indices over $\hilb_S \otimes \hilb_E$. For example, the index $a$ should be thought of as a pair $(a_s, a_e)$, and functions $\lambda(a)$ should be thought of as $\lambda(a_s, a_e)$. Once the final form of the expression is reached we will substitute in pairs of indices for easier use of the lemma in other places.
    \begin{align}
        \int G(x) G(y) dG &= \int e^{+i H x} U_G D U_G^\dagger e^{-i H x} e^{+i H y} U_G D U_G^\dagger e^{-i H y} dU_G ~dD \\
        &= \int \bigg[\sum_a e^{+i \lambda(a)x}\ketbra{a}{a}  U_G \sum_b D(b)\ketbra{b}{b} U_G^\dagger \nonumber \\
        &\quad \sum_c e^{-i \lambda(c) (x - y)} \ketbra{c}{c} U_G \sum_d D(d)\ketbra{d}{d} U_G^\dagger \sum_e e^{-i \lambda(e) y} \ketbra{e}{e} \bigg] dU_G ~dD\\
        &=\sum_{a,b,c,d,e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \nonumber \\
        &\quad \times \int \bra{a} U_G \ket{b} \bra{c} U_G \ket{d} \bra{b} U_G^{\dagger} \ket{c} \bra{d} U_G^\dagger \ket{e} dU_G \int D(b) D(d) dD \\
        &=  \sum_{a, b, c, d, e} \delta_{bd} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \nonumber \\
        &\quad \times \int \bra{a} U_G \ket{b} \bra{c} U_G \ket{d} \bra{b} U_G^{\dagger} \ket{c} \bra{d} U_G^\dagger \ket{e} dU_G. \\
    \end{align}
    Now the summation over $d$ fixes $d=b$ and we use Lemma \ref{lem:haar_two_moment} to compute the Haar integral, which simplifies greatly due to the repeated $b$ index. Plugging the result into the above yields the following
    \begin{align}
        &= \frac{1}{\dim^2 - 1} \sum_{a, b, c, e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \parens{\delta_{ac} \delta_{ce} + \delta_{ae} - \frac{1}{\dim} \parens{\delta_{ac} \delta_{ce} + \delta_{ae}}}  \\
        &= \frac{1}{\dim^2 - 1} \parens{1 - \frac{1}{\dim}} \sum_{a, b, c, e} \ketbra{a}{e} e^{-i (\lambda(c) - \lambda(a))x} e^{-i (\lambda(e) - \lambda(c))y} \delta_{ae} (1 + \delta_{ac}) \\
        &= \frac{1}{\dim^2 - 1} \parens{1 - \frac{1}{\dim}} \sum_{a, b, c} \ketbra{a}{a} e^{i (\lambda(a) - \lambda(c))(x-y)} (1 + \delta_{ac}) \\
        &= \frac{1 \parens{\dim - 1}}{\dim^2 - 1} \sum_{a,c} \ketbra{a}{a} e^{i (\lambda(a) - \lambda(c))(x - y)} (1 + \delta_{ac}) \\
        &= \frac{1}{\dim + 1} \parens{\sum_{a,c} e^{i (\lambda(a) - \lambda(c))(x-y)} \ketbra{a}{a} + \identity}.
    \end{align}
    Reindexing by $a \mapsto i,j$, $c \mapsto k,l$, and plugging in the definition of $\Delta$ yields the statement of the lemma.
\end{proof}


\begin{lemma} \label{lem:sandwiched_interaction}
    Given two Heisenberg evolved random interactions, $G(x)$ and $G(y)$, we compute their action on an outer-product $\ketbra{i,j}{k,l}$ as
    \begin{equation}
        \int G(x) \ketbra{i,j}{k,l} G(y) ~dG = \frac{1}{\dim + 1} \parens{\ketbra{i,j}{k,l} + \delta(i,j|k,l) \sum_{m,n} e^{i \Delta(m,n | i,j) (x-y)} \ketbra{m,n}{m,n}}
    \end{equation}
\end{lemma}
\begin{proof}
This proof is structured the same as Lemma \ref{lem:two_heisenberg_interactions} and similarly we will use a single index of the total Hilbert space $\hilb$ and switch to two indices to match the rest of the exposition.
    \begin{align}
        \int G(x) \ketbra{a}{b} G(y) dG &=  \int e^{i H x} U_G D U_G^{\dagger} e^{-i H x} \ketbra{a}{b} e^{i H y} U_G D U_G^\dagger e^{-i H y} ~dG \\
        &= \sum_{c, d, e, f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \nonumber \\
        &\quad \times \int \ketbra{c}{c} U_G D(d) \ketbra{d}{d} U_G^\dagger \ketbra{a}{b} U_G D(e) \ketbra{e}{e} U_G^\dagger \ketbra{f}{f} dG \\
        &= \sum_{c, d, e, f}  e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} \nonumber \\
        &\quad \times \int D(d) D(e) dD \int \bra{c} U_G \ket{d} \bra{b} U_G \ket{e} \bra{d} U_G^\dagger \ket{a} \bra{e} U_G^\dagger \ket{f} dU_G \\
        &=  \sum_{c,d,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} \nonumber \\ 
        &\quad \times \int \bra{c} U_G \ket{d} \bra{b} U_G \ket{d} \bra{a} \overline{U_G} \ket{d} \bra{f} \overline{U_G} \ket{d} dU_G \\
        &= \frac{1}{\dim^2 - 1} \sum_{c,d,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} (\delta_{ca} \delta_{bf} + \delta_{cf}\delta_{ab})\parens{1 - \frac{1}{\dim}} \\
        &= \frac{1}{\dim + 1} \sum_{c,f} e^{i (\lambda(c) - \lambda(a))x} e^{i (\lambda(b) - \lambda(f))y} \ketbra{c}{f} (\delta_{ca} \delta_{bf} + \delta_{cf}\delta_{ab}) \\
        &= \frac{1}{\dim + 1} \parens{\ketbra{a}{b} + \delta_{ab} \sum_{c} e^{i(\lambda(c) - \lambda(a))(x-y)} \ketbra{c}{c} }.
    \end{align}
    Now re-indexing by $a \mapsto (i,j)$, $b \mapsto (k,l)$ and $c \mapsto (m,n)$ results in the expression given in the statement of the lemma.
\end{proof}


\secondOrderChannelHaar*
\begin{proof}
To start we would like to note that we will use a single index notation to refer to the joint system-environment eigenbasis during this proof to help shorten the already lengthy expressions. We will convert back to a double index notation to match the statement of the theorem. We start from the expression for the first derivative of the channel $\frac{\partial}{\partial \alpha} \Phi_G(\rho_S)$ given by Eq. \eqref{eq:first_order_alpha_derivative}. To take the second derivative there are six factors involving $\alpha$, so we will end up with six terms. We repeat Eq. \eqref{eq:first_order_alpha_derivative} below, add a derivative, and label each factor containing an $\alpha$ for easier computation
\begin{align}
    \frac{\partial^2}{\partial \alpha^2} \Phi_G(\rho_S) =& \frac{\partial}{\partial \alpha} \parens{\int_{0}^{1} \underset{\substack{\downarrow \\ (A)}}{e^{i s (H+\alpha G)t}} (i t G) \underset{\substack{\downarrow \\ (B)}}{e^{i (1-s) (H+\alpha G)t}} ds ~ \rho \underset{\substack{\downarrow \\ (C)}}{e^{-i(H+\alpha G)t}} } \nonumber \\
    &~ ~+\frac{\partial}{\partial \alpha} \parens{ \underset{\substack{\downarrow \\ (D)} }{e^{i(H+\alpha G)t}} \rho \int_{0}^1 \underset{\substack{\downarrow \\ (E)} }{e^{-i s (H+\alpha G) t} } (- i t G) \underset{\substack{\downarrow \\ (F)}}{e^{-i (1-s) (H+\alpha G)t}} ds }.
\end{align}
Our goal is to get each of these terms in a form in which we can use either Lemma \ref{lem:two_heisenberg_interactions} or \ref{lem:sandwiched_interaction}. 
\begin{align}
    (A) &=i t\int_0^1 \parens{\frac{\partial}{\partial \alpha} e^{i s_1 (H+ \alpha G)t}} G e^{i(1-s_1)(H+\alpha G)t} ds_1 \rho e^{-i (H+\alpha G)t} \bigg|_{\alpha=0} \\
    &= (it)^2 \int_0^1 \parens{\int_0^1 e^{i s_1 s_2 (H+\alpha G)t} s_1 G e^{i s_1 (1-s_2) (H+\alpha G)t} ds_2} G e^{i(1-s_1) (H+\alpha G)t} ds_1 \rho e^{-i(H+\alpha G) t} \bigg|_{\alpha=0} \label{eq:second_order_deriv_intermediate_a}\\
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 s_2 H t} G e^{-i s_1 s_2 H t} e^{i s_1 H t} G e^{-i s_1 H t} s_1 ds_1 ds_2 e^{i H t} \rho e^{-i H t} \\
    &= -t^2 \int_0^1 \int_0^1 G(s_1 s_2 t) G(s_1 t) s_1 ds_1 ds_2 \rho(t). \label{eq:second_deriv_alpha_first_term}
\end{align}

\begin{align}
    (B) &= it \int_0^1 e^{i s_1 (H + \alpha G)t} G \frac{\partial}{\partial \alpha}\parens{e^{i(1-s_1)(H + \alpha G)t}} ds_1 \rho e^{-i(H + \alpha G) t} \bigg|_{\alpha = 0} \\
    &= (it)^2 \int_0^1 e^{i s_1 (H + \alpha G)t} G \parens{\int_0^1 e^{i(1-s_1)s_2 (H + \alpha G)t} (1-s_1) G e^{i(1 - s_1)(1 - s_2)(H + \alpha G)t} ds_2} ds_1 ~ \rho e^{-i ( H + \alpha G)t} \bigg|_{\alpha = 0} \\
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 H t} G e^{i(1-s_1)s_2 H t} G e^{i(1-s_1)(1-s_2) H t} (1-s_1) ds_1 ds_2 ~ \rho e^{-i H t}\\ 
    &= -t^2 \int_0^1 \int_0^1 e^{i s_1 H t} G e^{-i s_1 H t} e^{i(s_1 + s_2 - s_1 s_2) H t} G e^{-i (s_1 + s_2 - s_1 s_2) H t} (1-s_1) ds_1 ds_2 ~ \rho(t) \\
    &= -t^2 \int_0^1 \int_0^1 G(s_1 t) G((s_1 + s_2 - s_1 s_2)t) (1-s_1) ds_1 ds_2 ~ \rho(t)
\end{align}

\begin{align}
    (C) &= it \int_0^1 e^{i s (H + \alpha G)t} G e^{i(1-s) (H + \alpha G) t} ds ~\rho ~ \frac{\partial}{\partial \alpha} \parens{ e^{-i (H + \alpha G) t} } \bigg|_{\alpha = 0} \\
    &= (i t) (-it) \int_0^1 e^{i s (H + \alpha G)t} G e^{i (1 - s) (H + \alpha G)t} ds ~ \rho ~ \parens{ \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1- s) ( H + \alpha G)t } ds}\bigg|_{\alpha = 0} \\
    &= + t^2 \parens{\int_0^1 e^{i s H t} G e^{-i s H t} ds} e^{i H t} \rho e^{-i H t} \parens{\int_0^1 e^{i (1-s) H t} G e^{-i (1-s) H t} ds} \\
    &= + t^2 \int_0^1 G(st) ds ~ \rho(t) \int_0^1 G((1-s)t) ds
\end{align}

\begin{align}
    (D) &= (-it) \frac{\partial}{\partial \alpha} \parens{e^{i(H + \alpha G)t}} \rho \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1-s)(H + \alpha G)t} ds \bigg|_{\alpha = 0} \\
    &= t^2 \parens{\int_0^1 e^{i s (H+ \alpha G)t} G e^{i (1-s) (H + \alpha G)t}ds} \rho \int_0^1 e^{-i s (H + \alpha G)t} G e^{-i (1-s)(H + \alpha G)t} ds \bigg|_{\alpha = 0} \\
    &=  t^2 \int_0^1 e^{i s H t} G e^{-i s H t} ds ~\rho(t) \int_0^1 e^{i (1-s) H t} G e^{-i (1-s) H t} ds \\
    &= t^2 \int_0^1 G(st) ds ~ \rho(t) ~ \int_0^1 G((1-s)t) ds
\end{align}

\begin{align}
    (E) &= (-it) e^{i (H+ \alpha G) t} ~ \rho ~\int_0^1 \frac{\partial}{\partial \alpha} \parens{e^{-i s_1 (H + \alpha G)t}} G e^{-i (1-s_1)(H + \alpha G)t} ds_1 \bigg|_{\alpha = 0} \\
    &= - t^2 e^{i(H + \alpha G)t} ~ \rho ~\int_0^1 \parens{\int_0^1 e^{-i s_1 s_2 (H + \alpha G) t} (s_1 G) e^{-i s_1 (1-s_2) (H + \alpha G)t} ds_2} G e^{-i(1-s_1)(H + \alpha G)t} ds_1 \bigg|_{\alpha = 0} \\
    &= -t^2 e^{i H t} \rho e^{-i H t} \int_0^1 \int_0^1 e^{i (1 - s_1 s_2) H t} G e^{-i (s_1 - s_1 s_2)H t} G e^{-i (1-s_1)H t} s_1 ds_1 ds_2 \\
    &= -t^2 \rho(t) \int_0^1 \int_0^1 G((1- s_1 s_2) t) G((1-s_1)t) s_1 ds_1 ds_2
\end{align}

\begin{align}
    (F) &= (-it) e^{i(H + \alpha G) t} \rho \int_0^1 e^{-i s_1 ( H + \alpha G) t} G \frac{\partial}{\partial \alpha} \parens{ e^{-i (1-s_1) ( H +\alpha G)t}} ds_1 \bigg|_{\alpha = 0} \\
    &= (-it)^2 e^{i (H + \alpha G)t} \rho \int_0^1 e^{-i s_1 (H + \alpha G)t} G \parens{\int_0^1 e^{-i(1-s_1) s_2 (H + \alpha G)t} (1-s_1) G e^{-i(1-s_1) (1-s_2) (H + \alpha G) t} ds_2} ds_1 \bigg|_{\alpha = 0} \\
    &= -t^2 e^{-i H t} \rho e^{-i H t} \int_0^1 \int_0^1 e^{i (1- s_1) H t} G e^{-i (1-s_1) H t} e^{i (1-s_1)(1-s_2) H t} G e^{-i(1-s_1)(1-s_2) H t} (1-s_1) ds_1 ds_2 \\
    &= -t^2 \rho(t) \int_0^1 \int_0^1 G((1-s_1)t) G((1-s_1)(1 - s_2) t) (1-s_1)ds_1 ds_2
\end{align}

Now our goal is to compute the effects of averaging over the interaction $G$ on the above terms, starting with $(A)$. As this involves a lot of index manipulations, similarly to the proofs of Lemmas \ref{lem:two_heisenberg_interactions} and \ref{lem:sandwiched_interaction} we will use a single index for the total system-environment Hilbert space and switch back to a double index to state the results. We will make heavy use of Lemma \ref{lem:two_heisenberg_interactions}.
\begin{align}
    \int (A) dG &= -t^2 \int_0^1 \int_0^1 \int G(s_1 s_2 t) G(s_1 t) dG s_1 ds_1 ds_2 \rho(t) \\
    &= \frac{-t^2 }{\dim + 1} \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i (\lambda(i) - \lambda(j)) (s_1 s_2 t - s_1 t)} \ketbra{i}{i} + \identity} s_1 ds_1 ds_2 \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \int_0^1 \int_0^1 e^{i(\lambda(i) - \lambda(j))t (s_1 s_2 - s_1)} s_1 ds_1 ds_2 \ketbra{i}{i} + \sum_{i} \sum_{j : \lambda(i) = \lambda(j)}\frac{1}{2} \ketbra{i}{i} + \frac{1}{2} \identity} \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 - i (\lambda(i) - \lambda(j))t - e^{-i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i} } \rho(t) \\
    &= \frac{- 1}{\dim + 1}\parens{\sum_{i} \sum_{j: \Delta_{ij} \neq 0} \frac{1 - i \Delta_{ij}t - e^{-i \Delta_{ij} t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2} \sum_{i} (\eta(i) + 1)\ketbra{i}{i} } \rho(t)
\end{align}

We can similarly compute the averaged $(B)$ term:
\begin{align}
    \int (B) dG &= -t^2 \int_0^1 \int_0^1 \int G(s_1 t) G((s_1 + s_2 - s_1 s_2) t) dG (1-s_1) ds_1 ds_2 ~ \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i (\lambda(i) - \lambda(j))(s_1 s_2 - s_2) t} \ketbra{i}{i} + \identity} (1 -s_1) ds_1 ds_2 \rho \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \int_0^1 \int_0^1 e^{i(\lambda(i) - \lambda(j))t (s_1 s_2 - s_2)} (1 - s_1) ds_1 ds_2 \ketbra{i}{i} + \sum_{i} \sum_{j : \lambda(i) = \lambda(j)}\frac{1}{2} \ketbra{i}{i} + \frac{1}{2} \identity} \rho(t) \\
    &= \frac{- t^2 }{\dim + 1} \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 - i (\lambda(i) - \lambda(j))t - e^{-i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i} } \rho(t) \\
    &= \frac{-1}{\dim + 1}\parens{\sum_{i} \sum_{j: \Delta_{ij} \neq 0} \frac{1 - i \Delta_{ij}t - e^{-i \Delta_{ij} t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2} \sum_{i} (\eta(i) + 1)\ketbra{i}{i} } \rho(t),
\end{align}
which we note is identical to $\int (A) dG$. As terms $(C)$ and $(D)$ involve a different method of computation we skip them for now and compute $(E)$ and $(F)$. 
\begin{align}
    \int (E) dG &= -t^2 \rho(t) \int_0^1 \int_0^1 \int G((1- s_1 s_2) t) G((1-s_1)t) dG s_1 ds_1 ds_2 \\
    &= \frac{- t^2}{\dim + 1} \rho(t) \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i(\lambda(i) - \lambda(j)) t (s_1 - s_1 s_2)} \ketbra{i}{i} + \identity } s_1 ds_1 ds_2 \\
    &= \frac{- t^2}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 + i (\lambda(i) - \lambda(j))t - e^{i(\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2}\ketbra{i}{i} + \frac{1}{2} \sum_{i} (\eta(i) + 1 )\ketbra{i}{i}} \\
    &= \frac{- 1}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j: (\Delta_{ij} \neq 0)} \frac{1 + i \Delta_{ij}t - e^{i\Delta_{ij}t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2}\sum_i (\eta(i) + 1) \ketbra{i}{i}}.
\end{align}
Computing $(F)$ yields
\begin{align}
    \int (F) dG &= -t^2 \rho(t) \int_0^1 \int_0^1 \int G((1-s_1)t) G((1-s_1)(1 - s_2) t) dG (1-s_1)ds_1 ds_2 \\
    &= \frac{- t^2 \sigma ^2}{\dim + 1} \rho(t) \int_0^1 \int_0^1 \parens{\sum_{i,j} e^{i(\lambda(i) - \lambda(j))t (s_2 - s_1 s_2)}\ketbra{i}{i} + \identity} (1-s_1) ds_1 ds_2 \\
    &= \frac{- t^2 }{\dim + 1} \rho(t) \parens{\sum_{i} \sum_{j : \lambda(i) \neq \lambda(j)} \frac{1 + i (\lambda(i) - \lambda(j))t - e^{i (\lambda(i) - \lambda(j))t}}{t^2 (\lambda(i) - \lambda(j))^2} \ketbra{i}{i} +\frac{1}{2} \sum_{i} (\eta(i) + 1) \ketbra{i}{i}} \\
    &= \frac{- 1}{\dim + 1} \rho(t) \parens{\sum_i \sum_{j: (\Delta_{ij} \neq 0)} \frac{1 + i \Delta_{ij}t - e^{i\Delta_{ij}t}}{\Delta_{ij}^2} \ketbra{i}{i} + \frac{t^2}{2}\sum_i (\eta(i) + 1) \ketbra{i}{i}}
\end{align}
 which is identical to $\int (E) dG$.

 The last two terms $(C) = (D)$ are computed as follows:
 \begin{align}
     \int (C) dG &= t^2 \int_0^1 \int_0^1 \int G(s_1 t) \rho(t) G((1-s_2)t) ~dG ~ ds_1 ds_2 \\
     &= t^2 \sum_{i,j} \rho_{ij} e^{i(\lambda(i) - \lambda(j))t} \int_0^1 \int_0^1 \int G(s_1 t) \ketbra{i}{j} G((1-s_2)t) ~ dG ~ ds_1 ds_2 \\
     &= \frac{ t^2}{\dim + 1} \sum_{i,j} \rho_{ij} e^{i(\lambda(i) - \lambda(j))t} \parens{ \ketbra{i}{j} + \delta_{ij} \sum_{a} \int_0^1 \int_0^1 e^{i(\lambda(a) - \lambda(i))(s_1 + s_2 - 1)t} ds_1 ds_2 \ketbra{a}{a}} \\
     &= \frac{ t^2}{\dim + 1} \sum_{i,j} \rho_{ij} e^{i \Delta_{ij} t} \parens{\ketbra{i}{j} + \delta_{ij} \sum_{a : \Delta_{ai} \neq 0} \frac{2( 1- \cos (\Delta_{ai} t))}{\Delta_{ai}^2 t^2} \ketbra{a}{a} + \delta_{ij} \sum_{a : \Delta_{ai} = 0} \ketbra{a}{a}}
 \end{align}

 We can now combine each of these terms to offer the full picture of the output of the channel to second order. We make two modifications to the results from each sum: first, we will switch to double index notation to make for easier use in other areas, and secondly we let $\rho = \ketbra{i,j}{k,l}$. We note that the first term in the following equation is provided by $(A) + (B)$, the second through $(E) + (F)$, and the last two through $(C) + (D)$. 
 \begin{align}
     &\int \frac{\partial^2}{\partial \alpha^2} \Phi_G(\ketbra{i,j}{k,l})\bigg|_{\alpha = 0} dG \\
     &= -\frac{2  e^{i \Delta(i,j|k,l) t}}{\dim + 1} \bigg(\sum_{(a,b): \Delta(i,j|a,b) \neq 0} \frac{1 - i \Delta(i,j|a,b)t - e^{-i \Delta(i,j|a,b) t}}{\Delta(i,j|a,b)^2} \nonumber \\
     &~+ \sum_{(a,b): \Delta(k,l|a,b) \neq 0} \frac{1 + i \Delta(k,l|a,b) t - e^{i \Delta(k,l|a,b) t}}{\Delta(k,l|a,b)^2} + \frac{t^2}{2}(\eta(i,j) + \eta(k,l)) \bigg) \ketbra{i,j}{k,l} \nonumber \\
    &~ +\delta_{i,k} \delta_{j,l} \frac{2 e^{i \Delta(i,j|k,l)t}}{\dim+1} \parens{ \sum_{(a,b): \Delta(i,j|a,b) \neq 0 } \frac{2(1- \cos (\Delta(i,j|a,b)t))}{\Delta(i,j|a,b)^2} \ketbra{a,b}{a,b} + t^2 \sum_{(a,b) : \Delta(i,j|a,b) = 0} \ketbra{a,b}{a,b}} \label{eq:second_order_output}
 \end{align}
\end{proof}

\end{document}